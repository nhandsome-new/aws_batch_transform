{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17897c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import os\n",
    "\n",
    "def make_dir(path):\n",
    "    if os.path.isdir(data_path):\n",
    "        pass\n",
    "    else:\n",
    "        os.path.make_dir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cde19064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初期設定\n",
    "import sagemaker\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# AWS設定\n",
    "role = 'FullAccessHan'\n",
    "region = boto3.Session().region_name\n",
    "bucket='sagemaker-han'\n",
    "prefix = 'sagemaker/cnn-cifar10'\n",
    "bucket_path = 'https://s3-{}.amazonaws.com/{}'.format(region,bucket)\n",
    "\n",
    "# Local設定\n",
    "data_dir = 'cifar10/testing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b30b349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to mnist_png/training/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_png/training/cifar-10-python.tar.gz to mnist_png/training\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to mnist_png/testing/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_png/testing/cifar-10-python.tar.gz to mnist_png/testing\n",
      "input spec (in this case, just an S3 path): s3://sagemaker-han/sagemaker/cnn-cifar10\n"
     ]
    }
   ],
   "source": [
    "# データセットの準備 → S3に保存　\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "train_data = datasets.CIFAR10(root=training_dir, train=True, download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10(root=test_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "training_data_loader = DataLoader(train_data, batch_size=len(train_data))\n",
    "training_data_loaded = next(iter(training_data_loader))\n",
    "torch.save(training_data_loaded, os.path.join(data_dir, 'training.pt'))\n",
    "\n",
    "test_data_loader = DataLoader(test_data, batch_size=len(test_data))\n",
    "test_data_loaded = next(iter(test_data_loader))\n",
    "torch.save(test_data_loaded, os.path.join(data_dir, 'test.pt'))\n",
    "\n",
    "inputs = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)\n",
    "print('input spec (in this case, just an S3 path): {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "273647df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;0;128;0;01mimport\u001b[39;00m \u001b[38;2;0;0;255;01margparse\u001b[39;00m\r\n",
      "\u001b[38;2;0;128;0;01mimport\u001b[39;00m \u001b[38;2;0;0;255;01mjson\u001b[39;00m\r\n",
      "\u001b[38;2;0;128;0;01mimport\u001b[39;00m \u001b[38;2;0;0;255;01mlogging\u001b[39;00m\r\n",
      "\u001b[38;2;0;128;0;01mimport\u001b[39;00m \u001b[38;2;0;0;255;01mos\u001b[39;00m\r\n",
      "\u001b[38;2;0;128;0;01mimport\u001b[39;00m \u001b[38;2;0;0;255;01msys\u001b[39;00m\r\n",
      "\u001b[38;2;0;128;0;01mimport\u001b[39;00m \u001b[38;2;0;0;255;01mtorch\u001b[39;00m\r\n",
      "\u001b[38;2;0;128;0;01mimport\u001b[39;00m \u001b[38;2;0;0;255;01mtorch\u001b[39;00m\u001b[38;2;0;0;255;01m.\u001b[39;00m\u001b[38;2;0;0;255;01mdistributed\u001b[39;00m \u001b[38;2;0;128;0;01mas\u001b[39;00m \u001b[38;2;0;0;255;01mdist\u001b[39;00m\r\n",
      "\u001b[38;2;0;128;0;01mimport\u001b[39;00m \u001b[38;2;0;0;255;01mtorch\u001b[39;00m\u001b[38;2;0;0;255;01m.\u001b[39;00m\u001b[38;2;0;0;255;01mnn\u001b[39;00m \u001b[38;2;0;128;0;01mas\u001b[39;00m \u001b[38;2;0;0;255;01mnn\u001b[39;00m\r\n",
      "\u001b[38;2;0;128;0;01mimport\u001b[39;00m \u001b[38;2;0;0;255;01mtorch\u001b[39;00m\u001b[38;2;0;0;255;01m.\u001b[39;00m\u001b[38;2;0;0;255;01mnn\u001b[39;00m\u001b[38;2;0;0;255;01m.\u001b[39;00m\u001b[38;2;0;0;255;01mfunctional\u001b[39;00m \u001b[38;2;0;128;0;01mas\u001b[39;00m \u001b[38;2;0;0;255;01mF\u001b[39;00m\r\n",
      "\u001b[38;2;0;128;0;01mimport\u001b[39;00m \u001b[38;2;0;0;255;01mtorch\u001b[39;00m\u001b[38;2;0;0;255;01m.\u001b[39;00m\u001b[38;2;0;0;255;01moptim\u001b[39;00m \u001b[38;2;0;128;0;01mas\u001b[39;00m \u001b[38;2;0;0;255;01moptim\u001b[39;00m\r\n",
      "\u001b[38;2;0;128;0;01mimport\u001b[39;00m \u001b[38;2;0;0;255;01mtorch\u001b[39;00m\u001b[38;2;0;0;255;01m.\u001b[39;00m\u001b[38;2;0;0;255;01mutils\u001b[39;00m\u001b[38;2;0;0;255;01m.\u001b[39;00m\u001b[38;2;0;0;255;01mdata\u001b[39;00m\r\n",
      "\u001b[38;2;0;128;0;01mimport\u001b[39;00m \u001b[38;2;0;0;255;01mtorch\u001b[39;00m\u001b[38;2;0;0;255;01m.\u001b[39;00m\u001b[38;2;0;0;255;01mutils\u001b[39;00m\u001b[38;2;0;0;255;01m.\u001b[39;00m\u001b[38;2;0;0;255;01mdata\u001b[39;00m\u001b[38;2;0;0;255;01m.\u001b[39;00m\u001b[38;2;0;0;255;01mdistributed\u001b[39;00m\r\n",
      "\u001b[38;2;0;128;0;01mfrom\u001b[39;00m \u001b[38;2;0;0;255;01mtorchvision\u001b[39;00m \u001b[38;2;0;128;0;01mimport\u001b[39;00m datasets, transforms\r\n",
      "\r\n",
      "logger \u001b[38;2;102;102;102m=\u001b[39m logging\u001b[38;2;102;102;102m.\u001b[39mgetLogger(\u001b[38;2;25;23;124m__name__\u001b[39m)\r\n",
      "logger\u001b[38;2;102;102;102m.\u001b[39msetLevel(logging\u001b[38;2;102;102;102m.\u001b[39mDEBUG)\r\n",
      "logger\u001b[38;2;102;102;102m.\u001b[39maddHandler(logging\u001b[38;2;102;102;102m.\u001b[39mStreamHandler(sys\u001b[38;2;102;102;102m.\u001b[39mstdout))\r\n",
      "\r\n",
      "\r\n",
      "\u001b[38;2;61;123;123;03m# Based on https://github.com/pytorch/examples/blob/master/mnist/main.py\u001b[39;00m\r\n",
      "\u001b[38;2;61;123;123;03m# https://colab.research.google.com/github/seyrankhademi/ResNet_CIFAR10/blob/master/CIFAR10_ResNet.ipynb\u001b[39;00m\r\n",
      "\r\n",
      "\u001b[38;2;0;128;0;01mclass\u001b[39;00m \u001b[38;2;0;0;255;01mBasicBlock\u001b[39;00m(nn\u001b[38;2;102;102;102m.\u001b[39mModule):\r\n",
      "    expansion \u001b[38;2;102;102;102m=\u001b[39m \u001b[38;2;102;102;102m1\u001b[39m\r\n",
      "\r\n",
      "    \u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255m__init__\u001b[39m(\u001b[38;2;0;128;0mself\u001b[39m, in_planes, planes, stride\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m1\u001b[39m, option\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mA\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m):\r\n",
      "        \u001b[38;2;0;128;0msuper\u001b[39m(BasicBlock, \u001b[38;2;0;128;0mself\u001b[39m)\u001b[38;2;102;102;102m.\u001b[39m\u001b[38;2;0;0;255m__init__\u001b[39m()\r\n",
      "        \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mconv1 \u001b[38;2;102;102;102m=\u001b[39m nn\u001b[38;2;102;102;102m.\u001b[39mConv2d(in_planes, planes, kernel_size\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m3\u001b[39m, stride\u001b[38;2;102;102;102m=\u001b[39mstride, padding\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m1\u001b[39m, bias\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;0;128;0;01mFalse\u001b[39;00m)\r\n",
      "        \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mbn1 \u001b[38;2;102;102;102m=\u001b[39m nn\u001b[38;2;102;102;102m.\u001b[39mBatchNorm2d(planes)\r\n",
      "        \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mconv2 \u001b[38;2;102;102;102m=\u001b[39m nn\u001b[38;2;102;102;102m.\u001b[39mConv2d(planes, planes, kernel_size\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m3\u001b[39m, stride\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m1\u001b[39m, padding\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m1\u001b[39m, bias\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;0;128;0;01mFalse\u001b[39;00m)\r\n",
      "        \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mbn2 \u001b[38;2;102;102;102m=\u001b[39m nn\u001b[38;2;102;102;102m.\u001b[39mBatchNorm2d(planes)\r\n",
      "        \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mshortcut \u001b[38;2;102;102;102m=\u001b[39m nn\u001b[38;2;102;102;102m.\u001b[39mSequential(\r\n",
      "                nn\u001b[38;2;102;102;102m.\u001b[39mConv2d(in_planes, \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mexpansion \u001b[38;2;102;102;102m*\u001b[39m planes, kernel_size\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m1\u001b[39m, stride\u001b[38;2;102;102;102m=\u001b[39mstride, bias\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;0;128;0;01mFalse\u001b[39;00m),\r\n",
      "                nn\u001b[38;2;102;102;102m.\u001b[39mBatchNorm2d(\u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mexpansion \u001b[38;2;102;102;102m*\u001b[39m planes)\r\n",
      "        )\r\n",
      "\r\n",
      "    \u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255mforward\u001b[39m(\u001b[38;2;0;128;0mself\u001b[39m, x):\r\n",
      "        out \u001b[38;2;102;102;102m=\u001b[39m F\u001b[38;2;102;102;102m.\u001b[39mrelu(\u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mbn1(\u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mconv1(x)))\r\n",
      "        out \u001b[38;2;102;102;102m=\u001b[39m \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mbn2(\u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mconv2(out))\r\n",
      "        out \u001b[38;2;102;102;102m+\u001b[39m\u001b[38;2;102;102;102m=\u001b[39m \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mshortcut(x)\r\n",
      "        out \u001b[38;2;102;102;102m=\u001b[39m F\u001b[38;2;102;102;102m.\u001b[39mrelu(out)\r\n",
      "        \u001b[38;2;0;128;0;01mreturn\u001b[39;00m out\r\n",
      "    \r\n",
      "\u001b[38;2;0;128;0;01mclass\u001b[39;00m \u001b[38;2;0;0;255;01mResNet\u001b[39;00m(nn\u001b[38;2;102;102;102m.\u001b[39mModule):\r\n",
      "    \u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255m__init__\u001b[39m(\u001b[38;2;0;128;0mself\u001b[39m, block, num_blocks, num_classes\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m10\u001b[39m):\r\n",
      "        \u001b[38;2;0;128;0msuper\u001b[39m(ResNet, \u001b[38;2;0;128;0mself\u001b[39m)\u001b[38;2;102;102;102m.\u001b[39m\u001b[38;2;0;0;255m__init__\u001b[39m()\r\n",
      "        \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39min_planes \u001b[38;2;102;102;102m=\u001b[39m \u001b[38;2;102;102;102m16\u001b[39m\r\n",
      "        \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mconv1 \u001b[38;2;102;102;102m=\u001b[39m nn\u001b[38;2;102;102;102m.\u001b[39mConv2d(\u001b[38;2;102;102;102m3\u001b[39m, \u001b[38;2;102;102;102m16\u001b[39m, kernel_size\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m3\u001b[39m, stride\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m1\u001b[39m, padding\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m1\u001b[39m, bias\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;0;128;0;01mFalse\u001b[39;00m)\r\n",
      "        \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mbn1 \u001b[38;2;102;102;102m=\u001b[39m nn\u001b[38;2;102;102;102m.\u001b[39mBatchNorm2d(\u001b[38;2;102;102;102m16\u001b[39m)\r\n",
      "        \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mlayer1 \u001b[38;2;102;102;102m=\u001b[39m \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39m_make_layer(block, \u001b[38;2;102;102;102m16\u001b[39m, num_blocks[\u001b[38;2;102;102;102m0\u001b[39m], stride\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m1\u001b[39m)\r\n",
      "        \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mlayer2 \u001b[38;2;102;102;102m=\u001b[39m \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39m_make_layer(block, \u001b[38;2;102;102;102m32\u001b[39m, num_blocks[\u001b[38;2;102;102;102m1\u001b[39m], stride\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m2\u001b[39m)\r\n",
      "        \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mlayer3 \u001b[38;2;102;102;102m=\u001b[39m \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39m_make_layer(block, \u001b[38;2;102;102;102m64\u001b[39m, num_blocks[\u001b[38;2;102;102;102m2\u001b[39m], stride\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m2\u001b[39m)\r\n",
      "        \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mlinear \u001b[38;2;102;102;102m=\u001b[39m nn\u001b[38;2;102;102;102m.\u001b[39mLinear(\u001b[38;2;102;102;102m64\u001b[39m, num_classes)\r\n",
      "        \r\n",
      "    \u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255m_make_layer\u001b[39m(\u001b[38;2;0;128;0mself\u001b[39m, block, planes, num_blocks, stride):\r\n",
      "        strides \u001b[38;2;102;102;102m=\u001b[39m [stride] \u001b[38;2;102;102;102m+\u001b[39m [\u001b[38;2;102;102;102m1\u001b[39m]\u001b[38;2;102;102;102m*\u001b[39m(num_blocks\u001b[38;2;102;102;102m-\u001b[39m\u001b[38;2;102;102;102m1\u001b[39m)\r\n",
      "        layers \u001b[38;2;102;102;102m=\u001b[39m []\r\n",
      "        \u001b[38;2;0;128;0;01mfor\u001b[39;00m stride \u001b[38;2;170;34;255;01min\u001b[39;00m strides:\r\n",
      "            layers\u001b[38;2;102;102;102m.\u001b[39mappend(block(\u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39min_planes, planes, stride))\r\n",
      "            \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39min_planes \u001b[38;2;102;102;102m=\u001b[39m planes \u001b[38;2;102;102;102m*\u001b[39m block\u001b[38;2;102;102;102m.\u001b[39mexpansion\r\n",
      "\r\n",
      "        \u001b[38;2;0;128;0;01mreturn\u001b[39;00m nn\u001b[38;2;102;102;102m.\u001b[39mSequential(\u001b[38;2;102;102;102m*\u001b[39mlayers)\r\n",
      "\r\n",
      "    \u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255mforward\u001b[39m(\u001b[38;2;0;128;0mself\u001b[39m, x):\r\n",
      "        out \u001b[38;2;102;102;102m=\u001b[39m F\u001b[38;2;102;102;102m.\u001b[39mrelu(\u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mbn1(\u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mconv1(x)))\r\n",
      "        out \u001b[38;2;102;102;102m=\u001b[39m \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mlayer1(out)\r\n",
      "        out \u001b[38;2;102;102;102m=\u001b[39m \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mlayer2(out)\r\n",
      "        out \u001b[38;2;102;102;102m=\u001b[39m \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mlayer3(out)\r\n",
      "        out \u001b[38;2;102;102;102m=\u001b[39m F\u001b[38;2;102;102;102m.\u001b[39mavg_pool2d(out, out\u001b[38;2;102;102;102m.\u001b[39msize()[\u001b[38;2;102;102;102m3\u001b[39m])\r\n",
      "        out \u001b[38;2;102;102;102m=\u001b[39m out\u001b[38;2;102;102;102m.\u001b[39mview(out\u001b[38;2;102;102;102m.\u001b[39msize(\u001b[38;2;102;102;102m0\u001b[39m), \u001b[38;2;102;102;102m-\u001b[39m\u001b[38;2;102;102;102m1\u001b[39m)\r\n",
      "        out \u001b[38;2;102;102;102m=\u001b[39m \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mlinear(out)\r\n",
      "        \u001b[38;2;0;128;0;01mreturn\u001b[39;00m out\r\n",
      "\r\n",
      "\u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255mresnet20\u001b[39m():\r\n",
      "    \u001b[38;2;0;128;0;01mreturn\u001b[39;00m ResNet(BasicBlock, [\u001b[38;2;102;102;102m3\u001b[39m, \u001b[38;2;102;102;102m3\u001b[39m, \u001b[38;2;102;102;102m3\u001b[39m])\r\n",
      "\r\n",
      "\r\n",
      "\u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255m_get_train_data_loader\u001b[39m(batch_size, training_dir, is_distributed, \u001b[38;2;102;102;102m*\u001b[39m\u001b[38;2;102;102;102m*\u001b[39mkwargs):\r\n",
      "    logger\u001b[38;2;102;102;102m.\u001b[39minfo(\u001b[38;2;186;33;33m\"\u001b[39m\u001b[38;2;186;33;33mGet train data loader\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m)\r\n",
      "    train_tensor \u001b[38;2;102;102;102m=\u001b[39m torch\u001b[38;2;102;102;102m.\u001b[39mload(os\u001b[38;2;102;102;102m.\u001b[39mpath\u001b[38;2;102;102;102m.\u001b[39mjoin(training_dir, \u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mtraining.pt\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m))\r\n",
      "    dataset \u001b[38;2;102;102;102m=\u001b[39m torch\u001b[38;2;102;102;102m.\u001b[39mutils\u001b[38;2;102;102;102m.\u001b[39mdata\u001b[38;2;102;102;102m.\u001b[39mTensorDataset(train_tensor[\u001b[38;2;102;102;102m0\u001b[39m], train_tensor[\u001b[38;2;102;102;102m1\u001b[39m])\r\n",
      "    train_sampler \u001b[38;2;102;102;102m=\u001b[39m torch\u001b[38;2;102;102;102m.\u001b[39mutils\u001b[38;2;102;102;102m.\u001b[39mdata\u001b[38;2;102;102;102m.\u001b[39mdistributed\u001b[38;2;102;102;102m.\u001b[39mDistributedSampler(dataset) \u001b[38;2;0;128;0;01mif\u001b[39;00m is_distributed \u001b[38;2;0;128;0;01melse\u001b[39;00m \u001b[38;2;0;128;0;01mNone\u001b[39;00m\r\n",
      "    \u001b[38;2;0;128;0;01mreturn\u001b[39;00m torch\u001b[38;2;102;102;102m.\u001b[39mutils\u001b[38;2;102;102;102m.\u001b[39mdata\u001b[38;2;102;102;102m.\u001b[39mDataLoader(dataset, \r\n",
      "                                       batch_size\u001b[38;2;102;102;102m=\u001b[39mbatch_size, \r\n",
      "                                       shuffle\u001b[38;2;102;102;102m=\u001b[39mtrain_sampler \u001b[38;2;170;34;255;01mis\u001b[39;00m \u001b[38;2;0;128;0;01mNone\u001b[39;00m,\r\n",
      "                                       sampler\u001b[38;2;102;102;102m=\u001b[39mtrain_sampler, \u001b[38;2;102;102;102m*\u001b[39m\u001b[38;2;102;102;102m*\u001b[39mkwargs)\r\n",
      "\r\n",
      "\u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255m_get_test_data_loader\u001b[39m(test_batch_size, training_dir, \u001b[38;2;102;102;102m*\u001b[39m\u001b[38;2;102;102;102m*\u001b[39mkwargs):\r\n",
      "    logger\u001b[38;2;102;102;102m.\u001b[39minfo(\u001b[38;2;186;33;33m\"\u001b[39m\u001b[38;2;186;33;33mGet test data loader\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m)\r\n",
      "    test_tensor \u001b[38;2;102;102;102m=\u001b[39m torch\u001b[38;2;102;102;102m.\u001b[39mload(os\u001b[38;2;102;102;102m.\u001b[39mpath\u001b[38;2;102;102;102m.\u001b[39mjoin(training_dir, \u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mtest.pt\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m))\r\n",
      "    dataset \u001b[38;2;102;102;102m=\u001b[39m torch\u001b[38;2;102;102;102m.\u001b[39mutils\u001b[38;2;102;102;102m.\u001b[39mdata\u001b[38;2;102;102;102m.\u001b[39mTensorDataset(test_tensor[\u001b[38;2;102;102;102m0\u001b[39m], test_tensor[\u001b[38;2;102;102;102m1\u001b[39m])\r\n",
      "    \u001b[38;2;0;128;0;01mreturn\u001b[39;00m torch\u001b[38;2;102;102;102m.\u001b[39mutils\u001b[38;2;102;102;102m.\u001b[39mdata\u001b[38;2;102;102;102m.\u001b[39mDataLoader(\r\n",
      "        dataset,\r\n",
      "        batch_size\u001b[38;2;102;102;102m=\u001b[39mtest_batch_size,\r\n",
      "        shuffle\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;0;128;0;01mFalse\u001b[39;00m, \u001b[38;2;102;102;102m*\u001b[39m\u001b[38;2;102;102;102m*\u001b[39mkwargs)\r\n",
      "\r\n",
      "\u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255m_average_gradients\u001b[39m(model):\r\n",
      "    \u001b[38;2;61;123;123;03m# Gradient averaging.\u001b[39;00m\r\n",
      "    size \u001b[38;2;102;102;102m=\u001b[39m \u001b[38;2;0;128;0mfloat\u001b[39m(dist\u001b[38;2;102;102;102m.\u001b[39mget_world_size())\r\n",
      "    \u001b[38;2;0;128;0;01mfor\u001b[39;00m param \u001b[38;2;170;34;255;01min\u001b[39;00m model\u001b[38;2;102;102;102m.\u001b[39mparameters():\r\n",
      "        dist\u001b[38;2;102;102;102m.\u001b[39mall_reduce(param\u001b[38;2;102;102;102m.\u001b[39mgrad\u001b[38;2;102;102;102m.\u001b[39mdata, op\u001b[38;2;102;102;102m=\u001b[39mdist\u001b[38;2;102;102;102m.\u001b[39mreduce_op\u001b[38;2;102;102;102m.\u001b[39mSUM)\r\n",
      "        param\u001b[38;2;102;102;102m.\u001b[39mgrad\u001b[38;2;102;102;102m.\u001b[39mdata \u001b[38;2;102;102;102m/\u001b[39m\u001b[38;2;102;102;102m=\u001b[39m size\r\n",
      "        \r\n",
      "\u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255mtrain\u001b[39m(args):\r\n",
      "    is_distributed \u001b[38;2;102;102;102m=\u001b[39m \u001b[38;2;0;128;0mlen\u001b[39m(args\u001b[38;2;102;102;102m.\u001b[39mhosts) \u001b[38;2;102;102;102m>\u001b[39m \u001b[38;2;102;102;102m1\u001b[39m \u001b[38;2;170;34;255;01mand\u001b[39;00m args\u001b[38;2;102;102;102m.\u001b[39mbackend \u001b[38;2;170;34;255;01mis\u001b[39;00m \u001b[38;2;170;34;255;01mnot\u001b[39;00m \u001b[38;2;0;128;0;01mNone\u001b[39;00m\r\n",
      "    logger\u001b[38;2;102;102;102m.\u001b[39mdebug(\u001b[38;2;186;33;33m\"\u001b[39m\u001b[38;2;186;33;33mDistributed training - \u001b[39m\u001b[38;2;164;90;119;01m{}\u001b[39;00m\u001b[38;2;186;33;33m\"\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mformat(is_distributed))\r\n",
      "    use_cuda \u001b[38;2;102;102;102m=\u001b[39m args\u001b[38;2;102;102;102m.\u001b[39mnum_gpus \u001b[38;2;102;102;102m>\u001b[39m \u001b[38;2;102;102;102m0\u001b[39m\r\n",
      "    logger\u001b[38;2;102;102;102m.\u001b[39mdebug(\u001b[38;2;186;33;33m\"\u001b[39m\u001b[38;2;186;33;33mNumber of gpus available - \u001b[39m\u001b[38;2;164;90;119;01m{}\u001b[39;00m\u001b[38;2;186;33;33m\"\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mformat(args\u001b[38;2;102;102;102m.\u001b[39mnum_gpus))\r\n",
      "    kwargs \u001b[38;2;102;102;102m=\u001b[39m {\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mnum_workers\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m: \u001b[38;2;102;102;102m1\u001b[39m, \u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mpin_memory\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m: \u001b[38;2;0;128;0;01mTrue\u001b[39;00m} \u001b[38;2;0;128;0;01mif\u001b[39;00m use_cuda \u001b[38;2;0;128;0;01melse\u001b[39;00m {}\r\n",
      "    device \u001b[38;2;102;102;102m=\u001b[39m torch\u001b[38;2;102;102;102m.\u001b[39mdevice(\u001b[38;2;186;33;33m\"\u001b[39m\u001b[38;2;186;33;33mcuda\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m \u001b[38;2;0;128;0;01mif\u001b[39;00m use_cuda \u001b[38;2;0;128;0;01melse\u001b[39;00m \u001b[38;2;186;33;33m\"\u001b[39m\u001b[38;2;186;33;33mcpu\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m)\r\n",
      "\r\n",
      "    \u001b[38;2;0;128;0;01mif\u001b[39;00m is_distributed:\r\n",
      "        \u001b[38;2;61;123;123;03m# Initialize the distributed environment.\u001b[39;00m\r\n",
      "        world_size \u001b[38;2;102;102;102m=\u001b[39m \u001b[38;2;0;128;0mlen\u001b[39m(args\u001b[38;2;102;102;102m.\u001b[39mhosts)\r\n",
      "        os\u001b[38;2;102;102;102m.\u001b[39menviron[\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mWORLD_SIZE\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m] \u001b[38;2;102;102;102m=\u001b[39m \u001b[38;2;0;128;0mstr\u001b[39m(world_size)\r\n",
      "        host_rank \u001b[38;2;102;102;102m=\u001b[39m args\u001b[38;2;102;102;102m.\u001b[39mhosts\u001b[38;2;102;102;102m.\u001b[39mindex(args\u001b[38;2;102;102;102m.\u001b[39mcurrent_host)\r\n",
      "        os\u001b[38;2;102;102;102m.\u001b[39menviron[\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mRANK\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m] \u001b[38;2;102;102;102m=\u001b[39m \u001b[38;2;0;128;0mstr\u001b[39m(host_rank)\r\n",
      "        dist\u001b[38;2;102;102;102m.\u001b[39minit_process_group(backend\u001b[38;2;102;102;102m=\u001b[39margs\u001b[38;2;102;102;102m.\u001b[39mbackend, rank\u001b[38;2;102;102;102m=\u001b[39mhost_rank, world_size\u001b[38;2;102;102;102m=\u001b[39mworld_size)\r\n",
      "        logger\u001b[38;2;102;102;102m.\u001b[39minfo(\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mInitialized the distributed environment: \u001b[39m\u001b[38;2;170;93;31;01m\\'\u001b[39;00m\u001b[38;2;164;90;119;01m{}\u001b[39;00m\u001b[38;2;170;93;31;01m\\'\u001b[39;00m\u001b[38;2;186;33;33m backend on \u001b[39m\u001b[38;2;164;90;119;01m{}\u001b[39;00m\u001b[38;2;186;33;33m nodes. \u001b[39m\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mformat(\r\n",
      "            args\u001b[38;2;102;102;102m.\u001b[39mbackend, dist\u001b[38;2;102;102;102m.\u001b[39mget_world_size()) \u001b[38;2;102;102;102m+\u001b[39m \u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mCurrent host rank is \u001b[39m\u001b[38;2;164;90;119;01m{}\u001b[39;00m\u001b[38;2;186;33;33m. Number of gpus: \u001b[39m\u001b[38;2;164;90;119;01m{}\u001b[39;00m\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mformat(\r\n",
      "            dist\u001b[38;2;102;102;102m.\u001b[39mget_rank(), args\u001b[38;2;102;102;102m.\u001b[39mnum_gpus))\r\n",
      "\r\n",
      "    \u001b[38;2;61;123;123;03m# set the seed for generating random numbers\u001b[39;00m\r\n",
      "    torch\u001b[38;2;102;102;102m.\u001b[39mmanual_seed(args\u001b[38;2;102;102;102m.\u001b[39mseed)\r\n",
      "    \u001b[38;2;0;128;0;01mif\u001b[39;00m use_cuda:\r\n",
      "        torch\u001b[38;2;102;102;102m.\u001b[39mcuda\u001b[38;2;102;102;102m.\u001b[39mmanual_seed(args\u001b[38;2;102;102;102m.\u001b[39mseed)\r\n",
      "\r\n",
      "    train_loader \u001b[38;2;102;102;102m=\u001b[39m _get_train_data_loader(args\u001b[38;2;102;102;102m.\u001b[39mbatch_size, args\u001b[38;2;102;102;102m.\u001b[39mdata_dir, is_distributed, \u001b[38;2;102;102;102m*\u001b[39m\u001b[38;2;102;102;102m*\u001b[39mkwargs)\r\n",
      "    test_loader \u001b[38;2;102;102;102m=\u001b[39m _get_test_data_loader(args\u001b[38;2;102;102;102m.\u001b[39mtest_batch_size, args\u001b[38;2;102;102;102m.\u001b[39mdata_dir, \u001b[38;2;102;102;102m*\u001b[39m\u001b[38;2;102;102;102m*\u001b[39mkwargs)\r\n",
      "\r\n",
      "    logger\u001b[38;2;102;102;102m.\u001b[39mdebug(\u001b[38;2;186;33;33m\"\u001b[39m\u001b[38;2;186;33;33mProcesses \u001b[39m\u001b[38;2;164;90;119;01m{}\u001b[39;00m\u001b[38;2;186;33;33m/\u001b[39m\u001b[38;2;164;90;119;01m{}\u001b[39;00m\u001b[38;2;186;33;33m (\u001b[39m\u001b[38;2;164;90;119;01m{:.0f}\u001b[39;00m\u001b[38;2;186;33;33m%\u001b[39m\u001b[38;2;186;33;33m) of train data\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mformat(\r\n",
      "        \u001b[38;2;0;128;0mlen\u001b[39m(train_loader\u001b[38;2;102;102;102m.\u001b[39msampler), \u001b[38;2;0;128;0mlen\u001b[39m(train_loader\u001b[38;2;102;102;102m.\u001b[39mdataset),\r\n",
      "        \u001b[38;2;102;102;102m100.\u001b[39m \u001b[38;2;102;102;102m*\u001b[39m \u001b[38;2;0;128;0mlen\u001b[39m(train_loader\u001b[38;2;102;102;102m.\u001b[39msampler) \u001b[38;2;102;102;102m/\u001b[39m \u001b[38;2;0;128;0mlen\u001b[39m(train_loader\u001b[38;2;102;102;102m.\u001b[39mdataset)\r\n",
      "    ))\r\n",
      "\r\n",
      "    logger\u001b[38;2;102;102;102m.\u001b[39mdebug(\u001b[38;2;186;33;33m\"\u001b[39m\u001b[38;2;186;33;33mProcesses \u001b[39m\u001b[38;2;164;90;119;01m{}\u001b[39;00m\u001b[38;2;186;33;33m/\u001b[39m\u001b[38;2;164;90;119;01m{}\u001b[39;00m\u001b[38;2;186;33;33m (\u001b[39m\u001b[38;2;164;90;119;01m{:.0f}\u001b[39;00m\u001b[38;2;186;33;33m%\u001b[39m\u001b[38;2;186;33;33m) of test data\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mformat(\r\n",
      "        \u001b[38;2;0;128;0mlen\u001b[39m(test_loader\u001b[38;2;102;102;102m.\u001b[39msampler), \u001b[38;2;0;128;0mlen\u001b[39m(test_loader\u001b[38;2;102;102;102m.\u001b[39mdataset),\r\n",
      "        \u001b[38;2;102;102;102m100.\u001b[39m \u001b[38;2;102;102;102m*\u001b[39m \u001b[38;2;0;128;0mlen\u001b[39m(test_loader\u001b[38;2;102;102;102m.\u001b[39msampler) \u001b[38;2;102;102;102m/\u001b[39m \u001b[38;2;0;128;0mlen\u001b[39m(test_loader\u001b[38;2;102;102;102m.\u001b[39mdataset)\r\n",
      "    ))\r\n",
      "\r\n",
      "    model \u001b[38;2;102;102;102m=\u001b[39m resnet20()\u001b[38;2;102;102;102m.\u001b[39mto(device)\r\n",
      "    \u001b[38;2;0;128;0;01mif\u001b[39;00m is_distributed \u001b[38;2;170;34;255;01mand\u001b[39;00m use_cuda:\r\n",
      "        \u001b[38;2;61;123;123;03m# multi-machine multi-gpu case\u001b[39;00m\r\n",
      "        model \u001b[38;2;102;102;102m=\u001b[39m torch\u001b[38;2;102;102;102m.\u001b[39mnn\u001b[38;2;102;102;102m.\u001b[39mparallel\u001b[38;2;102;102;102m.\u001b[39mDistributedDataParallel(model)\r\n",
      "    \u001b[38;2;0;128;0;01melse\u001b[39;00m:\r\n",
      "        \u001b[38;2;61;123;123;03m# single-machine multi-gpu case or single-machine or multi-machine cpu case\u001b[39;00m\r\n",
      "        model \u001b[38;2;102;102;102m=\u001b[39m torch\u001b[38;2;102;102;102m.\u001b[39mnn\u001b[38;2;102;102;102m.\u001b[39mDataParallel(model)\r\n",
      "\r\n",
      "    optimizer \u001b[38;2;102;102;102m=\u001b[39m optim\u001b[38;2;102;102;102m.\u001b[39mSGD(model\u001b[38;2;102;102;102m.\u001b[39mparameters(), lr\u001b[38;2;102;102;102m=\u001b[39margs\u001b[38;2;102;102;102m.\u001b[39mlr, momentum\u001b[38;2;102;102;102m=\u001b[39margs\u001b[38;2;102;102;102m.\u001b[39mmomentum)\r",
      "\r\n",
      "\r\n",
      "    \u001b[38;2;0;128;0;01mfor\u001b[39;00m epoch \u001b[38;2;170;34;255;01min\u001b[39;00m \u001b[38;2;0;128;0mrange\u001b[39m(\u001b[38;2;102;102;102m1\u001b[39m, args\u001b[38;2;102;102;102m.\u001b[39mepochs \u001b[38;2;102;102;102m+\u001b[39m \u001b[38;2;102;102;102m1\u001b[39m):\r\n",
      "        model\u001b[38;2;102;102;102m.\u001b[39mtrain()\r\n",
      "        \u001b[38;2;0;128;0;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;2;170;34;255;01min\u001b[39;00m \u001b[38;2;0;128;0menumerate\u001b[39m(train_loader, \u001b[38;2;102;102;102m1\u001b[39m):\r\n",
      "            data, target \u001b[38;2;102;102;102m=\u001b[39m data\u001b[38;2;102;102;102m.\u001b[39mto(device), target\u001b[38;2;102;102;102m.\u001b[39mto(device)\r\n",
      "            optimizer\u001b[38;2;102;102;102m.\u001b[39mzero_grad()\r\n",
      "            output \u001b[38;2;102;102;102m=\u001b[39m model(data)\r\n",
      "            loss \u001b[38;2;102;102;102m=\u001b[39m F\u001b[38;2;102;102;102m.\u001b[39mnll_loss(output, target)\r\n",
      "            loss\u001b[38;2;102;102;102m.\u001b[39mbackward()\r\n",
      "            \u001b[38;2;0;128;0;01mif\u001b[39;00m is_distributed \u001b[38;2;170;34;255;01mand\u001b[39;00m \u001b[38;2;170;34;255;01mnot\u001b[39;00m use_cuda:\r\n",
      "                \u001b[38;2;61;123;123;03m# average gradients manually for multi-machine cpu case only\u001b[39;00m\r\n",
      "                _average_gradients(model)\r\n",
      "            optimizer\u001b[38;2;102;102;102m.\u001b[39mstep()\r\n",
      "            \u001b[38;2;0;128;0;01mif\u001b[39;00m batch_idx \u001b[38;2;102;102;102m%\u001b[39m args\u001b[38;2;102;102;102m.\u001b[39mlog_interval \u001b[38;2;102;102;102m==\u001b[39m \u001b[38;2;102;102;102m0\u001b[39m:\r\n",
      "                logger\u001b[38;2;102;102;102m.\u001b[39minfo(\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mTrain Epoch: \u001b[39m\u001b[38;2;164;90;119;01m{}\u001b[39;00m\u001b[38;2;186;33;33m [\u001b[39m\u001b[38;2;164;90;119;01m{}\u001b[39;00m\u001b[38;2;186;33;33m/\u001b[39m\u001b[38;2;164;90;119;01m{}\u001b[39;00m\u001b[38;2;186;33;33m (\u001b[39m\u001b[38;2;164;90;119;01m{:.0f}\u001b[39;00m\u001b[38;2;186;33;33m%\u001b[39m\u001b[38;2;186;33;33m)] Loss: \u001b[39m\u001b[38;2;164;90;119;01m{:.6f}\u001b[39;00m\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mformat(\r\n",
      "                    epoch, batch_idx \u001b[38;2;102;102;102m*\u001b[39m \u001b[38;2;0;128;0mlen\u001b[39m(data), \u001b[38;2;0;128;0mlen\u001b[39m(train_loader\u001b[38;2;102;102;102m.\u001b[39msampler),\r\n",
      "                    \u001b[38;2;102;102;102m100.\u001b[39m \u001b[38;2;102;102;102m*\u001b[39m batch_idx \u001b[38;2;102;102;102m/\u001b[39m \u001b[38;2;0;128;0mlen\u001b[39m(train_loader), loss\u001b[38;2;102;102;102m.\u001b[39mitem()))\r\n",
      "    test(model, test_loader, device)\r\n",
      "    save_model(model, args\u001b[38;2;102;102;102m.\u001b[39mmodel_dir)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255mtest\u001b[39m(model, test_loader, device):\r\n",
      "    model\u001b[38;2;102;102;102m.\u001b[39meval()\r\n",
      "    test_loss \u001b[38;2;102;102;102m=\u001b[39m \u001b[38;2;102;102;102m0\u001b[39m\r\n",
      "    correct \u001b[38;2;102;102;102m=\u001b[39m \u001b[38;2;102;102;102m0\u001b[39m\r\n",
      "    \u001b[38;2;0;128;0;01mwith\u001b[39;00m torch\u001b[38;2;102;102;102m.\u001b[39mno_grad():\r\n",
      "        \u001b[38;2;0;128;0;01mfor\u001b[39;00m data, target \u001b[38;2;170;34;255;01min\u001b[39;00m test_loader:\r\n",
      "            data, target \u001b[38;2;102;102;102m=\u001b[39m data\u001b[38;2;102;102;102m.\u001b[39mto(device), target\u001b[38;2;102;102;102m.\u001b[39mto(device)\r\n",
      "            output \u001b[38;2;102;102;102m=\u001b[39m model(data)\r\n",
      "            test_loss \u001b[38;2;102;102;102m+\u001b[39m\u001b[38;2;102;102;102m=\u001b[39m F\u001b[38;2;102;102;102m.\u001b[39mnll_loss(output, target, size_average\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;0;128;0;01mFalse\u001b[39;00m)\u001b[38;2;102;102;102m.\u001b[39mitem()  \u001b[38;2;61;123;123;03m# sum up batch loss\u001b[39;00m\r\n",
      "            pred \u001b[38;2;102;102;102m=\u001b[39m output\u001b[38;2;102;102;102m.\u001b[39mmax(\u001b[38;2;102;102;102m1\u001b[39m, keepdim\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;0;128;0;01mTrue\u001b[39;00m)[\u001b[38;2;102;102;102m1\u001b[39m]  \u001b[38;2;61;123;123;03m# get the index of the max log-probability\u001b[39;00m\r\n",
      "            correct \u001b[38;2;102;102;102m+\u001b[39m\u001b[38;2;102;102;102m=\u001b[39m pred\u001b[38;2;102;102;102m.\u001b[39meq(target\u001b[38;2;102;102;102m.\u001b[39mview_as(pred))\u001b[38;2;102;102;102m.\u001b[39msum()\u001b[38;2;102;102;102m.\u001b[39mitem()\r\n",
      "\r\n",
      "    test_loss \u001b[38;2;102;102;102m/\u001b[39m\u001b[38;2;102;102;102m=\u001b[39m \u001b[38;2;0;128;0mlen\u001b[39m(test_loader\u001b[38;2;102;102;102m.\u001b[39mdataset)\r\n",
      "    logger\u001b[38;2;102;102;102m.\u001b[39minfo(\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mTest set: Average loss: \u001b[39m\u001b[38;2;164;90;119;01m{:.4f}\u001b[39;00m\u001b[38;2;186;33;33m, Accuracy: \u001b[39m\u001b[38;2;164;90;119;01m{}\u001b[39;00m\u001b[38;2;186;33;33m/\u001b[39m\u001b[38;2;164;90;119;01m{}\u001b[39;00m\u001b[38;2;186;33;33m (\u001b[39m\u001b[38;2;164;90;119;01m{:.0f}\u001b[39;00m\u001b[38;2;186;33;33m%\u001b[39m\u001b[38;2;186;33;33m)\u001b[39m\u001b[38;2;170;93;31;01m\\n\u001b[39;00m\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mformat(\r\n",
      "        test_loss, correct, \u001b[38;2;0;128;0mlen\u001b[39m(test_loader\u001b[38;2;102;102;102m.\u001b[39mdataset),\r\n",
      "        \u001b[38;2;102;102;102m100.\u001b[39m \u001b[38;2;102;102;102m*\u001b[39m correct \u001b[38;2;102;102;102m/\u001b[39m \u001b[38;2;0;128;0mlen\u001b[39m(test_loader\u001b[38;2;102;102;102m.\u001b[39mdataset)))\r\n",
      "\r\n",
      "\r\n",
      "\u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255mmodel_fn\u001b[39m(model_dir):\r\n",
      "    device \u001b[38;2;102;102;102m=\u001b[39m torch\u001b[38;2;102;102;102m.\u001b[39mdevice(\u001b[38;2;186;33;33m\"\u001b[39m\u001b[38;2;186;33;33mcuda\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m \u001b[38;2;0;128;0;01mif\u001b[39;00m torch\u001b[38;2;102;102;102m.\u001b[39mcuda\u001b[38;2;102;102;102m.\u001b[39mis_available() \u001b[38;2;0;128;0;01melse\u001b[39;00m \u001b[38;2;186;33;33m\"\u001b[39m\u001b[38;2;186;33;33mcpu\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m)\r\n",
      "    model \u001b[38;2;102;102;102m=\u001b[39m torch\u001b[38;2;102;102;102m.\u001b[39mnn\u001b[38;2;102;102;102m.\u001b[39mDataParallel(resnet20())\r\n",
      "    \u001b[38;2;0;128;0;01mwith\u001b[39;00m \u001b[38;2;0;128;0mopen\u001b[39m(os\u001b[38;2;102;102;102m.\u001b[39mpath\u001b[38;2;102;102;102m.\u001b[39mjoin(model_dir, \u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mmodel.pth\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m), \u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mrb\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m) \u001b[38;2;0;128;0;01mas\u001b[39;00m f:\r\n",
      "        model\u001b[38;2;102;102;102m.\u001b[39mload_state_dict(torch\u001b[38;2;102;102;102m.\u001b[39mload(f))\r\n",
      "    \u001b[38;2;0;128;0;01mreturn\u001b[39;00m model\u001b[38;2;102;102;102m.\u001b[39mto(device)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255msave_model\u001b[39m(model, model_dir):\r\n",
      "    logger\u001b[38;2;102;102;102m.\u001b[39minfo(\u001b[38;2;186;33;33m\"\u001b[39m\u001b[38;2;186;33;33mSaving the model.\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m)\r\n",
      "    path \u001b[38;2;102;102;102m=\u001b[39m os\u001b[38;2;102;102;102m.\u001b[39mpath\u001b[38;2;102;102;102m.\u001b[39mjoin(model_dir, \u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mmodel.pth\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m)\r\n",
      "    \u001b[38;2;61;123;123;03m# recommended way from http://pytorch.org/docs/master/notes/serialization.html\u001b[39;00m\r\n",
      "    torch\u001b[38;2;102;102;102m.\u001b[39msave(model\u001b[38;2;102;102;102m.\u001b[39mcpu()\u001b[38;2;102;102;102m.\u001b[39mstate_dict(), path)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[38;2;0;128;0;01mif\u001b[39;00m \u001b[38;2;25;23;124m__name__\u001b[39m \u001b[38;2;102;102;102m==\u001b[39m \u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33m__main__\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m:\r\n",
      "    parser \u001b[38;2;102;102;102m=\u001b[39m argparse\u001b[38;2;102;102;102m.\u001b[39mArgumentParser()\r\n",
      "\r\n",
      "    \u001b[38;2;61;123;123;03m# Data and model checkpoints directories\u001b[39;00m\r\n",
      "    parser\u001b[38;2;102;102;102m.\u001b[39madd_argument(\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33m--batch-size\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m, \u001b[38;2;0;128;0mtype\u001b[39m\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;0;128;0mint\u001b[39m, default\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m64\u001b[39m, metavar\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mN\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m,\r\n",
      "                        help\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33minput batch size for training (default: 64)\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m)\r\n",
      "    parser\u001b[38;2;102;102;102m.\u001b[39madd_argument(\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33m--test-batch-size\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m, \u001b[38;2;0;128;0mtype\u001b[39m\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;0;128;0mint\u001b[39m, default\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m1000\u001b[39m, metavar\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mN\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m,\r\n",
      "                        help\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33minput batch size for testing (default: 1000)\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m)\r\n",
      "    parser\u001b[38;2;102;102;102m.\u001b[39madd_argument(\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33m--epochs\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m, \u001b[38;2;0;128;0mtype\u001b[39m\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;0;128;0mint\u001b[39m, default\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m10\u001b[39m, metavar\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mN\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m,\r\n",
      "                        help\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mnumber of epochs to train (default: 10)\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m)\r\n",
      "    parser\u001b[38;2;102;102;102m.\u001b[39madd_argument(\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33m--lr\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m, \u001b[38;2;0;128;0mtype\u001b[39m\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;0;128;0mfloat\u001b[39m, default\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m0.01\u001b[39m, metavar\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mLR\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m,\r\n",
      "                        help\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mlearning rate (default: 0.01)\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m)\r\n",
      "    parser\u001b[38;2;102;102;102m.\u001b[39madd_argument(\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33m--momentum\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m, \u001b[38;2;0;128;0mtype\u001b[39m\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;0;128;0mfloat\u001b[39m, default\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m0.5\u001b[39m, metavar\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mM\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m,\r\n",
      "                        help\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mSGD momentum (default: 0.5)\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m)\r\n",
      "    parser\u001b[38;2;102;102;102m.\u001b[39madd_argument(\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33m--seed\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m, \u001b[38;2;0;128;0mtype\u001b[39m\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;0;128;0mint\u001b[39m, default\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m1\u001b[39m, metavar\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mS\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m,\r\n",
      "                        help\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mrandom seed (default: 1)\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m)\r\n",
      "    parser\u001b[38;2;102;102;102m.\u001b[39madd_argument(\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33m--log-interval\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m, \u001b[38;2;0;128;0mtype\u001b[39m\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;0;128;0mint\u001b[39m, default\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m100\u001b[39m, metavar\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mN\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m,\r\n",
      "                        help\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mhow many batches to wait before logging training status\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m)\r\n",
      "    parser\u001b[38;2;102;102;102m.\u001b[39madd_argument(\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33m--backend\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m, \u001b[38;2;0;128;0mtype\u001b[39m\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;0;128;0mstr\u001b[39m, default\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;0;128;0;01mNone\u001b[39;00m,\r\n",
      "                        help\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mbackend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m)\r\n",
      "\r\n",
      "    \u001b[38;2;61;123;123;03m# Container environment\u001b[39;00m\r\n",
      "    parser\u001b[38;2;102;102;102m.\u001b[39madd_argument(\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33m--hosts\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m, \u001b[38;2;0;128;0mtype\u001b[39m\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;0;128;0mlist\u001b[39m, default\u001b[38;2;102;102;102m=\u001b[39mjson\u001b[38;2;102;102;102m.\u001b[39mloads(os\u001b[38;2;102;102;102m.\u001b[39menviron[\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mSM_HOSTS\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m]))\r\n",
      "    parser\u001b[38;2;102;102;102m.\u001b[39madd_argument(\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33m--current-host\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m, \u001b[38;2;0;128;0mtype\u001b[39m\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;0;128;0mstr\u001b[39m, default\u001b[38;2;102;102;102m=\u001b[39mos\u001b[38;2;102;102;102m.\u001b[39menviron[\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mSM_CURRENT_HOST\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m])\r\n",
      "    parser\u001b[38;2;102;102;102m.\u001b[39madd_argument(\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33m--model-dir\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m, \u001b[38;2;0;128;0mtype\u001b[39m\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;0;128;0mstr\u001b[39m, default\u001b[38;2;102;102;102m=\u001b[39mos\u001b[38;2;102;102;102m.\u001b[39menviron[\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mSM_MODEL_DIR\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m])\r\n",
      "    parser\u001b[38;2;102;102;102m.\u001b[39madd_argument(\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33m--data-dir\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m, \u001b[38;2;0;128;0mtype\u001b[39m\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;0;128;0mstr\u001b[39m, default\u001b[38;2;102;102;102m=\u001b[39mos\u001b[38;2;102;102;102m.\u001b[39menviron[\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mSM_CHANNEL_TRAINING\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m])\r\n",
      "    parser\u001b[38;2;102;102;102m.\u001b[39madd_argument(\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33m--num-gpus\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m, \u001b[38;2;0;128;0mtype\u001b[39m\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;0;128;0mint\u001b[39m, default\u001b[38;2;102;102;102m=\u001b[39mos\u001b[38;2;102;102;102m.\u001b[39menviron[\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mSM_NUM_GPUS\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m])\r\n",
      "\r\n",
      "    train(parser\u001b[38;2;102;102;102m.\u001b[39mparse_args())\r\n"
     ]
    }
   ],
   "source": [
    "# Train Script\n",
    "!pygmentize ./src/resnet_train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d73c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Training Container\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point=\"resnet_train.py\",\n",
    "                    role=role,\n",
    "                    source_dir = \"src\",\n",
    "                    framework_version='1.12.0',\n",
    "                    py_version='py38',\n",
    "                    instance_count=1,\n",
    "                    instance_type='ml.m4.xlarge',\n",
    "                    hyperparameters={\n",
    "                        'batch-size':128,\n",
    "                        'lr': 0.01,\n",
    "                        'epochs': 1,\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b22e2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-20 08:30:34 Starting - Starting the training job...\n",
      "2022-08-20 08:31:00 Starting - Preparing the instances for trainingProfilerReport-1660984232: InProgress\n",
      "......\n",
      "2022-08-20 08:32:12 Downloading - Downloading input data......\n",
      "2022-08-20 08:33:12 Training - Downloading the training image......\n",
      "2022-08-20 08:34:18 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-08-20 08:34:15,963 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-08-20 08:34:15,965 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-08-20 08:34:15,974 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-08-20 08:34:15,983 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-08-20 08:34:16,685 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-08-20 08:34:16,705 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-08-20 08:34:16,718 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-08-20 08:34:16,732 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.m4.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 128,\n",
      "        \"epochs\": 1,\n",
      "        \"lr\": 0.01\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.m4.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"pytorch-training-2022-08-20-08-30-28-845\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-608095525235/pytorch-training-2022-08-20-08-30-28-845/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"resnet_train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m4.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m4.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"resnet_train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":128,\"epochs\":1,\"lr\":0.01}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=resnet_train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m4.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m4.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.m4.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m4.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=resnet_train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-608095525235/pytorch-training-2022-08-20-08-30-28-845/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.m4.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":128,\"epochs\":1,\"lr\":0.01},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m4.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"pytorch-training-2022-08-20-08-30-28-845\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-608095525235/pytorch-training-2022-08-20-08-30-28-845/source/sourcedir.tar.gz\",\"module_name\":\"resnet_train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m4.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m4.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"resnet_train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"128\",\"--epochs\",\"1\",\"--lr\",\"0.01\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=128\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.01\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220816-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 resnet_train.py --batch-size 128 --epochs 1 --lr 0.01\u001b[0m\n",
      "\u001b[34mDistributed training - False\u001b[0m\n",
      "\u001b[34mNumber of gpus available - 0\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mGet test data loader\u001b[0m\n",
      "\u001b[34mProcesses 50000/50000 (100%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34m[2022-08-20 08:34:21.068 algo-1:27 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220816-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220816-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[2022-08-20 08:34:21.828 algo-1:27 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-08-20 08:34:21.830 algo-1:27 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-08-20 08:34:21.831 algo-1:27 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-08-20 08:34:21.831 algo-1:27 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-08-20 08:34:21.831 algo-1:27 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mTrain Epoch: 1 [12800/50000 (26%)] Loss: -15.599797\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [12800/50000 (26%)] Loss: -15.599797\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [25600/50000 (51%)] Loss: -75.351624\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [25600/50000 (51%)] Loss: -75.351624\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [38400/50000 (77%)] Loss: -376.315094\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [38400/50000 (77%)] Loss: -376.315094\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\u001b[0m\n",
      "\u001b[34mTest set: Average loss: -1459.9684, Accuracy: 1851/10000 (19%)\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: -1459.9684, Accuracy: 1851/10000 (19%)\u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving the model.\u001b[0m\n",
      "\u001b[34m2022-08-20 08:40:13,274 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-08-20 08:40:13,274 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-08-20 08:40:13,275 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-08-20 08:40:39 Uploading - Uploading generated training model\n",
      "2022-08-20 08:40:39 Completed - Training job completed\n",
      "ProfilerReport-1660984232: NoIssuesFound\n",
      "Training seconds: 504\n",
      "Billable seconds: 504\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93ef11f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;0;128;0;01mimport\u001b[39;00m \u001b[38;2;0;0;255;01mtorch\u001b[39;00m\r\n",
      "\u001b[38;2;0;128;0;01mimport\u001b[39;00m \u001b[38;2;0;0;255;01mos\u001b[39;00m\r\n",
      "\r\n",
      "\u001b[38;2;0;128;0;01mimport\u001b[39;00m \u001b[38;2;0;0;255;01msys\u001b[39;00m\r\n",
      "\u001b[38;2;0;128;0;01mimport\u001b[39;00m \u001b[38;2;0;0;255;01mtorch\u001b[39;00m\r\n",
      "\u001b[38;2;0;128;0;01mimport\u001b[39;00m \u001b[38;2;0;0;255;01mtorch\u001b[39;00m\u001b[38;2;0;0;255;01m.\u001b[39;00m\u001b[38;2;0;0;255;01mnn\u001b[39;00m \u001b[38;2;0;128;0;01mas\u001b[39;00m \u001b[38;2;0;0;255;01mnn\u001b[39;00m\r\n",
      "\u001b[38;2;0;128;0;01mimport\u001b[39;00m \u001b[38;2;0;0;255;01mtorch\u001b[39;00m\u001b[38;2;0;0;255;01m.\u001b[39;00m\u001b[38;2;0;0;255;01mnn\u001b[39;00m\u001b[38;2;0;0;255;01m.\u001b[39;00m\u001b[38;2;0;0;255;01mfunctional\u001b[39;00m \u001b[38;2;0;128;0;01mas\u001b[39;00m \u001b[38;2;0;0;255;01mF\u001b[39;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[38;2;61;123;123;03m# Based on https://github.com/pytorch/examples/blob/master/mnist/main.py\u001b[39;00m\r\n",
      "\u001b[38;2;0;128;0;01mclass\u001b[39;00m \u001b[38;2;0;0;255;01mBasicBlock\u001b[39;00m(nn\u001b[38;2;102;102;102m.\u001b[39mModule):\r\n",
      "    expansion \u001b[38;2;102;102;102m=\u001b[39m \u001b[38;2;102;102;102m1\u001b[39m\r\n",
      "\r\n",
      "    \u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255m__init__\u001b[39m(\u001b[38;2;0;128;0mself\u001b[39m, in_planes, planes, stride\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m1\u001b[39m, option\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mA\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m):\r\n",
      "        \u001b[38;2;0;128;0msuper\u001b[39m(BasicBlock, \u001b[38;2;0;128;0mself\u001b[39m)\u001b[38;2;102;102;102m.\u001b[39m\u001b[38;2;0;0;255m__init__\u001b[39m()\r\n",
      "        \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mconv1 \u001b[38;2;102;102;102m=\u001b[39m nn\u001b[38;2;102;102;102m.\u001b[39mConv2d(in_planes, planes, kernel_size\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m3\u001b[39m, stride\u001b[38;2;102;102;102m=\u001b[39mstride, padding\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m1\u001b[39m, bias\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;0;128;0;01mFalse\u001b[39;00m)\r\n",
      "        \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mbn1 \u001b[38;2;102;102;102m=\u001b[39m nn\u001b[38;2;102;102;102m.\u001b[39mBatchNorm2d(planes)\r\n",
      "        \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mconv2 \u001b[38;2;102;102;102m=\u001b[39m nn\u001b[38;2;102;102;102m.\u001b[39mConv2d(planes, planes, kernel_size\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m3\u001b[39m, stride\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m1\u001b[39m, padding\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m1\u001b[39m, bias\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;0;128;0;01mFalse\u001b[39;00m)\r\n",
      "        \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mbn2 \u001b[38;2;102;102;102m=\u001b[39m nn\u001b[38;2;102;102;102m.\u001b[39mBatchNorm2d(planes)\r\n",
      "        \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mshortcut \u001b[38;2;102;102;102m=\u001b[39m nn\u001b[38;2;102;102;102m.\u001b[39mSequential(\r\n",
      "                nn\u001b[38;2;102;102;102m.\u001b[39mConv2d(in_planes, \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mexpansion \u001b[38;2;102;102;102m*\u001b[39m planes, kernel_size\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m1\u001b[39m, stride\u001b[38;2;102;102;102m=\u001b[39mstride, bias\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;0;128;0;01mFalse\u001b[39;00m),\r\n",
      "                nn\u001b[38;2;102;102;102m.\u001b[39mBatchNorm2d(\u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mexpansion \u001b[38;2;102;102;102m*\u001b[39m planes)\r\n",
      "        )\r\n",
      "\r\n",
      "    \u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255mforward\u001b[39m(\u001b[38;2;0;128;0mself\u001b[39m, x):\r\n",
      "        out \u001b[38;2;102;102;102m=\u001b[39m F\u001b[38;2;102;102;102m.\u001b[39mrelu(\u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mbn1(\u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mconv1(x)))\r\n",
      "        out \u001b[38;2;102;102;102m=\u001b[39m \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mbn2(\u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mconv2(out))\r\n",
      "        out \u001b[38;2;102;102;102m+\u001b[39m\u001b[38;2;102;102;102m=\u001b[39m \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mshortcut(x)\r\n",
      "        out \u001b[38;2;102;102;102m=\u001b[39m F\u001b[38;2;102;102;102m.\u001b[39mrelu(out)\r\n",
      "        \u001b[38;2;0;128;0;01mreturn\u001b[39;00m out\r\n",
      "    \r\n",
      "\u001b[38;2;0;128;0;01mclass\u001b[39;00m \u001b[38;2;0;0;255;01mResNet\u001b[39;00m(nn\u001b[38;2;102;102;102m.\u001b[39mModule):\r\n",
      "    \u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255m__init__\u001b[39m(\u001b[38;2;0;128;0mself\u001b[39m, block, num_blocks, num_classes\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m10\u001b[39m):\r\n",
      "        \u001b[38;2;0;128;0msuper\u001b[39m(ResNet, \u001b[38;2;0;128;0mself\u001b[39m)\u001b[38;2;102;102;102m.\u001b[39m\u001b[38;2;0;0;255m__init__\u001b[39m()\r\n",
      "        \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39min_planes \u001b[38;2;102;102;102m=\u001b[39m \u001b[38;2;102;102;102m16\u001b[39m\r\n",
      "        \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mconv1 \u001b[38;2;102;102;102m=\u001b[39m nn\u001b[38;2;102;102;102m.\u001b[39mConv2d(\u001b[38;2;102;102;102m3\u001b[39m, \u001b[38;2;102;102;102m16\u001b[39m, kernel_size\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m3\u001b[39m, stride\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m1\u001b[39m, padding\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m1\u001b[39m, bias\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;0;128;0;01mFalse\u001b[39;00m)\r\n",
      "        \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mbn1 \u001b[38;2;102;102;102m=\u001b[39m nn\u001b[38;2;102;102;102m.\u001b[39mBatchNorm2d(\u001b[38;2;102;102;102m16\u001b[39m)\r\n",
      "        \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mlayer1 \u001b[38;2;102;102;102m=\u001b[39m \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39m_make_layer(block, \u001b[38;2;102;102;102m16\u001b[39m, num_blocks[\u001b[38;2;102;102;102m0\u001b[39m], stride\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m1\u001b[39m)\r\n",
      "        \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mlayer2 \u001b[38;2;102;102;102m=\u001b[39m \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39m_make_layer(block, \u001b[38;2;102;102;102m32\u001b[39m, num_blocks[\u001b[38;2;102;102;102m1\u001b[39m], stride\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m2\u001b[39m)\r\n",
      "        \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mlayer3 \u001b[38;2;102;102;102m=\u001b[39m \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39m_make_layer(block, \u001b[38;2;102;102;102m64\u001b[39m, num_blocks[\u001b[38;2;102;102;102m2\u001b[39m], stride\u001b[38;2;102;102;102m=\u001b[39m\u001b[38;2;102;102;102m2\u001b[39m)\r\n",
      "        \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mlinear \u001b[38;2;102;102;102m=\u001b[39m nn\u001b[38;2;102;102;102m.\u001b[39mLinear(\u001b[38;2;102;102;102m64\u001b[39m, num_classes)\r\n",
      "        \r\n",
      "    \u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255m_make_layer\u001b[39m(\u001b[38;2;0;128;0mself\u001b[39m, block, planes, num_blocks, stride):\r\n",
      "        strides \u001b[38;2;102;102;102m=\u001b[39m [stride] \u001b[38;2;102;102;102m+\u001b[39m [\u001b[38;2;102;102;102m1\u001b[39m]\u001b[38;2;102;102;102m*\u001b[39m(num_blocks\u001b[38;2;102;102;102m-\u001b[39m\u001b[38;2;102;102;102m1\u001b[39m)\r\n",
      "        layers \u001b[38;2;102;102;102m=\u001b[39m []\r\n",
      "        \u001b[38;2;0;128;0;01mfor\u001b[39;00m stride \u001b[38;2;170;34;255;01min\u001b[39;00m strides:\r\n",
      "            layers\u001b[38;2;102;102;102m.\u001b[39mappend(block(\u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39min_planes, planes, stride))\r\n",
      "            \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39min_planes \u001b[38;2;102;102;102m=\u001b[39m planes \u001b[38;2;102;102;102m*\u001b[39m block\u001b[38;2;102;102;102m.\u001b[39mexpansion\r\n",
      "\r\n",
      "        \u001b[38;2;0;128;0;01mreturn\u001b[39;00m nn\u001b[38;2;102;102;102m.\u001b[39mSequential(\u001b[38;2;102;102;102m*\u001b[39mlayers)\r\n",
      "\r\n",
      "    \u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255mforward\u001b[39m(\u001b[38;2;0;128;0mself\u001b[39m, x):\r\n",
      "        out \u001b[38;2;102;102;102m=\u001b[39m F\u001b[38;2;102;102;102m.\u001b[39mrelu(\u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mbn1(\u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mconv1(x)))\r\n",
      "        out \u001b[38;2;102;102;102m=\u001b[39m \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mlayer1(out)\r\n",
      "        out \u001b[38;2;102;102;102m=\u001b[39m \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mlayer2(out)\r\n",
      "        out \u001b[38;2;102;102;102m=\u001b[39m \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mlayer3(out)\r\n",
      "        out \u001b[38;2;102;102;102m=\u001b[39m F\u001b[38;2;102;102;102m.\u001b[39mavg_pool2d(out, out\u001b[38;2;102;102;102m.\u001b[39msize()[\u001b[38;2;102;102;102m3\u001b[39m])\r\n",
      "        out \u001b[38;2;102;102;102m=\u001b[39m out\u001b[38;2;102;102;102m.\u001b[39mview(out\u001b[38;2;102;102;102m.\u001b[39msize(\u001b[38;2;102;102;102m0\u001b[39m), \u001b[38;2;102;102;102m-\u001b[39m\u001b[38;2;102;102;102m1\u001b[39m)\r\n",
      "        out \u001b[38;2;102;102;102m=\u001b[39m \u001b[38;2;0;128;0mself\u001b[39m\u001b[38;2;102;102;102m.\u001b[39mlinear(out)\r\n",
      "        \u001b[38;2;0;128;0;01mreturn\u001b[39;00m out\r\n",
      "\r\n",
      "\u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255mresnet20\u001b[39m():\r\n",
      "    \u001b[38;2;0;128;0;01mreturn\u001b[39;00m ResNet(BasicBlock, [\u001b[38;2;102;102;102m3\u001b[39m, \u001b[38;2;102;102;102m3\u001b[39m, \u001b[38;2;102;102;102m3\u001b[39m])\r\n",
      "\r\n",
      "    \r\n",
      "\u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255mmodel_fn\u001b[39m(model_dir):\r\n",
      "    device \u001b[38;2;102;102;102m=\u001b[39m torch\u001b[38;2;102;102;102m.\u001b[39mdevice(\u001b[38;2;186;33;33m\"\u001b[39m\u001b[38;2;186;33;33mcuda\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m \u001b[38;2;0;128;0;01mif\u001b[39;00m torch\u001b[38;2;102;102;102m.\u001b[39mcuda\u001b[38;2;102;102;102m.\u001b[39mis_available() \u001b[38;2;0;128;0;01melse\u001b[39;00m \u001b[38;2;186;33;33m\"\u001b[39m\u001b[38;2;186;33;33mcpu\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m)\r\n",
      "    model \u001b[38;2;102;102;102m=\u001b[39m torch\u001b[38;2;102;102;102m.\u001b[39mnn\u001b[38;2;102;102;102m.\u001b[39mDataParallel(resnet20())\r\n",
      "    \u001b[38;2;0;128;0;01mwith\u001b[39;00m \u001b[38;2;0;128;0mopen\u001b[39m(os\u001b[38;2;102;102;102m.\u001b[39mpath\u001b[38;2;102;102;102m.\u001b[39mjoin(model_dir, \u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mmodel.pth\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m), \u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mrb\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m) \u001b[38;2;0;128;0;01mas\u001b[39;00m f:\r\n",
      "        model\u001b[38;2;102;102;102m.\u001b[39mload_state_dict(torch\u001b[38;2;102;102;102m.\u001b[39mload(f))\r\n",
      "    \u001b[38;2;0;128;0;01mreturn\u001b[39;00m model\u001b[38;2;102;102;102m.\u001b[39mto(device)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./src/resnet_deploy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "11a047a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "model_data = 's3://sagemaker-us-west-2-608095525235/pytorch-training-2022-08-20-08-30-28-845/output/model.tar.gz'\n",
    "inference_inputs = 's3://sagemaker-han/sagemaker/batch_transform'\n",
    "output_s3_path = 'https://sagemaker-han.s3.us-west-2.amazonaws.com/sagemaker/batch_transform_output_one_predict'\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data = model_data,\n",
    "                             entry_point='resnet_train.py',\n",
    "                             source_dir = 'src',\n",
    "                             framework_version='1.12.0',\n",
    "                             py_version='py38',\n",
    "                             role = role)\n",
    "predictor = pytorch_model.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "421c26fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: [8, 6, 6, 8, 8, 8, 6, 6, 8, 8, 6, 8, 6, 8, 6, 6, 6, 6, 8, 6, 8, 6, 8, 6, 6, 8, 6, 6, 8, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 6, 8, 8, 6, 6, 6, 8, 6, 8, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 6, 8, 8, 8, 6, 6, 8, 8, 8, 6, 8, 6, 6, 6, 6, 6, 6, 8, 6, 6, 6, 8, 6, 6, 6, 6, 8, 6, 8, 8, 8, 6, 8, 8, 6, 6, 6, 6, 6, 6, 6, 8, 6, 6, 6, 6, 6, 8, 6, 6, 6, 8, 6, 8, 6, 6, 6, 6, 8, 8, 8, 6, 6, 6, 6, 8, 6, 6, 6, 6, 6, 8, 6, 6, 6, 6, 6, 6, 8, 6, 6, 8, 8, 6, 6, 6, 8, 6, 8, 6, 8, 6, 6, 6, 8, 8, 8, 6, 8, 8, 6, 6, 8, 6, 6, 8, 6, 8, 6, 6, 6, 8, 8, 6, 8, 8, 6, 6, 6, 8, 8, 6, 8, 6, 8, 6, 6, 6, 8, 6, 8, 8, 6, 6, 8, 6, 8, 8, 8, 8, 8, 6, 6, 6, 8, 8, 8, 8, 6, 8, 8, 8, 8, 6, 6, 6, 8, 6, 6, 8, 6, 6, 8, 6, 6, 8, 6, 6, 6, 6, 8, 6, 6, 6, 8, 6, 6, 4, 6, 6, 8, 6, 6, 8, 8, 6, 6, 6, 8, 8, 6, 6, 8, 6, 8, 6, 8, 6, 6, 8, 8, 8, 6, 8, 6, 6, 8, 6, 6, 8, 8, 8, 6, 6, 6, 6, 8, 6, 8, 6, 6, 6, 6, 6, 6, 8, 6, 8, 8, 6, 8, 8, 8, 8, 6, 6, 8, 8, 6, 8, 8, 6, 8, 6, 8, 6, 8, 8, 8, 6, 8, 8, 6, 6, 6, 8, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 6, 6, 6, 6, 6, 8, 8, 6, 8, 6, 6, 6, 8, 6, 6, 8, 6, 6, 6, 8, 6, 8, 6, 6, 6, 8, 8, 6, 8, 6, 6, 6, 8, 8, 8, 6, 8, 6, 8, 6, 6, 6, 8, 8, 8, 6, 6, 6, 8, 8, 8, 6, 6, 6, 6, 8, 6, 8, 8, 6, 6, 6, 6, 8, 6, 6, 8, 8, 6, 8, 8, 6, 6, 6, 8, 6, 8, 6, 6, 6, 6, 6, 8, 8, 8, 8, 6, 8, 8, 6, 8, 8, 6, 6, 8, 6, 6, 6, 8, 6, 6, 8, 6, 6, 8, 6, 6, 8, 8, 8, 8, 6, 6, 8, 8, 8, 8, 8, 6, 6, 8, 8, 6, 6, 6, 6, 6, 6, 6, 6, 8, 6, 8, 6, 6, 6, 6, 6, 6, 6, 8, 8, 6, 6, 8, 8, 6, 6, 8, 8, 8, 6, 8, 6, 8, 6, 6, 8, 6, 6, 6, 8, 6, 6, 8, 6, 8, 8, 8, 6, 8, 8, 6, 6, 8, 6, 6, 6, 6, 6, 8, 6, 6, 6, 8, 6, 6, 6, 8, 8, 8, 8, 6, 4, 8, 6, 6, 6, 8, 6, 6, 6, 8, 6, 6, 8, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 6, 8, 6, 6, 6, 6, 6, 6, 8, 6, 8, 8, 6, 8, 8, 6, 8, 6, 8, 6, 6, 8, 6, 4, 6, 8, 8, 8, 8, 8, 6, 8, 8, 6, 8, 8, 8, 8, 6, 8, 6, 8, 8, 8, 6, 6, 6, 6, 6, 6, 6, 8, 6, 6, 6, 8, 8, 6, 6, 6, 6, 6, 6, 8, 6, 4, 6, 8, 8, 6, 6, 8, 8, 6, 8, 6, 6, 8, 6, 8, 6, 8, 6, 8, 6, 6, 8, 6, 8, 8, 6, 6, 8, 8, 6, 6, 8, 6, 6, 6, 6, 8, 6, 8, 6, 6, 8, 6, 4, 8, 8, 6, 8, 6, 8, 6, 8, 6, 6, 6, 8, 8, 8, 8, 8, 8, 8, 6, 6, 8, 8, 6, 8, 6, 8, 6, 6, 6, 6, 6, 6, 6, 8, 6, 6, 6, 8, 6, 6, 6, 8, 6, 6, 6, 8, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 6, 8, 8, 8, 8, 6, 8, 6, 6, 6, 8, 6, 8, 8, 6, 6, 6, 8, 6, 6, 8, 6, 8, 6, 8, 8, 6, 6, 6, 8, 8, 6, 6, 6, 6, 6, 6, 8, 6, 8, 8, 6, 8, 6, 6, 6, 8, 8, 6, 6, 6, 8, 8, 6, 6, 8, 6, 6, 8, 6, 8, 6, 6, 8, 8, 8, 6, 6, 8, 6, 6, 6, 6, 8, 6, 6, 8, 6, 6, 6, 8, 6, 8, 6, 6, 8, 8, 8, 8, 6, 6, 8, 6, 6, 8, 8, 6, 8, 6, 8, 6, 6, 8, 8, 8, 8, 8, 6, 8, 6, 8, 6, 6, 8, 8, 6, 8, 6, 8, 6, 8, 8, 6, 8, 6, 6, 8, 6, 8, 6, 8, 6, 6, 6, 6, 8, 8, 8, 6, 8, 8, 6, 6, 8, 6, 8, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 6, 6, 6, 8, 6, 8, 6, 6, 8, 6, 6, 6, 6, 6, 6, 6, 6, 8, 6, 6, 8, 6, 6, 6, 8, 6, 6, 6, 6, 8, 6, 8, 6, 8, 6, 6, 8, 6, 8, 8, 8, 6, 8, 8, 6, 8, 6, 6, 6, 6, 6, 8, 6, 8, 6, 6, 6, 6, 6, 8, 6, 6, 6, 8, 8, 6, 6, 6, 6, 8, 6, 8, 6, 6, 6, 6, 8, 6, 6, 6, 6, 8, 8, 6, 6, 4, 8, 8, 6, 8, 6, 6, 6, 6, 8, 6, 6, 8, 8, 6, 8, 8, 8, 6, 6, 8, 6, 8, 8, 8, 8, 8, 6, 8, 6, 8, 6, 6, 8, 6, 8, 6, 8, 8, 6, 6, 6, 8, 6, 6, 8, 6, 6, 6, 6, 6, 8, 8, 6, 6, 8, 8, 6, 8, 8, 6, 8, 8, 8, 8, 8, 6, 8, 6, 8, 8, 8, 6, 8, 8, 6, 8, 6, 6, 6, 6, 8, 6, 6, 8, 6, 6, 6, 8, 6, 6, 6, 6, 8, 6, 6, 6, 8, 6, 6, 8, 8, 8, 6, 6, 6, 6, 6, 6, 8, 6, 6, 8, 6, 8, 8, 6, 8, 6, 8, 8, 6, 6, 6, 8, 6, 8, 8, 8, 6, 8, 8, 8, 6, 8, 8, 8, 6, 6, 6, 8, 6]\n",
      "Ground Truth:     [8 2 1 ... 7 2 3]\n"
     ]
    }
   ],
   "source": [
    "# Select 5 images and labels from test set\n",
    "import numpy as np\n",
    "n_sample = 1100\n",
    "n_test = len(test_data)\n",
    "sampled_index = np.random.choice(n_test, size=n_sample, replace=False)\n",
    "sampled_testimg = np.array([test_data[i][0].numpy() for i in sampled_index])\n",
    "sampled_testlabel = np.array([test_data[i][1] for i in sampled_index])\n",
    "\n",
    "# Run inference and pick up the most likely label based on the score\n",
    "labels = []\n",
    "for split_sample in np.array_split(sampled_testimg,3):\n",
    "    prediction_scores = predictor.predict(split_sample)\n",
    "    labels += np.argmax(prediction_scores,axis = 1).tolist()\n",
    "    \n",
    "print(\"Predicted labels: {}\".format(labels))\n",
    "print(\"Ground Truth:     {}\".format(sampled_testlabel))\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d938b0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_dataloaded = test_data_loaded[0][:1100].numpy()\n",
    "# sampled_1100_path = './data/sampled_1100.npy'\n",
    "# np.save(sampled_1100_path,sample_dataloaded)\n",
    "\n",
    "# prefix = 'sagemaker-han/sagemaker/cifar10'\n",
    "\n",
    "# new_input = sagemaker_session.upload_data(path=sampled_1100_path, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "2c46ba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datset files\n",
    "dataset_jsonl_file=\"./data/cifar10.jsonl\"\n",
    "\n",
    "with open(dataset_jsonl_file, \"w+\") as outfile:\n",
    "    for row in sample_dataloaded:\n",
    "        json.dump(row.tolist(), outfile)\n",
    "        outfile.write('\\n')\n",
    "        \n",
    "prefix = 'sagemaker-han/sagemaker/cifar10'\n",
    "\n",
    "new_input = sagemaker_session.upload_data(path=dataset_jsonl_file, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1763d5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jsonlines in /Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages (3.1.0)\r\n",
      "Requirement already satisfied: attrs>=19.2.0 in /Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages (from jsonlines) (22.1.0)\r\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "pop expected at most 1 argument, got 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [165]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m reader:\n\u001b[1;32m      8\u001b[0m         inputs\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray(obj))\n\u001b[0;32m----> 9\u001b[0m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: pop expected at most 1 argument, got 2"
     ]
    }
   ],
   "source": [
    "!pip install jsonlines\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "tmp = './data/test_img_path.json'\n",
    "inputs = []\n",
    "with jsonlines.open(dataset_jsonl_file) as reader:\n",
    "    for obj in reader:\n",
    "        inputs.append(np.array(obj))\n",
    "inputs.pop(0)\n",
    "print(len(inputs))\n",
    "print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ce104dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = 's3://sagemaker-us-west-2-608095525235/pytorch-training-2022-08-20-08-30-28-845/output/model.tar.gz'\n",
    "inference_inputs = 's3://sagemaker-han/sagemaker/batch_transform'\n",
    "output_s3_path = 'https://sagemaker-han.s3.us-west-2.amazonaws.com/sagemaker/batch_transform_output_one_predict'\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data = model_data,\n",
    "                             entry_point='resnet_deploy.py',\n",
    "                             source_dir = 'src',\n",
    "                             framework_version='1.12.0',\n",
    "                             py_version='py38',\n",
    "                             role = role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "56cbeaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Tranform\n",
    "\n",
    "max_concurrent_transforms = None\n",
    "max_payload = None\n",
    "\n",
    "inference_inputs = 's3://sagemaker-han/sagemaker/batch_transform'\n",
    "output_s3_path = 'https://sagemaker-han.s3.us-west-2.amazonaws.com/sagemaker/batch_transform_output_cifar10_{}_{}'.format(max_concurrent_transforms,max_payload)\n",
    "\n",
    "transformer = pytorch_model.transformer(instance_count=1, \n",
    "                                        instance_type=\"ml.m4.xlarge\",\n",
    "                                        output_path=output_s3_path,\n",
    "                                        max_concurrent_transforms = max_concurrent_transforms,\n",
    "                                        max_payload = max_payload\n",
    "                                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b12eee61",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................\u001b[34mWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:13,468 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:13,601 [INFO ] main org.pytorch.serve.ModelServer - \u001b[0m\n",
      "\u001b[34mTorchserve version: 0.6.0\u001b[0m\n",
      "\u001b[34mTS Home: /opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mCurrent directory: /\u001b[0m\n",
      "\u001b[34mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[34mNumber of GPUs: 0\u001b[0m\n",
      "\u001b[34mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[34mMax heap size: 3088 M\u001b[0m\n",
      "\u001b[34mPython executable: /opt/conda/bin/python3.8\u001b[0m\n",
      "\u001b[34mConfig file: /etc/sagemaker-ts.properties\u001b[0m\n",
      "\u001b[34mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mMetrics address: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mInitial Models: model=/opt/ml/model\u001b[0m\n",
      "\u001b[34mLog dir: /logs\u001b[0m\n",
      "\u001b[34mMetrics dir: /logs\u001b[0m\n",
      "\u001b[34mNetty threads: 0\u001b[0m\n",
      "\u001b[34mNetty client threads: 0\u001b[0m\n",
      "\u001b[34mDefault workers per model: 4\u001b[0m\n",
      "\u001b[34mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[34mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[34mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[34mLimit Maximum Image Pixels: true\u001b[0m\n",
      "\u001b[34mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[34mAllowed Urls: [file://.*|http(s)?://.*]\u001b[0m\n",
      "\u001b[34mCustom python dependency for model allowed: false\u001b[0m\n",
      "\u001b[34mMetrics report format: prometheus\u001b[0m\n",
      "\u001b[34mEnable metrics API: true\u001b[0m\n",
      "\u001b[34mWorkflow Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mModel config: N/A\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:13,616 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:13,666 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:13,672 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:13,673 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:13,676 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:13,713 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:14,098 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:14,100 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:14,112 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel server started.\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,080 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,285 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102415\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,299 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:48.20623779296875|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102415\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,300 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:7.658893585205078|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102415\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,301 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:13.7|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102415\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,302 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:14766.35546875|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102415\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,303 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:944.6015625|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102415\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,304 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:7.9|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102415\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,635 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,637 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]64\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,643 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,648 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]60\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,649 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,650 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,651 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,653 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,657 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,660 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,682 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,686 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]63\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,688 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,689 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,688 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,697 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,700 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]65\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,701 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,701 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,701 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,703 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,708 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,715 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,716 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,720 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661102415720\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,720 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661102415720\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,720 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661102415720\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,723 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661102415723\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,930 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,975 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,971 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:15,991 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:16,675 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 706\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:16,677 [INFO ] W-9000-model_1.0 TS_METRICS - W-9000-model_1.0.ms:2987|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102416\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:16,678 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:249|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102416\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:16,759 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 831\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:16,768 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 801\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:16,759 [INFO ] W-9002-model_1.0 TS_METRICS - W-9002-model_1.0.ms:3057|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102416\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:16,771 [INFO ] W-9002-model_1.0 TS_METRICS - WorkerThreadTime.ms:220|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102416\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:16,768 [INFO ] W-9003-model_1.0 TS_METRICS - W-9003-model_1.0.ms:3065|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102416\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:16,772 [INFO ] W-9003-model_1.0 TS_METRICS - WorkerThreadTime.ms:251|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102416\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:16,814 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 811\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:16,815 [INFO ] W-9001-model_1.0 TS_METRICS - W-9001-model_1.0.ms:3121|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102416\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:16,816 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:285|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102416\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m2022-08-21T17:20:20,876 [INFO ] pool-2-thread-5 ACCESS_LOG - /169.254.255.130:33978 \"GET /ping HTTP/1.1\" 200 35\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:20,878 [INFO ] pool-2-thread-5 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102420\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:20,907 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:33982 \"GET /execution-parameters HTTP/1.1\" 404 2\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:20,910 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102420\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:20,876 [INFO ] pool-2-thread-5 ACCESS_LOG - /169.254.255.130:33978 \"GET /ping HTTP/1.1\" 200 35\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:20,878 [INFO ] pool-2-thread-5 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102420\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:20,907 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:33982 \"GET /execution-parameters HTTP/1.1\" 404 2\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:20,910 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102420\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:22,655 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661102422655\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:22,655 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661102422655\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:22,669 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1661102422\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:22,673 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - transform-han\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:22,677 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:7.06|#ModelName:model,Level:Model|#hostname:56a3d289cfe0,requestID:5bf6a644-5608-49bd-8431-b43f79f57f93,timestamp:1661102422\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:22,678 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 9\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:22,679 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:49118 \"POST /invocations HTTP/1.1\" 500 49\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:22,679 [INFO ] W-9000-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102420\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:22,679 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102422\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:22,669 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1661102422\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:22,673 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - transform-han\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:22,677 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:7.06|#ModelName:model,Level:Model|#hostname:56a3d289cfe0,requestID:5bf6a644-5608-49bd-8431-b43f79f57f93,timestamp:1661102422\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:22,678 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 9\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:22,679 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:49118 \"POST /invocations HTTP/1.1\" 500 49\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:22,679 [INFO ] W-9000-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102420\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:22,679 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102422\u001b[0m\n",
      "\u001b[32m2022-08-21T17:20:20.919:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[32m2022-08-21T17:20:23.063:[sagemaker logs]: sagemaker-han/sagemaker-han/sagemaker/cifar10/cifar10.jsonl: Bad HTTP status received from algorithm: 500\u001b[0m\n",
      "\u001b[32m2022-08-21T17:20:23.064:[sagemaker logs]: sagemaker-han/sagemaker-han/sagemaker/cifar10/cifar10.jsonl: \u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:22,680 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:16|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102422\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:22,790 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661102422790\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:22,806 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend received inference at: 1661102422\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:22,818 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - transform-han\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:22,680 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:16|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102422\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:22,790 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661102422790\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:22,806 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend received inference at: 1661102422\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:22,818 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - transform-han\u001b[0m\n",
      "\u001b[32m2022-08-21T17:20:23.064:[sagemaker logs]: sagemaker-han/sagemaker-han/sagemaker/cifar10/cifar10.jsonl: Message:\u001b[0m\n",
      "\u001b[32m2022-08-21T17:20:23.064:[sagemaker logs]: sagemaker-han/sagemaker-han/sagemaker/cifar10/cifar10.jsonl: Extra data: line 2 column 1 (char 65293)\u001b[0m\n",
      "\u001b[32m2022-08-21T17:20:23.064:[sagemaker logs]: sagemaker-han/sagemaker-han/sagemaker/cifar10/cifar10.jsonl: Traceback (most recent call last):\u001b[0m\n",
      "\u001b[32m2022-08-21T17:20:23.064:[sagemaker logs]: sagemaker-han/sagemaker-han/sagemaker/cifar10/cifar10.jsonl:   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 128, in transform\u001b[0m\n",
      "\u001b[32m2022-08-21T17:20:23.064:[sagemaker logs]: sagemaker-han/sagemaker-han/sagemaker/cifar10/cifar10.jsonl:     result = self._transform_fn(self._model, input_data, content_type, accept)\u001b[0m\n",
      "\u001b[32m2022-08-21T17:20:23.064:[sagemaker logs]: sagemaker-han/sagemaker-han/sagemaker/cifar10/cifar10.jsonl:   File \"/opt/ml/model/code/resnet_deploy.py\", line 132, in transform_fn\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:22,824 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 19\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:22,824 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.255.130:49126 \"POST /invocations HTTP/1.1\" 500 67\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:22,825 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102420\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:22,825 [INFO ] W-9002-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102422\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:22,826 [INFO ] W-9002-model_1.0 TS_METRICS - WorkerThreadTime.ms:17|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102422\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:22,824 [INFO ] W-9002-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:17.86|#ModelName:model,Level:Model|#hostname:56a3d289cfe0,requestID:05270d1b-805f-42d0-818f-290124cbf216,timestamp:1661102422\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:22,824 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 19\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:22,824 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.255.130:49126 \"POST /invocations HTTP/1.1\" 500 67\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:22,825 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102420\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:22,825 [INFO ] W-9002-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102422\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:22,826 [INFO ] W-9002-model_1.0 TS_METRICS - WorkerThreadTime.ms:17|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102422\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:22,824 [INFO ] W-9002-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:17.86|#ModelName:model,Level:Model|#hostname:56a3d289cfe0,requestID:05270d1b-805f-42d0-818f-290124cbf216,timestamp:1661102422\u001b[0m\n",
      "\u001b[32m2022-08-21T17:20:23.064:[sagemaker logs]: sagemaker-han/sagemaker-han/sagemaker/cifar10/cifar10.jsonl:     processed_data = preprocess(input_data, content_type)\u001b[0m\n",
      "\u001b[32m2022-08-21T17:20:23.065:[sagemaker logs]: sagemaker-han/sagemaker-han/sagemaker/cifar10/cifar10.jsonl:   File \"/opt/ml/model/code/resnet_deploy.py\", line 111, in preprocess\u001b[0m\n",
      "\u001b[32m2022-08-21T17:20:23.065:[sagemaker logs]: sagemaker-han/sagemaker-han/sagemaker/cifar10/cifar10.jsonl:     return json.loads(input_data)\u001b[0m\n",
      "\u001b[32m2022-08-21T17:20:23.065:[sagemaker logs]: sagemaker-han/sagemaker-han/sagemaker/cifar10/cifar10.jsonl:   File \"/opt/conda/lib/python3.8/json/__init__.py\", line 357, in loads\u001b[0m\n",
      "\u001b[32m2022-08-21T17:20:23.065:[sagemaker logs]: sagemaker-han/sagemaker-han/sagemaker/cifar10/cifar10.jsonl:     return _default_decoder.decode(s)\u001b[0m\n",
      "\u001b[32m2022-08-21T17:20:23.065:[sagemaker logs]: sagemaker-han/sagemaker-han/sagemaker/cifar10/cifar10.jsonl:   File \"/opt/conda/lib/python3.8/json/decoder.py\", line 340, in decode\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:22,905 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661102422905\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:22,930 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend received inference at: 1661102422\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:22,935 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - transform-han\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:22,938 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 9\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:22,938 [INFO ] W-9003-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:8.12|#ModelName:model,Level:Model|#hostname:56a3d289cfe0,requestID:af579481-81cb-47dd-bc19-419137ec117f,timestamp:1661102422\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:22,940 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.255.130:49140 \"POST /invocations HTTP/1.1\" 500 55\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:22,940 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102420\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:22,940 [INFO ] W-9003-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102422\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:22,941 [INFO ] W-9003-model_1.0 TS_METRICS - WorkerThreadTime.ms:27|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102422\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:23,026 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661102423026\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:23,041 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1661102423\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:23,051 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - transform-han\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:23,056 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 15\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:23,056 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:14.12|#ModelName:model,Level:Model|#hostname:56a3d289cfe0,requestID:9fcc2e7b-1740-4c0c-9590-3a982fa8d404,timestamp:1661102423\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:23,056 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:49156 \"POST /invocations HTTP/1.1\" 500 55\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:23,056 [INFO ] W-9001-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102420\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:23,057 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102423\u001b[0m\n",
      "\u001b[34m2022-08-21T17:20:23,057 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:16|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102423\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:22,905 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661102422905\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:22,930 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend received inference at: 1661102422\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:22,935 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - transform-han\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:22,938 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 9\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:22,938 [INFO ] W-9003-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:8.12|#ModelName:model,Level:Model|#hostname:56a3d289cfe0,requestID:af579481-81cb-47dd-bc19-419137ec117f,timestamp:1661102422\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:22,940 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.255.130:49140 \"POST /invocations HTTP/1.1\" 500 55\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:22,940 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102420\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:22,940 [INFO ] W-9003-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102422\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:22,941 [INFO ] W-9003-model_1.0 TS_METRICS - WorkerThreadTime.ms:27|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102422\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:23,026 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661102423026\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:23,041 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1661102423\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:23,051 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - transform-han\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:23,056 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 15\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:23,056 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:14.12|#ModelName:model,Level:Model|#hostname:56a3d289cfe0,requestID:9fcc2e7b-1740-4c0c-9590-3a982fa8d404,timestamp:1661102423\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:23,056 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:49156 \"POST /invocations HTTP/1.1\" 500 55\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:23,056 [INFO ] W-9001-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102420\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:23,057 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102423\u001b[0m\n",
      "\u001b[35m2022-08-21T17:20:23,057 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:16|#Level:Host|#hostname:56a3d289cfe0,timestamp:1661102423\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2022-08-21T17:20:23.065:[sagemaker logs]: sagemaker-han/sagemaker-han/sagemaker/cifar10/cifar10.jsonl:     raise JSONDecodeError(\"Extra data\", s, end)\u001b[0m\n",
      "\u001b[32m2022-08-21T17:20:23.065:[sagemaker logs]: sagemaker-han/sagemaker-han/sagemaker/cifar10/cifar10.jsonl: json.decoder.JSONDecodeError: Extra data: line 2 column 1 (char 65293)\u001b[0m\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Transform job pytorch-inference-2022-08-21-17-14-31-810: Failed. Reason: AlgorithmError: See job logs for more information",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [140]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLine\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/workflow/pipeline_context.py:248\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m     run_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m self_instance\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mcontext\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/transformer.py:243\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, data, data_type, content_type, compression_type, split_type, job_name, input_filter, output_filter, join_source, experiment_config, model_client_config, wait, logs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_transform_job \u001b[38;5;241m=\u001b[39m _TransformJob\u001b[38;5;241m.\u001b[39mstart_new(\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    230\u001b[0m     data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m     model_client_config,\n\u001b[1;32m    240\u001b[0m )\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m--> 243\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_transform_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/transformer.py:440\u001b[0m, in \u001b[0;36m_TransformJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logs:\n\u001b[0;32m--> 440\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_transform_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_transform_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/session.py:4023\u001b[0m, in \u001b[0;36mSession.logs_for_transform_job\u001b[0;34m(self, job_name, wait, poll)\u001b[0m\n\u001b[1;32m   4020\u001b[0m             state \u001b[38;5;241m=\u001b[39m LogState\u001b[38;5;241m.\u001b[39mJOB_COMPLETE\n\u001b[1;32m   4022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 4023\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTransformJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4024\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dot:\n\u001b[1;32m   4025\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/session.py:3391\u001b[0m, in \u001b[0;36mSession._check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   3386\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   3387\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   3388\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   3389\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   3390\u001b[0m     )\n\u001b[0;32m-> 3391\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   3392\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   3393\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   3394\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   3395\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Transform job pytorch-inference-2022-08-21-17-14-31-810: Failed. Reason: AlgorithmError: See job logs for more information"
     ]
    }
   ],
   "source": [
    "\n",
    "transformer.transform(\n",
    "    data=new_input,\n",
    "    wait=True,\n",
    "    content_type='application/jsonlines',    \n",
    "    split_type='Line'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4b0e0fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................................\u001b[34mWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:47,004 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:47,157 [INFO ] main org.pytorch.serve.ModelServer - \u001b[0m\n",
      "\u001b[34mTorchserve version: 0.6.0\u001b[0m\n",
      "\u001b[34mTS Home: /opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mCurrent directory: /\u001b[0m\n",
      "\u001b[34mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[34mNumber of GPUs: 0\u001b[0m\n",
      "\u001b[34mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[34mMax heap size: 3070 M\u001b[0m\n",
      "\u001b[34mPython executable: /opt/conda/bin/python3.8\u001b[0m\n",
      "\u001b[34mConfig file: /etc/sagemaker-ts.properties\u001b[0m\n",
      "\u001b[34mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mMetrics address: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mInitial Models: model=/opt/ml/model\u001b[0m\n",
      "\u001b[34mLog dir: /logs\u001b[0m\n",
      "\u001b[34mMetrics dir: /logs\u001b[0m\n",
      "\u001b[34mNetty threads: 0\u001b[0m\n",
      "\u001b[34mNetty client threads: 0\u001b[0m\n",
      "\u001b[34mDefault workers per model: 4\u001b[0m\n",
      "\u001b[34mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[34mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[34mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[34mLimit Maximum Image Pixels: true\u001b[0m\n",
      "\u001b[34mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[34mAllowed Urls: [file://.*|http(s)?://.*]\u001b[0m\n",
      "\u001b[34mCustom python dependency for model allowed: false\u001b[0m\n",
      "\u001b[34mMetrics report format: prometheus\u001b[0m\n",
      "\u001b[34mEnable metrics API: true\u001b[0m\n",
      "\u001b[34mWorkflow Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mModel config: N/A\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:47,171 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:47,209 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:47,214 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:47,214 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:47,217 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:47,236 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:47,607 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:47,608 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:47,625 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel server started.\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:48,592 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:48,789 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:92efc52e3711,timestamp:1661100588\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:48,790 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:48.20624542236328|#Level:Host|#hostname:92efc52e3711,timestamp:1661100588\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:48,791 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:7.658885955810547|#Level:Host|#hostname:92efc52e3711,timestamp:1661100588\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:48,792 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:13.7|#Level:Host|#hostname:92efc52e3711,timestamp:1661100588\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:48,793 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:14783.57421875|#Level:Host|#hostname:92efc52e3711,timestamp:1661100588\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:48,793 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:927.2265625|#Level:Host|#hostname:92efc52e3711,timestamp:1661100588\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:48,794 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:7.8|#Level:Host|#hostname:92efc52e3711,timestamp:1661100588\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:48,917 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:48,923 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]69\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:48,924 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:48,936 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:48,938 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:49,024 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:49,036 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661100589036\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:49,139 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:49,315 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:49,316 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]72\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:49,317 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:49,317 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:49,317 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:49,332 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:49,332 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661100589332\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:49,393 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:49,431 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:49,431 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:49,433 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]70\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:49,434 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:49,434 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:49,434 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]71\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:49,437 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:49,438 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:49,437 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:49,440 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:49,454 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661100589454\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:49,455 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:49,486 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:49,486 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661100589486\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:49,504 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:49,540 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:49,905 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 743\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:49,906 [INFO ] W-9000-model_1.0 TS_METRICS - W-9000-model_1.0.ms:2678|#Level:Host|#hostname:92efc52e3711,timestamp:1661100589\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:49,909 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:130|#Level:Host|#hostname:92efc52e3711,timestamp:1661100589\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:50,128 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 736\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:50,131 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 583\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:50,132 [INFO ] W-9001-model_1.0 TS_METRICS - W-9001-model_1.0.ms:2901|#Level:Host|#hostname:92efc52e3711,timestamp:1661100590\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:50,133 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:65|#Level:Host|#hostname:92efc52e3711,timestamp:1661100590\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:50,133 [INFO ] W-9002-model_1.0 TS_METRICS - W-9002-model_1.0.ms:2901|#Level:Host|#hostname:92efc52e3711,timestamp:1661100590\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:50,134 [INFO ] W-9002-model_1.0 TS_METRICS - WorkerThreadTime.ms:65|#Level:Host|#hostname:92efc52e3711,timestamp:1661100590\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:50,151 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 638\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:50,152 [INFO ] W-9003-model_1.0 TS_METRICS - W-9003-model_1.0.ms:2920|#Level:Host|#hostname:92efc52e3711,timestamp:1661100590\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:50,152 [INFO ] W-9003-model_1.0 TS_METRICS - WorkerThreadTime.ms:60|#Level:Host|#hostname:92efc52e3711,timestamp:1661100590\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2022-08-21T16:49:53,855 [INFO ] pool-2-thread-5 ACCESS_LOG - /169.254.255.130:35634 \"GET /ping HTTP/1.1\" 200 34\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:53,856 [INFO ] pool-2-thread-5 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:92efc52e3711,timestamp:1661100593\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:53,900 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:35648 \"GET /execution-parameters HTTP/1.1\" 404 2\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:53,903 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:92efc52e3711,timestamp:1661100593\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:53,855 [INFO ] pool-2-thread-5 ACCESS_LOG - /169.254.255.130:35634 \"GET /ping HTTP/1.1\" 200 34\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:53,856 [INFO ] pool-2-thread-5 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:92efc52e3711,timestamp:1661100593\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:53,900 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:35648 \"GET /execution-parameters HTTP/1.1\" 404 2\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:53,903 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:92efc52e3711,timestamp:1661100593\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,276 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661100595276\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,290 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1661100595\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,276 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661100595276\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,290 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1661100595\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,390 [ERROR] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[34mio.netty.handler.codec.CorruptedFrameException: Message size exceed limit: 12461990\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.util.codec.CodecUtils.readLength(CodecUtils.java:24) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.util.codec.ModelResponseDecoder.decode(ModelResponseDecoder.java:75) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:795) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.epoll.EpollDomainSocketChannel$EpollDomainUnsafe.epollInReady(EpollDomainSocketChannel.java:138) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:475) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:378) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at java.lang.Thread.run(Thread.java:829) [?:?]\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,422 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,390 [ERROR] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[35mio.netty.handler.codec.CorruptedFrameException: Message size exceed limit: 12461990\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.util.codec.CodecUtils.readLength(CodecUtils.java:24) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.util.codec.ModelResponseDecoder.decode(ModelResponseDecoder.java:75) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:795) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.epoll.EpollDomainSocketChannel$EpollDomainUnsafe.epollInReady(EpollDomainSocketChannel.java:138) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:475) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:378) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at java.lang.Thread.run(Thread.java:829) [?:?]\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,422 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,424 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,424 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_MODEL_LOADED\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,424 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,425 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,425 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,425 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,425 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 137, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,426 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     cl_socket.sendall(resp)\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,426 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - BrokenPipeError: [Errno 32] Broken pipe\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,432 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:35662 \"POST /invocations HTTP/1.1\" 500 177\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,433 [INFO ] W-9000-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:92efc52e3711,timestamp:1661100593\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,434 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,435 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,436 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,454 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,456 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,457 [ERROR] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[34mio.netty.handler.codec.CorruptedFrameException: Message size exceed limit: 12461990\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.util.codec.CodecUtils.readLength(CodecUtils.java:24) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.util.codec.ModelResponseDecoder.decode(ModelResponseDecoder.java:75) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:404) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:371) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:354) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:819) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,424 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,424 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_MODEL_LOADED\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,424 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,425 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,425 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,425 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,425 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 137, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,426 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     cl_socket.sendall(resp)\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,426 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - BrokenPipeError: [Errno 32] Broken pipe\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,432 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:35662 \"POST /invocations HTTP/1.1\" 500 177\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,433 [INFO ] W-9000-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:92efc52e3711,timestamp:1661100593\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,434 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,435 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,436 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,454 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,456 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,457 [ERROR] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[35mio.netty.handler.codec.CorruptedFrameException: Message size exceed limit: 12461990\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.util.codec.CodecUtils.readLength(CodecUtils.java:24) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.util.codec.ModelResponseDecoder.decode(ModelResponseDecoder.java:75) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:404) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:371) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:354) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:819) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:384) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at java.lang.Thread.run(Thread.java:829) [?:?]\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,546 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661100595546\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,566 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1661100595\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,654 [ERROR] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[34mio.netty.handler.codec.CorruptedFrameException: Message size exceed limit: 12461990\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.util.codec.CodecUtils.readLength(CodecUtils.java:24) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.util.codec.ModelResponseDecoder.decode(ModelResponseDecoder.java:75) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:795) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.epoll.EpollDomainSocketChannel$EpollDomainUnsafe.epollInReady(EpollDomainSocketChannel.java:138) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:475) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:378) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at java.lang.Thread.run(Thread.java:829) [?:?]\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,657 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,657 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,657 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,657 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:384) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at java.lang.Thread.run(Thread.java:829) [?:?]\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,546 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661100595546\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,566 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1661100595\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,654 [ERROR] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[35mio.netty.handler.codec.CorruptedFrameException: Message size exceed limit: 12461990\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.util.codec.CodecUtils.readLength(CodecUtils.java:24) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.util.codec.ModelResponseDecoder.decode(ModelResponseDecoder.java:75) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:795) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.epoll.EpollDomainSocketChannel$EpollDomainUnsafe.epollInReady(EpollDomainSocketChannel.java:138) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:475) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:378) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at java.lang.Thread.run(Thread.java:829) [?:?]\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,657 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,657 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,657 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,657 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,658 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,658 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,658 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 137, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,658 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     cl_socket.sendall(resp)\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,659 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - BrokenPipeError: [Errno 32] Broken pipe\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,660 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_MODEL_LOADED\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,661 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:36742 \"POST /invocations HTTP/1.1\" 500 125\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,662 [INFO ] W-9001-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:92efc52e3711,timestamp:1661100593\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,662 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,662 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,663 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,680 [ERROR] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[34mio.netty.handler.codec.CorruptedFrameException: Message size exceed limit: 12461990\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.util.codec.CodecUtils.readLength(CodecUtils.java:24) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.util.codec.ModelResponseDecoder.decode(ModelResponseDecoder.java:75) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:404) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:371) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:354) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:819) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:384) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,658 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,658 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,658 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 137, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,658 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     cl_socket.sendall(resp)\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,659 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - BrokenPipeError: [Errno 32] Broken pipe\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,660 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_MODEL_LOADED\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,661 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:36742 \"POST /invocations HTTP/1.1\" 500 125\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,662 [INFO ] W-9001-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:92efc52e3711,timestamp:1661100593\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,662 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,662 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,663 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,680 [ERROR] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[35mio.netty.handler.codec.CorruptedFrameException: Message size exceed limit: 12461990\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.util.codec.CodecUtils.readLength(CodecUtils.java:24) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.util.codec.ModelResponseDecoder.decode(ModelResponseDecoder.java:75) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:404) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:371) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:354) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:819) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:384) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at java.lang.Thread.run(Thread.java:829) [?:?]\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,682 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,682 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,751 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661100595751\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,766 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend received inference at: 1661100595\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,846 [ERROR] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[34mio.netty.handler.codec.CorruptedFrameException: Message size exceed limit: 12461990\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.util.codec.CodecUtils.readLength(CodecUtils.java:24) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.util.codec.ModelResponseDecoder.decode(ModelResponseDecoder.java:75) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:795) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.epoll.EpollDomainSocketChannel$EpollDomainUnsafe.epollInReady(EpollDomainSocketChannel.java:138) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe$1.run(AbstractEpollChannel.java:388) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:384) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at java.lang.Thread.run(Thread.java:829) [?:?]\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,848 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_MODEL_LOADED\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,848 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m#011at java.lang.Thread.run(Thread.java:829) [?:?]\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,682 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,682 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,751 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661100595751\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,766 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend received inference at: 1661100595\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,846 [ERROR] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[35mio.netty.handler.codec.CorruptedFrameException: Message size exceed limit: 12461990\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.util.codec.CodecUtils.readLength(CodecUtils.java:24) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.util.codec.ModelResponseDecoder.decode(ModelResponseDecoder.java:75) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:795) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.epoll.EpollDomainSocketChannel$EpollDomainUnsafe.epollInReady(EpollDomainSocketChannel.java:138) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe$1.run(AbstractEpollChannel.java:388) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:384) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at java.lang.Thread.run(Thread.java:829) [?:?]\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,848 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_MODEL_LOADED\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,848 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,849 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,849 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,849 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,850 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,850 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,851 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 137, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,851 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     cl_socket.sendall(resp)\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,851 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - BrokenPipeError: [Errno 32] Broken pipe\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,851 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.255.130:36756 \"POST /invocations HTTP/1.1\" 500 109\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,852 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:92efc52e3711,timestamp:1661100593\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,853 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,853 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,853 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,863 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,863 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,862 [ERROR] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[34mio.netty.handler.codec.CorruptedFrameException: Message size exceed limit: 12461990\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.util.codec.CodecUtils.readLength(CodecUtils.java:24) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.util.codec.ModelResponseDecoder.decode(ModelResponseDecoder.java:75) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:404) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:371) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:354) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:819) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,849 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,849 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,849 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,850 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,850 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,851 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 137, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,851 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     cl_socket.sendall(resp)\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,851 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - BrokenPipeError: [Errno 32] Broken pipe\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,851 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.255.130:36756 \"POST /invocations HTTP/1.1\" 500 109\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,852 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:92efc52e3711,timestamp:1661100593\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,853 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,853 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,853 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,863 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,863 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,862 [ERROR] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[35mio.netty.handler.codec.CorruptedFrameException: Message size exceed limit: 12461990\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.util.codec.CodecUtils.readLength(CodecUtils.java:24) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.util.codec.ModelResponseDecoder.decode(ModelResponseDecoder.java:75) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:404) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:371) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:354) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:819) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:384) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at java.lang.Thread.run(Thread.java:829) [?:?]\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,948 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661100595948\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:55,962 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend received inference at: 1661100595\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:56,074 [ERROR] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[34mio.netty.handler.codec.CorruptedFrameException: Message size exceed limit: 12461990\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.util.codec.CodecUtils.readLength(CodecUtils.java:24) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.util.codec.ModelResponseDecoder.decode(ModelResponseDecoder.java:75) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:795) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.epoll.EpollDomainSocketChannel$EpollDomainUnsafe.epollInReady(EpollDomainSocketChannel.java:138) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:475) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:378) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at java.lang.Thread.run(Thread.java:829) [?:?]\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:56,084 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:56,085 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:56,085 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:56,085 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:56,087 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:56,088 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:384) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at java.lang.Thread.run(Thread.java:829) [?:?]\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,948 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661100595948\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:55,962 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend received inference at: 1661100595\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:56,074 [ERROR] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[35mio.netty.handler.codec.CorruptedFrameException: Message size exceed limit: 12461990\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.util.codec.CodecUtils.readLength(CodecUtils.java:24) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.util.codec.ModelResponseDecoder.decode(ModelResponseDecoder.java:75) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:795) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.epoll.EpollDomainSocketChannel$EpollDomainUnsafe.epollInReady(EpollDomainSocketChannel.java:138) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:475) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:378) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at java.lang.Thread.run(Thread.java:829) [?:?]\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:56,084 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:56,085 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:56,085 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:56,085 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:56,087 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:56,088 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:56,090 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 137, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:56,090 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     cl_socket.sendall(resp)\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:56,091 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - BrokenPipeError: [Errno 32] Broken pipe\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:56,092 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_MODEL_LOADED\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:56,093 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.255.130:36766 \"POST /invocations HTTP/1.1\" 500 158\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:56,093 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:92efc52e3711,timestamp:1661100593\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:56,094 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:56,098 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:56,099 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:56,121 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:56,122 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:56,121 [ERROR] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[34mio.netty.handler.codec.CorruptedFrameException: Message size exceed limit: 12461990\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.util.codec.CodecUtils.readLength(CodecUtils.java:24) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at org.pytorch.serve.util.codec.ModelResponseDecoder.decode(ModelResponseDecoder.java:75) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:404) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:371) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:354) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:819) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:384) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [model-server.jar:?]\u001b[0m\n",
      "\u001b[34m#011at java.lang.Thread.run(Thread.java:829) [?:?]\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:56,090 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 137, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:56,090 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     cl_socket.sendall(resp)\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:56,091 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - BrokenPipeError: [Errno 32] Broken pipe\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:56,092 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_MODEL_LOADED\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:56,093 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.255.130:36766 \"POST /invocations HTTP/1.1\" 500 158\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:56,093 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:92efc52e3711,timestamp:1661100593\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:56,094 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:56,098 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:56,099 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:56,121 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:56,122 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:56,121 [ERROR] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - Unknown exception\u001b[0m\n",
      "\u001b[35mio.netty.handler.codec.CorruptedFrameException: Message size exceed limit: 12461990\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.util.codec.CodecUtils.readLength(CodecUtils.java:24) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at org.pytorch.serve.util.codec.ModelResponseDecoder.decode(ModelResponseDecoder.java:75) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:404) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.channelInputClosed(ByteToMessageDecoder.java:371) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.handler.codec.ByteToMessageDecoder.channelInactive(ByteToMessageDecoder.java:354) ~[model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:819) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:384) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [model-server.jar:?]\u001b[0m\n",
      "\u001b[35m#011at java.lang.Thread.run(Thread.java:829) [?:?]\u001b[0m\n",
      "\u001b[32m2022-08-21T16:49:53.922:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[32m2022-08-21T16:49:56.129:[sagemaker logs]: sagemaker-han/sagemaker-han/sagemaker/cifar10/cifar10.jsonl: Bad HTTP status received from algorithm: 500\u001b[0m\n",
      "\u001b[32m2022-08-21T16:49:56.129:[sagemaker logs]: sagemaker-han/sagemaker-han/sagemaker/cifar10/cifar10.jsonl: \u001b[0m\n",
      "\u001b[32m2022-08-21T16:49:56.129:[sagemaker logs]: sagemaker-han/sagemaker-han/sagemaker/cifar10/cifar10.jsonl: Message:\u001b[0m\n",
      "\u001b[32m2022-08-21T16:49:56.129:[sagemaker logs]: sagemaker-han/sagemaker-han/sagemaker/cifar10/cifar10.jsonl: {\u001b[0m\n",
      "\u001b[32m2022-08-21T16:49:56.129:[sagemaker logs]: sagemaker-han/sagemaker-han/sagemaker/cifar10/cifar10.jsonl:   \"code\": 500,\u001b[0m\n",
      "\u001b[32m2022-08-21T16:49:56.129:[sagemaker logs]: sagemaker-han/sagemaker-han/sagemaker/cifar10/cifar10.jsonl:   \"type\": \"InternalServerException\",\u001b[0m\n",
      "\u001b[32m2022-08-21T16:49:56.130:[sagemaker logs]: sagemaker-han/sagemaker-han/sagemaker/cifar10/cifar10.jsonl:   \"message\": \"Worker died.\"\u001b[0m\n",
      "\u001b[32m2022-08-21T16:49:56.130:[sagemaker logs]: sagemaker-han/sagemaker-han/sagemaker/cifar10/cifar10.jsonl: }\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2022-08-21T16:49:57,475 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:57,477 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]158\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:57,477 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:57,478 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:57,478 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:57,480 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:57,475 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:57,477 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]158\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:57,477 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:57,478 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:57,478 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:57,480 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:57,481 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661100597481\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:57,488 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:57,954 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:57,955 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]162\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:57,956 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:57,956 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:57,957 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:57,961 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:57,962 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661100597962\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:57,964 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:58,090 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 602\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:58,090 [INFO ] W-9000-model_1.0 TS_METRICS - W-9000-model_1.0.ms:10862|#Level:Host|#hostname:92efc52e3711,timestamp:1661100598\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:58,091 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:8|#Level:Host|#hostname:92efc52e3711,timestamp:1661100598\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:58,329 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:58,333 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]166\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:58,336 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:57,481 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661100597481\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:57,488 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:57,954 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:57,955 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]162\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:57,956 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:57,956 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:57,957 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:57,961 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:57,962 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661100597962\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:57,964 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:58,090 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 602\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:58,090 [INFO ] W-9000-model_1.0 TS_METRICS - W-9000-model_1.0.ms:10862|#Level:Host|#hostname:92efc52e3711,timestamp:1661100598\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:58,091 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:8|#Level:Host|#hostname:92efc52e3711,timestamp:1661100598\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:58,329 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:58,333 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]166\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:58,336 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:58,337 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:58,337 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:58,346 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661100598346\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:58,346 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:58,383 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:58,337 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:58,337 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:58,346 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661100598346\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:58,346 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:58,383 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:58,487 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 522\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:58,488 [INFO ] W-9001-model_1.0 TS_METRICS - W-9001-model_1.0.ms:11257|#Level:Host|#hostname:92efc52e3711,timestamp:1661100598\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:58,489 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:5|#Level:Host|#hostname:92efc52e3711,timestamp:1661100598\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:58,731 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 351\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:58,732 [INFO ] W-9002-model_1.0 TS_METRICS - W-9002-model_1.0.ms:11500|#Level:Host|#hostname:92efc52e3711,timestamp:1661100598\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:58,732 [INFO ] W-9002-model_1.0 TS_METRICS - WorkerThreadTime.ms:35|#Level:Host|#hostname:92efc52e3711,timestamp:1661100598\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:58,765 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:58,766 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]173\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:58,767 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:58,768 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:58,767 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:58,487 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 522\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:58,488 [INFO ] W-9001-model_1.0 TS_METRICS - W-9001-model_1.0.ms:11257|#Level:Host|#hostname:92efc52e3711,timestamp:1661100598\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:58,489 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:5|#Level:Host|#hostname:92efc52e3711,timestamp:1661100598\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:58,731 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 351\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:58,732 [INFO ] W-9002-model_1.0 TS_METRICS - W-9002-model_1.0.ms:11500|#Level:Host|#hostname:92efc52e3711,timestamp:1661100598\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:58,732 [INFO ] W-9002-model_1.0 TS_METRICS - WorkerThreadTime.ms:35|#Level:Host|#hostname:92efc52e3711,timestamp:1661100598\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:58,765 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:58,766 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]173\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:58,767 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:58,768 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:58,767 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:58,780 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661100598780\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:58,780 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:58,809 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:59,162 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 354\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:59,163 [INFO ] W-9003-model_1.0 TS_METRICS - W-9003-model_1.0.ms:11931|#Level:Host|#hostname:92efc52e3711,timestamp:1661100599\u001b[0m\n",
      "\u001b[34m2022-08-21T16:49:59,163 [INFO ] W-9003-model_1.0 TS_METRICS - WorkerThreadTime.ms:29|#Level:Host|#hostname:92efc52e3711,timestamp:1661100599\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:58,780 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661100598780\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:58,780 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:58,809 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:59,162 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 354\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:59,163 [INFO ] W-9003-model_1.0 TS_METRICS - W-9003-model_1.0.ms:11931|#Level:Host|#hostname:92efc52e3711,timestamp:1661100599\u001b[0m\n",
      "\u001b[35m2022-08-21T16:49:59,163 [INFO ] W-9003-model_1.0 TS_METRICS - WorkerThreadTime.ms:29|#Level:Host|#hostname:92efc52e3711,timestamp:1661100599\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Transform job pytorch-inference-2022-08-21-16-43-47-224: Failed. Reason: AlgorithmError: See job logs for more information",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [131]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mapplication/jsonlines\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLine\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/workflow/pipeline_context.py:248\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m     run_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m self_instance\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mcontext\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/transformer.py:243\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, data, data_type, content_type, compression_type, split_type, job_name, input_filter, output_filter, join_source, experiment_config, model_client_config, wait, logs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_transform_job \u001b[38;5;241m=\u001b[39m _TransformJob\u001b[38;5;241m.\u001b[39mstart_new(\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    230\u001b[0m     data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m     model_client_config,\n\u001b[1;32m    240\u001b[0m )\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m--> 243\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_transform_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/transformer.py:440\u001b[0m, in \u001b[0;36m_TransformJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logs:\n\u001b[0;32m--> 440\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_transform_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_transform_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/session.py:4023\u001b[0m, in \u001b[0;36mSession.logs_for_transform_job\u001b[0;34m(self, job_name, wait, poll)\u001b[0m\n\u001b[1;32m   4020\u001b[0m             state \u001b[38;5;241m=\u001b[39m LogState\u001b[38;5;241m.\u001b[39mJOB_COMPLETE\n\u001b[1;32m   4022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 4023\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTransformJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4024\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dot:\n\u001b[1;32m   4025\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/session.py:3391\u001b[0m, in \u001b[0;36mSession._check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   3386\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   3387\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   3388\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   3389\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   3390\u001b[0m     )\n\u001b[0;32m-> 3391\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   3392\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   3393\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   3394\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   3395\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Transform job pytorch-inference-2022-08-21-16-43-47-224: Failed. Reason: AlgorithmError: See job logs for more information"
     ]
    }
   ],
   "source": [
    "transformer.transform(\n",
    "    data=new_input,\n",
    "    wait=True,\n",
    "    content_type='application/jsonlines',    \n",
    "    split_type='Line'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae640a44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
