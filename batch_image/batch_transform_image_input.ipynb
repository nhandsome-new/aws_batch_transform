{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb0c300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初期設定\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import os, boto3, json, sagemaker, numpy as np\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from io import BytesIO\n",
    "\n",
    "def make_dir(path):\n",
    "    if os.path.isdir(path):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(path)\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# AWS設定\n",
    "# role = 'han_s3_full_access'\n",
    "role = 'FullAccessHan'\n",
    "region = boto3.Session().region_name\n",
    "bucket='sagemaker-han'\n",
    "# bucket='sagemaker-han-batch'\n",
    "prefix = 'batch-images'\n",
    "bucket_path = 'https://s3-{}.amazonaws.com/{}'.format(region,bucket)\n",
    "model_prefix = os.path.join(prefix, 'model')\n",
    "input_prefix = os.path.join(prefix, 'inputs')\n",
    "output_prefix = os.path.join(prefix, 'outputs')\n",
    "inference_prefix = os.path.join(prefix, 'inference')\n",
    "\n",
    "# Local設定\n",
    "base_dir = os.getcwd()\n",
    "data_dir = os.path.join(base_dir, 'data')\n",
    "model_dir = os.path.join(base_dir, 'model')\n",
    "input_dir = os.path.join(base_dir, 'inputs')\n",
    "output_dir = os.path.join(base_dir, 'outputs')\n",
    "inference_dir = os.path.join(base_dir, 'inference')\n",
    "source_dir = os.path.join(base_dir, 'src')\n",
    "for dir_name in [model_dir, input_dir, output_dir, source_dir, inference_dir]:\n",
    "    make_dir(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f28de312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Sample 100 datas from CIFAR10 dataset\n",
    "\n",
    "from src.utils import image_to_bytes\n",
    "\n",
    "r_inference_path = os.path.join(base_dir, 'real_inference')\n",
    "n = 1000\n",
    "json_name = r_inference_path+f'/inf_data{n}.jsonl'\n",
    "\n",
    "image_to_bytes(json_name, data_dir, inference_dir, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84b4cd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_inference = sagemaker_session.upload_data(path=r_inference_path, bucket=bucket, key_prefix='batch-images/r_inference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e6c0bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset for the model training\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "train_data = datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "training_data_loader = DataLoader(train_data, batch_size=len(train_data))\n",
    "training_data_loaded = next(iter(training_data_loader))\n",
    "torch.save(training_data_loaded, os.path.join(input_dir, 'training.pt'))\n",
    "\n",
    "test_data_loader = DataLoader(test_data, batch_size=len(test_data))\n",
    "test_data_loaded = next(iter(test_data_loader))\n",
    "torch.save(test_data_loaded, os.path.join(output_dir, 'test.pt'))\n",
    "\n",
    "inputs = sagemaker_session.upload_data(path=input_dir, bucket=bucket, key_prefix=input_prefix)\n",
    "outputs = sagemaker_session.upload_data(path=output_dir, bucket=bucket, key_prefix=output_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59fbaf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_inputs= f's3://{bucket}/batch-images/inference'\n",
    "inputs= f's3://{bucket}/batch-images/inputs'\n",
    "outputs= f's3://{bucket}/batch-images/outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5274c069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-22 13:51:32 Starting - Starting the training job...\n",
      "2022-08-22 13:51:59 Starting - Preparing the instances for trainingProfilerReport-1661176290: InProgress\n",
      ".........\n",
      "2022-08-22 13:53:33 Downloading - Downloading input data...\n",
      "2022-08-22 13:54:13 Training - Downloading the training image......\n",
      "2022-08-22 13:55:14 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-08-22 13:55:15,163 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-08-22 13:55:15,165 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-08-22 13:55:15,175 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-08-22 13:55:15,186 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-08-22 13:55:15,860 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-08-22 13:55:15,881 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-08-22 13:55:15,899 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-08-22 13:55:15,916 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.m4.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 128,\n",
      "        \"epochs\": 1,\n",
      "        \"lr\": 0.01\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.m4.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"pytorch-training-2022-08-22-13-51-28-555\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-608095525235/pytorch-training-2022-08-22-13-51-28-555/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"resnet\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m4.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m4.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"resnet.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":128,\"epochs\":1,\"lr\":0.01}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=resnet.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m4.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m4.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.m4.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m4.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=resnet\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-608095525235/pytorch-training-2022-08-22-13-51-28-555/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.m4.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":128,\"epochs\":1,\"lr\":0.01},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m4.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"pytorch-training-2022-08-22-13-51-28-555\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-608095525235/pytorch-training-2022-08-22-13-51-28-555/source/sourcedir.tar.gz\",\"module_name\":\"resnet\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m4.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m4.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"resnet.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"128\",\"--epochs\",\"1\",\"--lr\",\"0.01\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=128\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.01\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220816-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 resnet.py --batch-size 128 --epochs 1 --lr 0.01\u001b[0m\n",
      "\u001b[34m[2022-08-22 13:55:18.880 algo-1:27 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220816-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220816-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[2022-08-22 13:55:19.587 algo-1:27 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-08-22 13:55:19.590 algo-1:27 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-08-22 13:55:19.590 algo-1:27 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-08-22 13:55:19.591 algo-1:27 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-08-22 13:55:19.591 algo-1:27 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [12800/50000 (26%)] Loss: -15.599797\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [25600/50000 (51%)] Loss: -75.351624\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [38400/50000 (77%)] Loss: -376.315094\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving the model.\u001b[0m\n",
      "\u001b[34m2022-08-22 14:00:37,306 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-08-22 14:00:37,306 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-08-22 14:00:37,307 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-08-22 14:01:35 Uploading - Uploading generated training model\n",
      "2022-08-22 14:01:35 Completed - Training job completed\n",
      "ProfilerReport-1661176290: NoIssuesFound\n",
      "Training seconds: 489\n",
      "Billable seconds: 489\n"
     ]
    }
   ],
   "source": [
    "# Create Training Container\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point=\"resnet.py\",\n",
    "                    role=role,\n",
    "                    source_dir = \"src\",\n",
    "                    framework_version='1.12.0',\n",
    "                    py_version='py38',\n",
    "                    instance_count=1,\n",
    "                    instance_type='ml.m4.xlarge',\n",
    "                    hyperparameters={\n",
    "                        'batch-size':128,\n",
    "                        'lr': 0.01,\n",
    "                        'epochs': 1,\n",
    "                    })\n",
    "\n",
    "estimator.fit({'training': inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1344bb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_predictor = estimator.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type='ml.m4.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5e52ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "test_input = torch.rand(1,3,32,32)\n",
    "test_output = cifar10_predictor.predict(test_input)\n",
    "print(test_input.shape)\n",
    "print(test_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccb4fb4",
   "metadata": {},
   "source": [
    "# Batch Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "eb925323",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "# home\n",
    "model_path = 's3://sagemaker-us-west-2-608095525235/pytorch-training-2022-08-22-14-02-01-637/model.tar.gz'\n",
    "# fusic\n",
    "# model_path = 's3://sagemaker-us-west-2-582981179587/pytorch-training-2022-08-22-11-21-23-382/model.tar.gz'\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data = model_path,\n",
    "                             entry_point='resnet_batch_transform.py',\n",
    "                             source_dir = 'src',\n",
    "                             framework_version='1.12.0',\n",
    "                             py_version='py38',\n",
    "                             role = role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ea2048d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_concurrent_transforms = 10\n",
    "max_payload = 10\n",
    "# strategy = 'SingleRecord'\n",
    "# split_type = None\n",
    "strategy = 'MultiRecord'\n",
    "split_type = 'Line'\n",
    "\n",
    "output_s3_path = 's3://{}/batch-images/outputs/bt_real_{}_{}_{}_{}'.format(bucket, max_concurrent_transforms,max_payload,strategy,split_type)\n",
    "\n",
    "transformer = pytorch_model.transformer(instance_count=1,\n",
    "                              instance_type=\"ml.m5.xlarge\",\n",
    "                              max_concurrent_transforms=max_concurrent_transforms,\n",
    "                              max_payload=max_payload,  # 1MB\n",
    "                              strategy=strategy,\n",
    "                              output_path=output_s3_path)\n",
    "\n",
    "f_inference = f's3://{bucket}/batch-images/f_inference'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a14655a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................\u001b[34mWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:54,134 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:54,221 [INFO ] main org.pytorch.serve.ModelServer - \u001b[0m\n",
      "\u001b[34mTorchserve version: 0.6.0\u001b[0m\n",
      "\u001b[34mTS Home: /opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mCurrent directory: /\u001b[0m\n",
      "\u001b[34mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[34mNumber of GPUs: 0\u001b[0m\n",
      "\u001b[34mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[34mMax heap size: 3010 M\u001b[0m\n",
      "\u001b[34mPython executable: /opt/conda/bin/python3.8\u001b[0m\n",
      "\u001b[34mConfig file: /etc/sagemaker-ts.properties\u001b[0m\n",
      "\u001b[34mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mMetrics address: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mInitial Models: model=/opt/ml/model\u001b[0m\n",
      "\u001b[34mLog dir: /logs\u001b[0m\n",
      "\u001b[34mMetrics dir: /logs\u001b[0m\n",
      "\u001b[34mNetty threads: 0\u001b[0m\n",
      "\u001b[34mNetty client threads: 0\u001b[0m\n",
      "\u001b[34mDefault workers per model: 4\u001b[0m\n",
      "\u001b[34mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[34mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[34mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[34mLimit Maximum Image Pixels: true\u001b[0m\n",
      "\u001b[34mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[34mAllowed Urls: [file://.*|http(s)?://.*]\u001b[0m\n",
      "\u001b[34mCustom python dependency for model allowed: false\u001b[0m\n",
      "\u001b[34mMetrics report format: prometheus\u001b[0m\n",
      "\u001b[34mEnable metrics API: true\u001b[0m\n",
      "\u001b[34mWorkflow Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mModel config: N/A\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:54,228 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:54,250 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:54,253 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:54,253 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:54,255 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:54,274 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:54,570 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:54,571 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:54,581 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel server started.\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,200 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,278 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:e334c20429de,timestamp:1661283115\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,280 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:48.20627975463867|#Level:Host|#hostname:e334c20429de,timestamp:1661283115\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,281 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:7.658851623535156|#Level:Host|#hostname:e334c20429de,timestamp:1661283115\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,282 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:13.7|#Level:Host|#hostname:e334c20429de,timestamp:1661283115\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,282 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:14525.921875|#Level:Host|#hostname:e334c20429de,timestamp:1661283115\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,283 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:888.52734375|#Level:Host|#hostname:e334c20429de,timestamp:1661283115\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,284 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:7.7|#Level:Host|#hostname:e334c20429de,timestamp:1661283115\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,689 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,691 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]48\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,692 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,693 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,703 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,722 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,728 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661283115728\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,784 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,822 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,827 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]47\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,828 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,828 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,830 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,833 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,834 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]49\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,834 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,834 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,835 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,855 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,856 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,856 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661283115856\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,856 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661283115856\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,892 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:55,924 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:56,091 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:56,092 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]46\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:56,092 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:56,093 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:56,093 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:56,112 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661283116112\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:56,112 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:56,184 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:56,446 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 653\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:56,448 [INFO ] W-9000-model_1.0 TS_METRICS - W-9000-model_1.0.ms:2187|#Level:Host|#hostname:e334c20429de,timestamp:1661283116\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:56,449 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:68|#Level:Host|#hostname:e334c20429de,timestamp:1661283116\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:56,567 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 630\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:56,567 [INFO ] W-9003-model_1.0 TS_METRICS - W-9003-model_1.0.ms:2303|#Level:Host|#hostname:e334c20429de,timestamp:1661283116\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:56,568 [INFO ] W-9003-model_1.0 TS_METRICS - WorkerThreadTime.ms:82|#Level:Host|#hostname:e334c20429de,timestamp:1661283116\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:56,680 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 782\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:56,682 [INFO ] W-9001-model_1.0 TS_METRICS - W-9001-model_1.0.ms:2419|#Level:Host|#hostname:e334c20429de,timestamp:1661283116\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:56,682 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:44|#Level:Host|#hostname:e334c20429de,timestamp:1661283116\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:56,753 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 575\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:56,754 [INFO ] W-9002-model_1.0 TS_METRICS - W-9002-model_1.0.ms:2489|#Level:Host|#hostname:e334c20429de,timestamp:1661283116\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:56,754 [INFO ] W-9002-model_1.0 TS_METRICS - WorkerThreadTime.ms:67|#Level:Host|#hostname:e334c20429de,timestamp:1661283116\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:59,683 [INFO ] pool-2-thread-5 ACCESS_LOG - /169.254.255.130:60704 \"GET /ping HTTP/1.1\" 200 28\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:59,684 [INFO ] pool-2-thread-5 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:e334c20429de,timestamp:1661283119\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:59,716 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:60706 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[34m2022-08-23T19:31:59,718 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:e334c20429de,timestamp:1661283119\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:00,004 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661283120004\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:00,009 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1661283120\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:00,398 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - request received : application/jsonlines\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:00,399 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Num of data in a request: 500\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:00,399 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Input Object Shape: torch.Size([500, 3, 32, 32])\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:00,399 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 391\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:00,399 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Input Data Size > BATCH_SIZE\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:00,400 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Split input data by BATCH_SIZE:128\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:00,400 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - <built-in method size of Tensor object at 0x7f0b5aec2630>\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:00,400 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:389.38|#ModelName:model,Level:Model|#hostname:e334c20429de,requestID:dfd0f365-c77e-4f92-92bb-7f33f400c84d,timestamp:1661283120\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:00,400 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:60722 \"POST /invocations HTTP/1.1\" 500 411\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:00,401 [INFO ] W-9000-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:e334c20429de,timestamp:1661283119\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:00,402 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:e334c20429de,timestamp:1661283120\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:00,402 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:7|#Level:Host|#hostname:e334c20429de,timestamp:1661283120\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:00,451 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661283120451\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:00,454 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend received inference at: 1661283120\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:00,840 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - request received : application/jsonlines\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:00,840 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 386\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:00,840 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Num of data in a request: 500\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:00,841 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Input Object Shape: torch.Size([500, 3, 32, 32])\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:00,841 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Input Data Size > BATCH_SIZE\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:00,841 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Split input data by BATCH_SIZE:128\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:00,841 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.255.130:60734 \"POST /invocations HTTP/1.1\" 500 394\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:00,841 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - <built-in method size of Tensor object at 0x7fa911760680>\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:00,841 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:e334c20429de,timestamp:1661283119\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:00,842 [INFO ] W-9003-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:385.5|#ModelName:model,Level:Model|#hostname:e334c20429de,requestID:596d5bd5-52d9-4b35-b730-a98e5e3e412c,timestamp:1661283120\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:00,842 [INFO ] W-9003-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:e334c20429de,timestamp:1661283120\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:00,842 [INFO ] W-9003-model_1.0 TS_METRICS - WorkerThreadTime.ms:5|#Level:Host|#hostname:e334c20429de,timestamp:1661283120\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:00,867 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661283120867\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:00,871 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1661283120\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:01,247 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - request received : application/jsonlines\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:01,247 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 377\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:01,247 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Num of data in a request: 500\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:01,247 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:60746 \"POST /invocations HTTP/1.1\" 500 383\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:01,248 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Input Object Shape: torch.Size([500, 3, 32, 32])\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:01,248 [INFO ] W-9001-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:e334c20429de,timestamp:1661283119\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:01,248 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Input Data Size > BATCH_SIZE\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:01,248 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:e334c20429de,timestamp:1661283121\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:01,248 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:e334c20429de,timestamp:1661283121\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:01,248 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Split input data by BATCH_SIZE:128\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:01,249 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - <built-in method size of Tensor object at 0x7f2de9d645e0>\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:01,249 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:375.76|#ModelName:model,Level:Model|#hostname:e334c20429de,requestID:5daa31c2-3a89-48ca-befc-8412f3d0838f,timestamp:1661283121\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:01,287 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661283121287\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:01,293 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend received inference at: 1661283121\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:01,679 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - request received : application/jsonlines\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:01,679 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 387\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:01,680 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Num of data in a request: 500\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:01,680 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Input Object Shape: torch.Size([500, 3, 32, 32])\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:01,680 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.255.130:60748 \"POST /invocations HTTP/1.1\" 500 411\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:01,681 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Input Data Size > BATCH_SIZE\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:01,681 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:e334c20429de,timestamp:1661283119\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:01,681 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Split input data by BATCH_SIZE:128\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:01,681 [INFO ] W-9002-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:e334c20429de,timestamp:1661283121\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:01,681 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - <built-in method size of Tensor object at 0x7f44d69af720>\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:01,682 [INFO ] W-9002-model_1.0 TS_METRICS - WorkerThreadTime.ms:8|#Level:Host|#hostname:e334c20429de,timestamp:1661283121\u001b[0m\n",
      "\u001b[34m2022-08-23T19:32:01,682 [INFO ] W-9002-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:386.34|#ModelName:model,Level:Model|#hostname:e334c20429de,requestID:70748e72-ee3b-4665-b65a-208502c73745,timestamp:1661283121\u001b[0m\n",
      "\u001b[32m2022-08-23T19:31:59.728:[sagemaker logs]: MaxConcurrentTransforms=10, MaxPayloadInMB=10, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[32m2022-08-23T19:32:01.685:[sagemaker logs]: sagemaker-han/batch-images/r_inference/inf_data500.jsonl: Bad HTTP status received from algorithm: 500\u001b[0m\n",
      "\u001b[32m2022-08-23T19:32:01.686:[sagemaker logs]: sagemaker-han/batch-images/r_inference/inf_data500.jsonl: \u001b[0m\n",
      "\u001b[32m2022-08-23T19:32:01.686:[sagemaker logs]: sagemaker-han/batch-images/r_inference/inf_data500.jsonl: Message:\u001b[0m\n",
      "\u001b[32m2022-08-23T19:32:01.686:[sagemaker logs]: sagemaker-han/batch-images/r_inference/inf_data500.jsonl: 'int' object is not iterable\u001b[0m\n",
      "\u001b[32m2022-08-23T19:32:01.686:[sagemaker logs]: sagemaker-han/batch-images/r_inference/inf_data500.jsonl: Traceback (most recent call last):\u001b[0m\n",
      "\u001b[32m2022-08-23T19:32:01.686:[sagemaker logs]: sagemaker-han/batch-images/r_inference/inf_data500.jsonl:   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 128, in transform\u001b[0m\n",
      "\u001b[32m2022-08-23T19:32:01.686:[sagemaker logs]: sagemaker-han/batch-images/r_inference/inf_data500.jsonl:     result = self._transform_fn(self._model, input_data, content_type, accept)\u001b[0m\n",
      "\u001b[32m2022-08-23T19:32:01.686:[sagemaker logs]: sagemaker-han/batch-images/r_inference/inf_data500.jsonl:   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 234, in _default_transform_fn\u001b[0m\n",
      "\u001b[32m2022-08-23T19:32:01.686:[sagemaker logs]: sagemaker-han/batch-images/r_inference/inf_data500.jsonl:     prediction = self._predict_fn(data, model)\u001b[0m\n",
      "\u001b[32m2022-08-23T19:32:01.686:[sagemaker logs]: sagemaker-han/batch-images/r_inference/inf_data500.jsonl:   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 154, in predict_fn\u001b[0m\n",
      "\u001b[32m2022-08-23T19:32:01.686:[sagemaker logs]: sagemaker-han/batch-images/r_inference/inf_data500.jsonl:     pred += torch.argmax(output).item()\u001b[0m\n",
      "\u001b[32m2022-08-23T19:32:01.686:[sagemaker logs]: sagemaker-han/batch-images/r_inference/inf_data500.jsonl: TypeError: 'int' object is not iterable\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Transform job pytorch-inference-2022-08-23-19-27-05-373: Failed. Reason: AlgorithmError: See job logs for more information",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/han/Desktop/Nhandsome/fusic/2022/batch_transform/batch_image/batch_transform_image_input.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/han/Desktop/Nhandsome/fusic/2022/batch_transform/batch_image/batch_transform_image_input.ipynb#ch0000013?line=0'>1</a>\u001b[0m r_inference \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms3://\u001b[39m\u001b[39m{\u001b[39;00mbucket\u001b[39m}\u001b[39;00m\u001b[39m/batch-images/r_inference\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/han/Desktop/Nhandsome/fusic/2022/batch_transform/batch_image/batch_transform_image_input.ipynb#ch0000013?line=1'>2</a>\u001b[0m transformer\u001b[39m.\u001b[39;49mtransform(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/han/Desktop/Nhandsome/fusic/2022/batch_transform/batch_image/batch_transform_image_input.ipynb#ch0000013?line=2'>3</a>\u001b[0m     data\u001b[39m=\u001b[39;49mr_inference,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/han/Desktop/Nhandsome/fusic/2022/batch_transform/batch_image/batch_transform_image_input.ipynb#ch0000013?line=3'>4</a>\u001b[0m     data_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mS3Prefix\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/han/Desktop/Nhandsome/fusic/2022/batch_transform/batch_image/batch_transform_image_input.ipynb#ch0000013?line=4'>5</a>\u001b[0m     content_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mapplication/jsonlines\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/han/Desktop/Nhandsome/fusic/2022/batch_transform/batch_image/batch_transform_image_input.ipynb#ch0000013?line=5'>6</a>\u001b[0m     wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/han/Desktop/Nhandsome/fusic/2022/batch_transform/batch_image/batch_transform_image_input.ipynb#ch0000013?line=6'>7</a>\u001b[0m     split_type\u001b[39m=\u001b[39;49msplit_type,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/han/Desktop/Nhandsome/fusic/2022/batch_transform/batch_image/batch_transform_image_input.ipynb#ch0000013?line=7'>8</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/workflow/pipeline_context.py:248\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/workflow/pipeline_context.py?line=244'>245</a>\u001b[0m     run_func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/workflow/pipeline_context.py?line=245'>246</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m self_instance\u001b[39m.\u001b[39msagemaker_session\u001b[39m.\u001b[39mcontext\n\u001b[0;32m--> <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/workflow/pipeline_context.py?line=247'>248</a>\u001b[0m \u001b[39mreturn\u001b[39;00m run_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/transformer.py:243\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, data, data_type, content_type, compression_type, split_type, job_name, input_filter, output_filter, join_source, experiment_config, model_client_config, wait, logs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/transformer.py?line=227'>228</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlatest_transform_job \u001b[39m=\u001b[39m _TransformJob\u001b[39m.\u001b[39mstart_new(\n\u001b[1;32m    <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/transformer.py?line=228'>229</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/transformer.py?line=229'>230</a>\u001b[0m     data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/transformer.py?line=238'>239</a>\u001b[0m     model_client_config,\n\u001b[1;32m    <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/transformer.py?line=239'>240</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/transformer.py?line=241'>242</a>\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[0;32m--> <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/transformer.py?line=242'>243</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlatest_transform_job\u001b[39m.\u001b[39;49mwait(logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/transformer.py:440\u001b[0m, in \u001b[0;36m_TransformJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/transformer.py?line=437'>438</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, logs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/transformer.py?line=438'>439</a>\u001b[0m     \u001b[39mif\u001b[39;00m logs:\n\u001b[0;32m--> <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/transformer.py?line=439'>440</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49mlogs_for_transform_job(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjob_name, wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/transformer.py?line=440'>441</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/transformer.py?line=441'>442</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session\u001b[39m.\u001b[39mwait_for_transform_job(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/session.py:4023\u001b[0m, in \u001b[0;36mSession.logs_for_transform_job\u001b[0;34m(self, job_name, wait, poll)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/session.py?line=4019'>4020</a>\u001b[0m             state \u001b[39m=\u001b[39m LogState\u001b[39m.\u001b[39mJOB_COMPLETE\n\u001b[1;32m   <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/session.py?line=4021'>4022</a>\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[0;32m-> <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/session.py?line=4022'>4023</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_job_status(job_name, description, \u001b[39m\"\u001b[39;49m\u001b[39mTransformJobStatus\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/session.py?line=4023'>4024</a>\u001b[0m     \u001b[39mif\u001b[39;00m dot:\n\u001b[1;32m   <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/session.py?line=4024'>4025</a>\u001b[0m         \u001b[39mprint\u001b[39m()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/session.py:3391\u001b[0m, in \u001b[0;36mSession._check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/session.py?line=3384'>3385</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mCapacityError\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(reason):\n\u001b[1;32m   <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/session.py?line=3385'>3386</a>\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mCapacityError(\n\u001b[1;32m   <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/session.py?line=3386'>3387</a>\u001b[0m         message\u001b[39m=\u001b[39mmessage,\n\u001b[1;32m   <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/session.py?line=3387'>3388</a>\u001b[0m         allowed_statuses\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mCompleted\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mStopped\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/session.py?line=3388'>3389</a>\u001b[0m         actual_status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m   <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/session.py?line=3389'>3390</a>\u001b[0m     )\n\u001b[0;32m-> <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/session.py?line=3390'>3391</a>\u001b[0m \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/session.py?line=3391'>3392</a>\u001b[0m     message\u001b[39m=\u001b[39mmessage,\n\u001b[1;32m   <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/session.py?line=3392'>3393</a>\u001b[0m     allowed_statuses\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mCompleted\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mStopped\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/session.py?line=3393'>3394</a>\u001b[0m     actual_status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m   <a href='file:///Users/han/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/session.py?line=3394'>3395</a>\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Transform job pytorch-inference-2022-08-23-19-27-05-373: Failed. Reason: AlgorithmError: See job logs for more information"
     ]
    }
   ],
   "source": [
    "\n",
    "r_inference = f's3://{bucket}/batch-images/r_inference'\n",
    "transformer.transform(\n",
    "    data=r_inference,\n",
    "    data_type=\"S3Prefix\",\n",
    "    content_type=\"application/jsonlines\",\n",
    "    wait=True,\n",
    "    split_type=split_type,\n",
    ")\n",
    "\n",
    "# {\"in0\": [6, 17, 606, 19, 53, 67, 52, 12, 5, 10, 15, 10178, 7, 33, 652, 80, 15, 69, 821, 4], \"in1\": [16, 21, 13, 45, 14, 9, 80, 59, 164, 4]}\n",
    "# {\"in0\": [22, 1016, 32, 13, 25, 11, 5, 64, 573, 45, 5, 80, 15, 67, 21, 7, 9, 107, 4], \"in1\": [22, 32, 13, 25, 1016, 573, 3252, 4]}\n",
    "# {\"in0\": [774, 14, 21, 206], \"in1\": [21, 366, 125]}\n",
    "\n",
    "# {\"scores\":[0.195667684078216,0.395351558923721,0.408980727195739]}\n",
    "# {\"scores\":[0.251988261938095,0.258233487606048,0.489778339862823]}\n",
    "# {\"scores\":[0.280087798833847,0.368331134319305,0.351581096649169]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "253fc975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('s3://sagemaker-han/batch-images/inference/sample31.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "41507e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest transform job: pytorch-inference-2022-08-21-08-18-59-697\n"
     ]
    }
   ],
   "source": [
    "print(\"Latest transform job:\", transformer.latest_transform_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719ccee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.stop_transform_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cf813c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CreationTime': datetime.datetime(2022, 8, 21, 12, 13, 37, 860000, tzinfo=tzlocal()),\n",
      " 'DataProcessing': {'InputFilter': '$',\n",
      "                    'JoinSource': 'None',\n",
      "                    'OutputFilter': '$'},\n",
      " 'FailureReason': 'AlgorithmError: See job logs for more information',\n",
      " 'ModelName': 'pytorch-inference-2022-08-21-03-13-10-667',\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '949',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Sun, 21 Aug 2022 05:10:13 GMT',\n",
      "                                      'x-amzn-requestid': 'cd218913-c7fe-4e1f-9b31-2b2fa07372d4'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': 'cd218913-c7fe-4e1f-9b31-2b2fa07372d4',\n",
      "                      'RetryAttempts': 0},\n",
      " 'TransformEndTime': datetime.datetime(2022, 8, 21, 12, 18, 21, 796000, tzinfo=tzlocal()),\n",
      " 'TransformInput': {'CompressionType': 'None',\n",
      "                    'ContentType': 'application/x-image',\n",
      "                    'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
      "                                                    'S3Uri': 's3://sagemaker-han/sagemaker/batch_transform'}},\n",
      "                    'SplitType': 'None'},\n",
      " 'TransformJobArn': 'arn:aws:sagemaker:us-west-2:608095525235:transform-job/pytorch-inference-2022-08-21-03-13-37-242',\n",
      " 'TransformJobName': 'pytorch-inference-2022-08-21-03-13-37-242',\n",
      " 'TransformJobStatus': 'Failed',\n",
      " 'TransformOutput': {'AssembleWith': 'None',\n",
      "                     'KmsKeyId': '',\n",
      "                     'S3OutputPath': 's3://sagemaker-us-west-2-608095525235/pytorch-inference-2022-08-21-03-13-37-242'},\n",
      " 'TransformResources': {'InstanceCount': 1, 'InstanceType': 'ml.m5.large'},\n",
      " 'TransformStartTime': datetime.datetime(2022, 8, 21, 12, 16, 42, 175000, tzinfo=tzlocal())}\n"
     ]
    }
   ],
   "source": [
    "import pprint as pp\n",
    "\n",
    "job_name = 'pytorch-inference-2022-08-21-03-13-37-242'\n",
    "sm_cli = sagemaker_session.sagemaker_client\n",
    "job_info = sm_cli.describe_transform_job(TransformJobName=job_name)\n",
    "pp.pprint(job_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "53b2cbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model = PyTorchModel(model_data = 's3://sagemaker-us-west-2-582981179587/pytorch-inference-2022-08-23-05-57-39-711/model.tar.gz',\n",
    "                             entry_point='resnet_transform.py',\n",
    "                             source_dir = 'src',\n",
    "                             framework_version='1.12.0',\n",
    "                             py_version='py38',\n",
    "                             role = role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054838cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('aws_batch_transform-pg7SNxiG')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "445eadaa75e92a02935b67c0f43eae30d5f0df35a0b86dd757723720e6c70438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
