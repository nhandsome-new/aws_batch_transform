{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb0c300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初期設定\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import os, boto3, json, sagemaker, numpy as np\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from io import BytesIO\n",
    "\n",
    "def make_dir(path):\n",
    "    if os.path.isdir(path):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(path)\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# AWS設定\n",
    "role = 'han_s3_full_access'\n",
    "# role = 'FullAccessHan'\n",
    "region = boto3.Session().region_name\n",
    "# bucket='sagemaker-han'\n",
    "bucket='sagemaker-han-batch'\n",
    "prefix = 'batch-images'\n",
    "bucket_path = 'https://s3-{}.amazonaws.com/{}'.format(region,bucket)\n",
    "model_prefix = os.path.join(prefix, 'model')\n",
    "input_prefix = os.path.join(prefix, 'inputs')\n",
    "output_prefix = os.path.join(prefix, 'outputs')\n",
    "inference_prefix = os.path.join(prefix, 'inference')\n",
    "\n",
    "# Local設定\n",
    "base_dir = './'\n",
    "data_dir = os.path.join(base_dir, 'data')\n",
    "model_dir = os.path.join(base_dir, 'model')\n",
    "input_dir = os.path.join(base_dir, 'inputs')\n",
    "output_dir = os.path.join(base_dir, 'outputs')\n",
    "inference_dir = os.path.join(base_dir, 'inference')\n",
    "source_dir = os.path.join(base_dir, 'src')\n",
    "for dir_name in [model_dir, input_dir, output_dir, source_dir, inference_dir]:\n",
    "    make_dir(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f28de312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Sample 100 datas from CIFAR10 dataset\n",
    "\n",
    "from src.utils import create_init_sample_data, convert_np_to_png\n",
    "\n",
    "sampled_imgs = create_init_sample_data(data_dir, 100)\n",
    "convert_np_to_png(inference_dir, sampled_imgs)\n",
    "\n",
    "# Upload png images to S3\n",
    "inference_inputs = sagemaker_session.upload_data(\n",
    "    path=inference_dir, bucket=bucket, key_prefix=inference_prefix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e6c0bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset for the model training\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "train_data = datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "training_data_loader = DataLoader(train_data, batch_size=len(train_data))\n",
    "training_data_loaded = next(iter(training_data_loader))\n",
    "torch.save(training_data_loaded, os.path.join(input_dir, 'training.pt'))\n",
    "\n",
    "test_data_loader = DataLoader(test_data, batch_size=len(test_data))\n",
    "test_data_loaded = next(iter(test_data_loader))\n",
    "torch.save(test_data_loaded, os.path.join(output_dir, 'test.pt'))\n",
    "\n",
    "inputs = sagemaker_session.upload_data(path=input_dir, bucket=bucket, key_prefix=input_prefix)\n",
    "outputs = sagemaker_session.upload_data(path=output_dir, bucket=bucket, key_prefix=output_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "59fbaf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_inputs= f's3://{bucket}/batch-images/inference'\n",
    "inputs= f's3://{bucket}/batch-images/inputs'\n",
    "outputs= f's3://{bucket}/batch-images/outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5274c069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-22 13:51:32 Starting - Starting the training job...\n",
      "2022-08-22 13:51:59 Starting - Preparing the instances for trainingProfilerReport-1661176290: InProgress\n",
      ".........\n",
      "2022-08-22 13:53:33 Downloading - Downloading input data...\n",
      "2022-08-22 13:54:13 Training - Downloading the training image......\n",
      "2022-08-22 13:55:14 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-08-22 13:55:15,163 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-08-22 13:55:15,165 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-08-22 13:55:15,175 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-08-22 13:55:15,186 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-08-22 13:55:15,860 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-08-22 13:55:15,881 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-08-22 13:55:15,899 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-08-22 13:55:15,916 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.m4.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 128,\n",
      "        \"epochs\": 1,\n",
      "        \"lr\": 0.01\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.m4.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"pytorch-training-2022-08-22-13-51-28-555\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-608095525235/pytorch-training-2022-08-22-13-51-28-555/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"resnet\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m4.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m4.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"resnet.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":128,\"epochs\":1,\"lr\":0.01}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=resnet.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m4.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m4.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.m4.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m4.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=resnet\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-608095525235/pytorch-training-2022-08-22-13-51-28-555/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.m4.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":128,\"epochs\":1,\"lr\":0.01},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m4.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"pytorch-training-2022-08-22-13-51-28-555\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-608095525235/pytorch-training-2022-08-22-13-51-28-555/source/sourcedir.tar.gz\",\"module_name\":\"resnet\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m4.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m4.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"resnet.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"128\",\"--epochs\",\"1\",\"--lr\",\"0.01\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=128\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.01\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220816-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 resnet.py --batch-size 128 --epochs 1 --lr 0.01\u001b[0m\n",
      "\u001b[34m[2022-08-22 13:55:18.880 algo-1:27 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220816-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220816-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[2022-08-22 13:55:19.587 algo-1:27 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-08-22 13:55:19.590 algo-1:27 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-08-22 13:55:19.590 algo-1:27 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-08-22 13:55:19.591 algo-1:27 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-08-22 13:55:19.591 algo-1:27 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [12800/50000 (26%)] Loss: -15.599797\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [25600/50000 (51%)] Loss: -75.351624\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [38400/50000 (77%)] Loss: -376.315094\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving the model.\u001b[0m\n",
      "\u001b[34m2022-08-22 14:00:37,306 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-08-22 14:00:37,306 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-08-22 14:00:37,307 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-08-22 14:01:35 Uploading - Uploading generated training model\n",
      "2022-08-22 14:01:35 Completed - Training job completed\n",
      "ProfilerReport-1661176290: NoIssuesFound\n",
      "Training seconds: 489\n",
      "Billable seconds: 489\n"
     ]
    }
   ],
   "source": [
    "# Create Training Container\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point=\"resnet.py\",\n",
    "                    role=role,\n",
    "                    source_dir = \"src\",\n",
    "                    framework_version='1.12.0',\n",
    "                    py_version='py38',\n",
    "                    instance_count=1,\n",
    "                    instance_type='ml.m4.xlarge',\n",
    "                    hyperparameters={\n",
    "                        'batch-size':128,\n",
    "                        'lr': 0.01,\n",
    "                        'epochs': 1,\n",
    "                    })\n",
    "\n",
    "estimator.fit({'training': inputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1344bb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "cifar10_predictor = estimator.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type='ml.m4.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5e52ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "test_input = torch.rand(1,3,32,32)\n",
    "test_output = cifar10_predictor.predict(test_input)\n",
    "print(test_input.shape)\n",
    "print(test_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6325866f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open('sample.jsonl', mode='w') as f:\n",
    "    _dict = {}\n",
    "    for inf_file in os.listdir(inference_dir):\n",
    "        _dict[\"img_path\"] = os.path.join(inference_inputs, inf_file)\n",
    "        json.dump(_dict, f)\n",
    "        f.write('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "988327ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_inference = sagemaker_session.upload_data(path='sample.jsonl', bucket=bucket, key_prefix='batch-images/f_inference')\n",
    "f_inference = sagemaker_session.upload_data(path='sample2.jsonl', bucket=bucket, key_prefix='batch-images/f_inference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "253fc975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-han-batch/batch-images/f_inference/sample2.jsonl'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "eb925323",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "# model_path = 's3://sagemaker-us-west-2-608095525235/pytorch-training-2022-08-22-14-02-01-637/model.tar.gz'\n",
    "model_path = 's3://sagemaker-us-west-2-582981179587/pytorch-training-2022-08-22-11-21-23-382/model.tar.gz'\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data = model_path,\n",
    "                             entry_point='resnet_batch_transform.py',\n",
    "                             source_dir = 'src',\n",
    "                             framework_version='1.12.0',\n",
    "                             py_version='py38',\n",
    "                             role = role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6e1ab917",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_concurrent_transforms = 1\n",
    "max_payload = 1\n",
    "strategy='SingleRecord'\n",
    "split_type=None\n",
    "model_name = 'pytorch-inference-2022-08-23-11-44-10-894'\n",
    "\n",
    "output_s3_path = 's3://{}/batch-images/outputs/bt_real_{}_{}_{}_{}'.format(bucket, max_concurrent_transforms,max_payload,strategy,split_type)\n",
    "# f_inference = f's3://{bucket}/batch-images/f_inference'\n",
    "# transformer.transform(\n",
    "#     data=f_inference,\n",
    "#     data_type=\"S3Prefix\",\n",
    "#     content_type=\"application/jsonlines\",\n",
    "#     wait=True,\n",
    "#     split_type=split_type\n",
    "# )\n",
    "\n",
    "\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "response = sagemaker_client.create_transform_job(\n",
    "    TransformJobName='BatchTransformHanNew',\n",
    "    ModelName=model_name,\n",
    "    MaxConcurrentTransforms=max_concurrent_transforms,\n",
    "    MaxPayloadInMB=max_payload,\n",
    "    BatchStrategy=strategy,\n",
    "    TransformInput={\n",
    "        'DataSource': {\n",
    "            'S3DataSource': {\n",
    "                'S3DataType': 'S3Prefix',\n",
    "                'S3Uri': f_inference\n",
    "            }\n",
    "        },\n",
    "        'ContentType': 'application/jsonlines',\n",
    "        'CompressionType': 'None',\n",
    "        'SplitType': 'Line'\n",
    "    },\n",
    "    TransformOutput={\n",
    "        'S3OutputPath': output_s3_path,\n",
    "        'Accept': 'string',\n",
    "        'AssembleWith': 'None',\n",
    "    },\n",
    "    TransformResources={\n",
    "        'InstanceType': 'ml.m5.xlarge',\n",
    "        'InstanceCount': 1,\n",
    "    }\n",
    ")\n",
    "\n",
    "# transform_job = sagemaker.transformer.Transformer(\n",
    "#     model_name = y_xgboost_21,\n",
    "#     instance_count = 1,\n",
    "#     instance_type = 'ml.m5.large',\n",
    "#     strategy = 'SingleRecord',\n",
    "#     assemble_with = 'Line',\n",
    "#     output_path = output_data_path,\n",
    "#     base_transform_job_name='y-test-batch',\n",
    "#     sagemaker_session=sagemaker.Session(),\n",
    "#     accept = 'text/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ea2048d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_concurrent_transforms = 1\n",
    "max_payload = 1\n",
    "strategy='SingleRecord'\n",
    "split_type=None\n",
    "# strategy='MultiRecord'\n",
    "# split_type='Line'\n",
    "\n",
    "output_s3_path = 's3://{}/batch-images/outputs/bt_real_{}_{}_{}_{}'.format(bucket, max_concurrent_transforms,max_payload,strategy,split_type)\n",
    "\n",
    "transformer = pytorch_model.transformer(instance_count=1,\n",
    "                              instance_type=\"ml.m5.xlarge\",\n",
    "                              max_concurrent_transforms=max_concurrent_transforms,\n",
    "                              max_payload=max_payload,  # 1MB\n",
    "                              strategy=strategy,\n",
    "                              output_path=output_s3_path)\n",
    "\n",
    "f_inference = f's3://{bucket}/batch-images/f_inference'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a14655a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................\u001b[34mWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:00,979 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:01,088 [INFO ] main org.pytorch.serve.ModelServer - \u001b[0m\n",
      "\u001b[34mTorchserve version: 0.6.0\u001b[0m\n",
      "\u001b[34mTS Home: /opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mCurrent directory: /\u001b[0m\n",
      "\u001b[34mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[34mNumber of GPUs: 0\u001b[0m\n",
      "\u001b[34mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[34mMax heap size: 2968 M\u001b[0m\n",
      "\u001b[34mPython executable: /opt/conda/bin/python3.8\u001b[0m\n",
      "\u001b[34mConfig file: /etc/sagemaker-ts.properties\u001b[0m\n",
      "\u001b[34mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mMetrics address: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mInitial Models: model=/opt/ml/model\u001b[0m\n",
      "\u001b[34mLog dir: /logs\u001b[0m\n",
      "\u001b[34mMetrics dir: /logs\u001b[0m\n",
      "\u001b[34mNetty threads: 0\u001b[0m\n",
      "\u001b[34mNetty client threads: 0\u001b[0m\n",
      "\u001b[34mDefault workers per model: 4\u001b[0m\n",
      "\u001b[34mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[34mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[34mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[34mLimit Maximum Image Pixels: true\u001b[0m\n",
      "\u001b[34mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[34mAllowed Urls: [file://.*|http(s)?://.*]\u001b[0m\n",
      "\u001b[34mCustom python dependency for model allowed: false\u001b[0m\n",
      "\u001b[34mMetrics report format: prometheus\u001b[0m\n",
      "\u001b[34mEnable metrics API: true\u001b[0m\n",
      "\u001b[34mWorkflow Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mModel config: N/A\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:01,096 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:01,120 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:01,124 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:01,125 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:01,127 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:01,154 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:01,482 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:01,482 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:01,496 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel server started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,020 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,174 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:832d6fd25590,timestamp:1661249642\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,176 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:48.206298828125|#Level:Host|#hostname:832d6fd25590,timestamp:1661249642\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,177 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:7.658832550048828|#Level:Host|#hostname:832d6fd25590,timestamp:1661249642\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,178 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:13.7|#Level:Host|#hostname:832d6fd25590,timestamp:1661249642\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,179 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:14359.3125|#Level:Host|#hostname:832d6fd25590,timestamp:1661249642\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,180 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:887.81640625|#Level:Host|#hostname:832d6fd25590,timestamp:1661249642\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,181 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:7.8|#Level:Host|#hostname:832d6fd25590,timestamp:1661249642\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,565 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,568 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]67\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,568 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,569 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,577 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,612 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,615 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249642615\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,699 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,770 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,772 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]62\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,772 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,773 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,773 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,798 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,798 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249642798\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,848 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,849 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,851 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]64\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,851 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,851 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,851 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,869 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249642869\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,869 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:02,919 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,012 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,013 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]63\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,040 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,041 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,049 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,094 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,095 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249643095\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,131 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,264 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,264 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,265 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,267 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,267 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,268 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,268 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,268 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,268 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,269 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,269 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,269 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,273 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,275 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,275 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,276 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,276 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,276 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,277 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,277 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,280 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,280 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,280 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,280 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,281 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,281 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,281 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,281 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,282 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,282 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,282 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,298 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,315 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,316 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,317 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,318 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,408 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,408 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,409 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,409 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,410 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,410 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,411 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,411 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,411 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,412 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,412 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,413 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,413 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,414 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,414 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,415 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,415 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,416 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,416 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,417 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,417 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,418 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,418 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,418 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,419 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,419 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,420 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,420 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,421 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,421 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,421 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,431 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,427 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,433 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,433 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,434 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,434 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,431 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,484 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,485 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,485 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,491 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,492 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,492 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,492 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,492 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,492 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,497 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,498 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,586 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,590 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,591 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,591 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,587 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,592 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,592 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,593 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,593 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,595 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,599 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,754 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,837 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,845 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:03,846 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,507 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,511 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]143\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,512 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,512 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,512 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,536 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,536 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249645536\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,661 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,674 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,678 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]147\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,679 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:05,661 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:05,674 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:05,678 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]147\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:05,679 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,679 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,679 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,697 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,698 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249645698\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,773 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,774 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]150\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,775 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,778 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,775 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,775 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,795 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,795 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249645795\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,888 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,966 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,968 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]155\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,968 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,970 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,970 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,981 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:05,983 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249645983\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,035 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,193 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,194 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,194 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,194 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,194 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:05,679 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:05,679 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:05,697 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:05,698 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249645698\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:05,773 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:05,774 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]150\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:05,775 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:05,778 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:05,775 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:05,775 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:05,795 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:05,795 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249645795\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:05,888 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:05,966 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:05,968 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]155\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:05,968 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:05,970 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:05,970 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:05,981 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:05,983 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249645983\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,035 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,193 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,194 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,194 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,194 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,194 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,195 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,195 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,195 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,195 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,195 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,195 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,196 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,196 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,196 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,196 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,197 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,197 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,197 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,197 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,197 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,197 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,198 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,198 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,198 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,198 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,198 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,199 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,199 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,199 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,199 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,199 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,195 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,195 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,195 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,195 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,195 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,195 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,196 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,196 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,196 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,196 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,197 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,197 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,197 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,197 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,197 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,197 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,198 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,198 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,198 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,198 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,198 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,199 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,199 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,199 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,199 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,199 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,212 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,213 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,214 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,214 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,214 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,300 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,301 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,301 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,301 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,301 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,301 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,301 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,301 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,302 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,302 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,302 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,303 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,303 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,304 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,305 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,311 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,311 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,312 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,306 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,314 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,313 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,314 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,212 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,213 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,214 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,214 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,214 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 1 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,300 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,301 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,301 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,301 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,301 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,301 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,301 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,301 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,302 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,302 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,302 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,303 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,303 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,304 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,305 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,311 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,311 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,312 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,306 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,314 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,313 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,314 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,336 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,336 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,337 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,337 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,337 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,337 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,337 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,338 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,338 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,338 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,338 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,338 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,338 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,339 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,339 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,339 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,339 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,339 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,340 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,340 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,340 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,340 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,340 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,340 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,341 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,341 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,341 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,341 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,341 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,336 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,336 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,337 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,337 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,337 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,337 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,337 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,338 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,338 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,338 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,338 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,338 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,338 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,339 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,339 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,339 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,339 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,339 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,340 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,340 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,340 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,340 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,340 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,340 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,341 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,341 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,341 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,341 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,341 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,341 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,341 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,343 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,344 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,344 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,345 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,345 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,389 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,390 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,390 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,390 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,390 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,390 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,390 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,391 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,391 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,391 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,391 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,391 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,391 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,392 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,392 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,392 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,390 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,393 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,393 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,393 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,394 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,394 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,341 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,341 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,343 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,344 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,344 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,345 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,345 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,389 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,390 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,390 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,390 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,390 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,390 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,390 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,391 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,391 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,391 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,391 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,391 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,391 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,392 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,392 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,392 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,390 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,393 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,393 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,393 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,394 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 1 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,394 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,414 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,415 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,521 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,414 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,415 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,521 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,690 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,740 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:06,740 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,690 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,740 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:06,740 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,519 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,522 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]199\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,523 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,519 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,522 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]199\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,523 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,523 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,531 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249648531\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,535 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,535 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,536 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,574 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,575 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]203\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,575 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,576 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,576 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,579 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,579 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249648579\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,523 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,531 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249648531\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,535 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,535 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,536 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,574 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,575 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]203\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,575 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,576 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,576 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,579 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,579 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249648579\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,586 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,662 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,663 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]206\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,664 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,666 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,664 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,586 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,662 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,663 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]206\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,664 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,666 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,664 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,667 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,667 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249648667\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,672 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,957 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,957 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,958 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,959 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,961 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,961 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,958 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,962 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,977 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,977 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,977 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,977 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,978 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,978 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,978 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,978 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,978 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,978 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,978 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,978 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,978 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,978 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,978 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,978 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,978 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,979 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,979 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,979 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,667 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,667 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249648667\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,672 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,957 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,957 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,958 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,959 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,961 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,961 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,958 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,962 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,977 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,977 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,977 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,977 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,978 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,978 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,978 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,978 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,978 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,978 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,978 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,978 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,978 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,978 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,978 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,978 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,978 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,979 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,979 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,979 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,979 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,979 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,979 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,979 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,979 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,979 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,979 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,979 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,979 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,979 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,979 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,991 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,992 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,992 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,993 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:08,993 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 2 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,048 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,050 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]209\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,051 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,051 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,051 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,055 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,056 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249649056\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,057 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,104 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,105 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,105 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,107 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,108 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,108 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,108 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,108 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,109 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,979 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,979 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,979 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,979 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,979 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,979 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,979 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,979 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,979 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,979 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,979 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,991 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,992 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,992 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,993 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:08,993 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 2 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,048 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,050 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]209\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,051 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,051 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,051 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,055 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,056 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249649056\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,057 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,104 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,105 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,105 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,107 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,108 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,108 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,108 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,108 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,109 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,109 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,109 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,110 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,110 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,110 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,110 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,113 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,114 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,114 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,114 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,114 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,115 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,115 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,115 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,116 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,116 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,116 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,116 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,117 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,117 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,117 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,118 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,118 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,119 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,119 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,120 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,120 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 2 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,234 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,234 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,109 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,109 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,110 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,110 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,110 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,110 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,113 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,114 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,114 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,114 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,114 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,115 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,115 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,115 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,116 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,116 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,116 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,116 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,117 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,117 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,117 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,118 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,118 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,119 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,119 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,120 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,120 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 2 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,234 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,234 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,350 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,350 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,351 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,351 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,351 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,351 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,352 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,352 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,352 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,353 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,353 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,353 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,353 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,354 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,354 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,354 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,354 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,355 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,356 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,356 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,356 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,356 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,357 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,357 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,357 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,358 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,358 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,358 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,358 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,359 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,359 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,350 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,350 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,350 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,351 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,351 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,351 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,351 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,352 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,352 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,352 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,353 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,353 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,353 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,353 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,354 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,354 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,354 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,354 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,355 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,356 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,356 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,356 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,356 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,357 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,357 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,357 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,358 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,358 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,358 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,358 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,359 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,359 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,350 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,360 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,360 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,366 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,366 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 2 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,457 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,457 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,497 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,360 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,360 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,366 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,366 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 2 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,457 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,457 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,497 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,689 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:09,689 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:10,325 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.255.130:54282 \"GET /ping HTTP/1.1\" 200 24\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:10,325 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:832d6fd25590,timestamp:1661249650\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:10,334 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:54284 \"GET /execution-parameters HTTP/1.1\" 404 2\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:10,334 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:832d6fd25590,timestamp:1661249650\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,689 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:09,689 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:10,325 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.255.130:54282 \"GET /ping HTTP/1.1\" 200 24\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:10,325 [INFO ] W-9003-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:832d6fd25590,timestamp:1661249650\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:10,334 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:54284 \"GET /execution-parameters HTTP/1.1\" 404 2\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:10,334 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:832d6fd25590,timestamp:1661249650\u001b[0m\n",
      "\u001b[32m2022-08-23T10:14:10.340:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=1, BatchStrategy=SINGLE_RECORD\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:11,801 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:11,803 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]255\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:11,803 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:11,803 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:11,803 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:11,804 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:11,805 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249651805\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:11,806 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,066 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:11,801 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:11,803 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]255\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:11,803 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:11,803 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:11,803 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:11,804 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:11,805 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249651805\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:11,806 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,066 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,072 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]258\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,072 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,073 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,073 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,074 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249652074\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,075 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,075 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,142 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]263\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,143 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,144 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,143 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,146 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249652146\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,146 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,147 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,258 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,258 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,258 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,258 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,258 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,258 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,258 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,072 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]258\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,072 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,073 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,073 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,074 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249652074\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,075 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,075 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,142 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]263\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,143 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,144 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,143 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,146 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249652146\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,146 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,147 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,258 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,258 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,258 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,258 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,258 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,258 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,258 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,260 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,260 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,260 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,260 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,260 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,260 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,260 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,260 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,273 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,273 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,274 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,274 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,274 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,390 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,390 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,390 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,390 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,390 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,391 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,391 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,391 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,391 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,259 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,260 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,260 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,260 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,260 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,260 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,260 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,260 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,260 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,273 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,273 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,274 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,274 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,274 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,390 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,390 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,390 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,390 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,390 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,391 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,391 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,391 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,391 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,391 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,391 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,391 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,392 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,392 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,392 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,392 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,392 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,392 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,393 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,393 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,393 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,393 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,393 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,393 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,393 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,394 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,394 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,394 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,394 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,394 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,395 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,395 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,395 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,396 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,396 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 3 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,394 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,403 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,576 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,576 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,576 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,391 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,391 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,391 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,392 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,392 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,392 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,392 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,392 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,392 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,393 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,393 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,393 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,393 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,393 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,393 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,393 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,394 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,394 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,394 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,394 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,394 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,395 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,395 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,395 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,396 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,396 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 3 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,394 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,403 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,576 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,576 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,576 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,576 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,576 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,577 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,577 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,577 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,577 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,577 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,578 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,578 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,578 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 3 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,577 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,578 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,576 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,576 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,577 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,577 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,577 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,577 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,577 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,578 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,578 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,578 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 3 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,577 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,578 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,662 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,663 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]270\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,663 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,663 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,664 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,666 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249652666\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,667 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,667 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,739 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,739 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,812 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,865 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,865 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,866 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,867 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,866 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,868 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,868 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,868 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 3 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,868 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,662 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,663 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]270\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,663 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,663 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,664 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,666 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249652666\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,667 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,667 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,739 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,739 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,812 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,865 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,865 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,866 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,867 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,866 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,868 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,868 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,868 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 3 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,868 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:12,885 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:13,144 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:12,885 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:13,144 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,155 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,156 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]332\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,157 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,160 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249656160\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,157 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,162 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,163 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,155 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,156 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]332\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,157 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,160 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249656160\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,157 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,162 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,163 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,172 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,346 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,347 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]336\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,348 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,349 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,349 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,352 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,352 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249656352\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,353 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,172 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,346 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,347 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]336\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,348 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,349 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,349 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,352 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,352 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249656352\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,353 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,603 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,604 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,604 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,604 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,604 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,603 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,604 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,604 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,604 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,604 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,606 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,606 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,606 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,606 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,606 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,606 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,606 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,606 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,606 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,606 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,607 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,607 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,607 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,787 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,789 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]340\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,789 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,789 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,790 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,791 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249656791\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,792 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,605 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,606 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,606 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,606 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,606 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,606 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,606 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,606 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,606 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,606 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,606 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,607 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,607 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,607 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,787 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,789 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]340\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,789 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,789 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,790 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,791 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249656791\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,792 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,792 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,891 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,892 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,892 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,893 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,893 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,892 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,893 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,894 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,894 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,894 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 5 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,894 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,920 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:16,922 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,019 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,021 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]350\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,021 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,021 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,023 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249657023\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,023 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,024 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,024 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,091 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,092 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,092 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,092 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,092 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,092 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 5 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,233 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,792 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,891 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,892 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,892 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,893 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,893 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,892 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,893 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,894 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,894 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,894 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 5 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,894 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,920 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:16,922 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,019 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,021 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]350\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,021 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,021 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,023 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249657023\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,023 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,024 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,024 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,091 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,092 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,092 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,092 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,092 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,092 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 5 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,233 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,275 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,275 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,275 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,275 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,275 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,275 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,275 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,275 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,275 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,275 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,275 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,275 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,275 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,276 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,276 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,276 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,276 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,276 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,276 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,276 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,276 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,276 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,279 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,279 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,279 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,279 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,279 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,279 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,280 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,280 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,280 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,275 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,275 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,275 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,275 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,275 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,275 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,275 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,275 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,275 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,275 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,275 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,275 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,275 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,276 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,276 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,276 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,276 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,276 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,276 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,276 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,276 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,276 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,279 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,279 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,279 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,279 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,279 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,279 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,280 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,280 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,280 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,280 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,280 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,280 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,280 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,280 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 5 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,395 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,560 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:17,560 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,280 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,280 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,280 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,280 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,280 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 5 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,395 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,560 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:17,560 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:22,667 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:22,668 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:22,669 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:22,671 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:22,669 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:22,671 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:22,672 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:22,673 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:22,673 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:22,673 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:22,917 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:22,918 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]388\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:22,918 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:22,918 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:22,918 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:22,920 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249662920\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:22,921 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:22,922 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,085 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,093 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,094 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]395\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:22,667 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:22,668 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:22,669 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:22,671 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:22,669 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:22,671 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:22,672 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:22,673 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:22,673 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:22,673 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:22,917 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:22,918 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]388\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:22,918 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:22,918 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:22,918 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:22,920 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249662920\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:22,921 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:22,922 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,085 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,093 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,094 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]395\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,095 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,095 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,099 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,100 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249663100\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,101 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,101 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,276 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,276 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,276 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,276 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,276 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,276 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,276 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,276 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,277 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,278 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,278 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,278 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,278 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 8 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,276 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,278 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,424 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,424 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,424 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,424 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,424 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,095 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,095 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,099 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,100 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249663100\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,101 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,101 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,276 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,276 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,276 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,276 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,276 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,276 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,276 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,276 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,277 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,278 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,278 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,278 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,278 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 8 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,276 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,278 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,424 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,424 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,424 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,424 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,424 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,425 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,425 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,425 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,425 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,425 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,426 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,426 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,426 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,426 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,426 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,426 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,426 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,426 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,426 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,426 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,426 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,426 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,427 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,427 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,427 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,427 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,427 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,427 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,427 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,427 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,427 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,427 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,425 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,425 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,425 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,425 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,425 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,426 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,426 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,426 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,426 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,426 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,426 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,426 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,426 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,426 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,426 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,426 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,426 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,427 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,427 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,427 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,427 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,427 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,427 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,427 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,427 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,427 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,427 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,427 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,427 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,427 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,428 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 8 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,431 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,431 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]399\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,431 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,431 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,432 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,433 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249663433\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,443 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,443 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,427 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,427 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,427 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,428 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 8 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,431 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,431 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]399\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,431 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,431 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,432 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,433 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249663433\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,443 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,443 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,620 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,620 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,620 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,620 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,620 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,620 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,620 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,620 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,620 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,620 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,620 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,620 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,620 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,620 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,620 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,620 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,620 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,620 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,622 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,622 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,623 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,623 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,623 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 8 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,627 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,693 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,693 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,893 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:23,893 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,622 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,622 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,623 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,623 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,623 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 8 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,627 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,693 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,693 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,893 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:23,893 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,325 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,325 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]436\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,325 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,325 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,325 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,326 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,327 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249671327\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,327 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,520 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,521 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,521 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,521 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,521 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,522 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,522 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,522 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,522 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,522 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,522 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,523 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,523 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,523 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,523 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,523 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,524 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,524 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,524 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,524 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,524 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,325 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,325 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]436\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,325 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,325 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,325 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,326 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,327 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249671327\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,327 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,520 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,521 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,521 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,521 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,521 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,522 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,522 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,522 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,522 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,522 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,522 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,523 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,523 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,523 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,523 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,523 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,524 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,524 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,524 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,524 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,524 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,524 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,525 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,525 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,525 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,525 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,525 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,526 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,526 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,526 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,526 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,527 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,527 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,528 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,528 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,529 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,524 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,525 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,525 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,525 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,525 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,525 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,526 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,526 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,526 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,526 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,527 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,527 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,528 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,528 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,529 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,905 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:31,905 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,146 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,905 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:31,905 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,146 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,147 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]443\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,147 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,147 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,147 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,148 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,149 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249672149\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,149 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,429 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,430 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]450\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,430 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,430 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,430 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,434 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,435 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249672435\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,435 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,503 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,503 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,503 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,503 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,503 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,503 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,503 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,503 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,503 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,503 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,503 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,503 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,147 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]443\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,147 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,147 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,147 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,148 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,149 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249672149\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,149 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,429 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,430 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]450\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,430 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,430 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,430 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,434 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,435 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249672435\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,435 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,503 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,503 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,503 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,503 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,503 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,503 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,503 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,503 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,503 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,503 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,503 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,503 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,511 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,512 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,512 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,513 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,513 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 13 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,504 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,511 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,512 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,512 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,513 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,513 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 13 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,616 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,620 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]457\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,621 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,622 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,622 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249672622\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,625 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,625 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,695 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,696 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,696 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,696 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,696 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,697 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,697 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,697 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,697 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,697 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,698 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,698 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 13 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,697 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,699 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,788 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,788 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,834 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,834 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,616 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,620 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]457\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,621 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,621 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,622 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,622 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249672622\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,625 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,625 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,695 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,696 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,696 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,696 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,696 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,697 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,697 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,697 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,697 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,697 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,698 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,698 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 13 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,697 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,699 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,788 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,788 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,834 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,834 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,836 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,836 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,836 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,836 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:32,836 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 13 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:33,013 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:33,124 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:33,124 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,835 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,836 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,836 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,836 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,836 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:32,836 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 13 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:33,013 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:33,124 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:33,124 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:45,348 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:45,348 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:45,348 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:45,348 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:45,348 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:45,348 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:45,349 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:45,349 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:45,349 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:45,349 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:45,349 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:45,350 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:45,350 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:45,350 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:45,350 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:45,350 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:45,355 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:45,355 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:45,348 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:45,348 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:45,348 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:45,348 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:45,348 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:45,348 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:45,349 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:45,349 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:45,349 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:45,349 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:45,349 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:45,350 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:45,350 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:45,350 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:45,350 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:45,350 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:45,355 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:45,355 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:45,644 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,255 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,256 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]501\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,256 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,257 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,257 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,259 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,259 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249686259\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,267 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:45,644 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,255 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,256 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]501\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,256 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,257 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,257 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,259 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,259 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249686259\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,267 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,556 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,556 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,556 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,556 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,556 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,556 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,556 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,563 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,564 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,564 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,564 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,564 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 21 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,555 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,556 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,556 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,556 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,556 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,556 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,556 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,556 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,563 [INFO ] epollEventLoopGroup-5-6 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,564 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,564 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,564 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,564 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 21 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,675 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,675 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]505\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,676 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,676 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,677 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,678 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,678 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249686678\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,679 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,801 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,801 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]509\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,801 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,801 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,801 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,802 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249686802\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,804 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,804 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,926 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,927 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,927 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,927 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,928 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,927 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,928 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,928 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,928 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,928 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,928 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,928 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,928 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,929 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,929 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,929 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,929 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,929 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,929 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,929 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,929 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,929 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,929 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,929 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,929 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,929 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,930 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,930 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,930 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,930 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,930 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,675 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,675 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]505\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,676 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,676 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,677 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,678 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,678 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249686678\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,679 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,801 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,801 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]509\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,801 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,801 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,801 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,802 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249686802\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,804 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,804 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,926 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,927 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,927 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,927 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,928 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,927 [INFO ] epollEventLoopGroup-5-7 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,928 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,928 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,928 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,928 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,928 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,928 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,928 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,929 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,929 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,929 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,929 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,929 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,929 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,929 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,929 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,929 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,929 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,929 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,929 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,929 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,930 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,930 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,930 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,930 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,930 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,930 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,930 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,930 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,935 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,935 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,935 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 21 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,945 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:46,946 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,057 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,057 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,057 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,057 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,057 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,057 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,057 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,057 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,057 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,058 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,058 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,059 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,059 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,059 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 21 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,060 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,202 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:14:47,367 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,930 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,930 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,930 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,935 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,935 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,935 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 21 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,945 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:46,946 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,057 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,057 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,057 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,057 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,057 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,057 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,057 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,057 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,057 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,058 [INFO ] epollEventLoopGroup-5-8 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,058 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,059 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,059 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,059 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 21 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,058 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,060 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,202 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:14:47,367 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/ts/metrics/process_memory_metric.py\", line 20, in get_cpu_usage\n",
      "    process = psutil.Process(int(pid))\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/__init__.py\", line 332, in __init__\n",
      "    self._init(pid)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/__init__.py\", line 373, in _init\n",
      "    raise NoSuchProcess(pid, msg='process PID not found')\u001b[0m\n",
      "\u001b[35mpsutil.NoSuchProcess: process PID not found (pid=505)\u001b[0m\n",
      "\u001b[35mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1089, in emit\n",
      "    self.flush()\n",
      "  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1069, in flush\n",
      "    self.stream.flush()\u001b[0m\n",
      "\u001b[35mBrokenPipeError: [Errno 32] Broken pipe\u001b[0m\n",
      "\u001b[35mCall stack:\n",
      "  File \"ts/metrics/metric_collector.py\", line 29, in <module>\n",
      "    check_process_mem_usage(sys.stdin)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/ts/metrics/process_memory_metric.py\", line 40, in check_process_mem_usage\n",
      "    logging.info(\"%s:%d\", process, get_cpu_usage(process))\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/ts/metrics/process_memory_metric.py\", line 22, in get_cpu_usage\n",
      "    logging.error(\"Failed get process for pid: %s\", pid, exc_info=True)\u001b[0m\n",
      "\u001b[35mMessage: 'Failed get process for pid: %s'\u001b[0m\n",
      "\u001b[35mArguments: ('505',)\u001b[0m\n",
      "\u001b[35m--- Logging error ---\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1089, in emit\n",
      "    self.flush()\n",
      "  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1069, in flush\n",
      "    self.stream.flush()\u001b[0m\n",
      "\u001b[35mBrokenPipeError: [Errno 32] Broken pipe\u001b[0m\n",
      "\u001b[35mCall stack:\n",
      "  File \"ts/metrics/metric_collector.py\", line 29, in <module>\n",
      "    check_process_mem_usage(sys.stdin)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/ts/metrics/process_memory_metric.py\", line 40, in check_process_mem_usage\n",
      "    logging.info(\"%s:%d\", process, get_cpu_usage(process))\u001b[0m\n",
      "\u001b[35mMessage: '%s:%d'\u001b[0m\n",
      "\u001b[35mArguments: ('505', 0)\u001b[0m\n",
      "\u001b[35m--- Logging error ---\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/_common.py\", line 442, in wrapper\n",
      "    ret = self._cache[fun]\u001b[0m\n",
      "\u001b[35mAttributeError: _cache\u001b[0m\n",
      "\u001b[35mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/_pslinux.py\", line 1642, in wrapper\n",
      "    return fun(self, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/_common.py\", line 445, in wrapper\n",
      "    return fun(self)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/_pslinux.py\", line 1684, in _parse_stat_file\n",
      "    data = bcat(\"%s/%s/stat\" % (self._procfs_path, self.pid))\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/_common.py\", line 775, in bcat\n",
      "    return cat(fname, fallback=fallback, _open=open_binary)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/_common.py\", line 763, in cat\n",
      "    with _open(fname) as f:\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/_common.py\", line 727, in open_binary\n",
      "    return open(fname, \"rb\", buffering=FILE_READ_BUFFER_SIZE)\u001b[0m\n",
      "\u001b[35mFileNotFoundError: [Errno 2] No such file or directory: '/proc/509/stat'\u001b[0m\n",
      "\u001b[35mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/__init__.py\", line 361, in _init\n",
      "    self.create_time()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/__init__.py\", line 714, in create_time\n",
      "    self._create_time = self._proc.create_time()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/_pslinux.py\", line 1642, in wrapper\n",
      "    return fun(self, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/_pslinux.py\", line 1852, in create_time\n",
      "    ctime = float(self._parse_stat_file()['create_time'])\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/_pslinux.py\", line 1649, in wrapper\n",
      "    raise NoSuchProcess(self.pid, self._name)\u001b[0m\n",
      "\u001b[35mpsutil.NoSuchProcess: process no longer exists (pid=509)\u001b[0m\n",
      "\u001b[35mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/ts/metrics/process_memory_metric.py\", line 20, in get_cpu_usage\n",
      "    process = psutil.Process(int(pid))\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/__init__.py\", line 332, in __init__\n",
      "    self._init(pid)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/__init__.py\", line 373, in _init\n",
      "    raise NoSuchProcess(pid, msg='process PID not found')\u001b[0m\n",
      "\u001b[35mpsutil.NoSuchProcess: process PID not found (pid=509)\u001b[0m\n",
      "\u001b[35mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1089, in emit\n",
      "    self.flush()\n",
      "  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1069, in flush\n",
      "    self.stream.flush()\u001b[0m\n",
      "\u001b[35mBrokenPipeError: [Errno 32] Broken pipe\u001b[0m\n",
      "\u001b[35mCall stack:\n",
      "  File \"ts/metrics/metric_collector.py\", line 29, in <module>\n",
      "    check_process_mem_usage(sys.stdin)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/ts/metrics/process_memory_metric.py\", line 40, in check_process_mem_usage\n",
      "    logging.info(\"%s:%d\", process, get_cpu_usage(process))\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/ts/metrics/process_memory_metric.py\", line 22, in get_cpu_usage\n",
      "    logging.error(\"Failed get process for pid: %s\", pid, exc_info=True)\u001b[0m\n",
      "\u001b[35mMessage: 'Failed get process for pid: %s'\u001b[0m\n",
      "\u001b[35mArguments: ('509',)\u001b[0m\n",
      "\u001b[35m--- Logging error ---\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1089, in emit\n",
      "    self.flush()\n",
      "  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1069, in flush\n",
      "    self.stream.flush()\u001b[0m\n",
      "\u001b[35mBrokenPipeError: [Errno 32] Broken pipe\u001b[0m\n",
      "\u001b[35mCall stack:\n",
      "  File \"ts/metrics/metric_collector.py\", line 29, in <module>\n",
      "    check_process_mem_usage(sys.stdin)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/ts/metrics/process_memory_metric.py\", line 40, in check_process_mem_usage\n",
      "    logging.info(\"%s:%d\", process, get_cpu_usage(process))\u001b[0m\n",
      "\u001b[35mMessage: '%s:%d'\u001b[0m\n",
      "\u001b[35mArguments: ('509', 0)\u001b[0m\n",
      "\u001b[35mException ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>\u001b[0m\n",
      "\u001b[35mBrokenPipeError: [Errno 32] Broken pipe\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:06,994 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:06,995 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]542\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:06,995 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:06,995 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:06,995 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:06,996 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:06,996 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249706996\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:06,997 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,155 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,156 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,156 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,156 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,157 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,157 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,157 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,157 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,158 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,159 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:06,994 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:06,995 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]542\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:06,995 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:06,995 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:06,995 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:06,996 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:06,996 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249706996\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:06,997 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,155 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,156 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,156 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,156 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,157 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,157 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,157 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,157 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,158 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,159 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,159 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,159 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,159 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,159 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,159 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,159 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,159 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,159 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,159 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,159 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,160 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,160 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,160 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,160 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,160 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,160 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,160 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,160 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,160 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,160 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,160 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,160 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,160 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,161 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,161 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,161 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,440 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:07,440 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,159 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,159 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,159 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,159 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,159 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,159 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,159 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,159 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,159 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,159 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,160 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,160 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,160 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,160 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,160 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,160 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,160 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,160 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,160 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,160 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,160 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,160 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,160 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,161 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,161 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,161 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,440 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:07,440 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,222 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,223 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]555\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,223 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,223 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,223 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,224 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249708224\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,235 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,235 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,466 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,466 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,467 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,467 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,467 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,467 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,468 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,468 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,222 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,223 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]555\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,223 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,223 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,223 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,224 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249708224\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,235 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,235 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,466 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,466 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,467 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,467 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,467 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,467 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,468 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,468 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,468 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,468 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,468 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,469 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,469 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,469 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,469 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,469 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,470 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,470 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,470 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,470 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,471 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,471 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,471 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,471 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,472 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,472 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,472 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,472 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,472 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,473 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,473 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,474 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,468 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,468 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,468 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,469 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,469 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,469 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,469 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,469 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,470 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,470 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,470 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,470 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,471 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,471 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,471 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,471 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,472 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,472 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,472 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,472 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,472 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,473 [INFO ] W-9002-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,473 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,474 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9002 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,474 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,475 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,475 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,475 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 34 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,474 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,475 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,475 [WARN ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,475 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9002 in 34 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,819 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,819 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,891 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,892 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]562\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,892 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,892 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,893 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,894 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,894 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249708894\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:08,895 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,000 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,001 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]566\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,002 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,002 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,003 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249709003\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,004 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,004 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,004 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,140 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,140 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,819 [INFO ] W-9002-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,819 [INFO ] W-9002-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9002-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,891 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,892 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]562\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,892 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,892 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,893 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,894 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,894 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249708894\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:08,895 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,000 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,001 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]566\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,002 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,002 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,003 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661249709003\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,004 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,004 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,004 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,140 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,140 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,143 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,143 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,144 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,144 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,144 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 34 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,226 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,226 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,226 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,226 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,226 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,226 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,226 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,227 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,227 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,227 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,227 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,227 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 34 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,226 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,228 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,396 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,396 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-23T10:15:09,505 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/resnet_batch_transform.py\", line 14, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     import s3fs\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,141 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named 's3fs'\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,143 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,143 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,144 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,144 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,144 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 34 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,226 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,226 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,226 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,226 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,226 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,226 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,226 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,227 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9003 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,227 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,227 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,227 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,227 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9003 in 34 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,226 [INFO ] W-9003-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,228 [INFO ] W-9003-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,396 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,396 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-23T10:15:09,505 [INFO ] W-9003-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9003-model_1.0-stderr\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/han/Desktop/Fusic/2022/aws_batch_transform/batch_image/batch_transform_image_input.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/han/Desktop/Fusic/2022/aws_batch_transform/batch_image/batch_transform_image_input.ipynb#ch0000014?line=0'>1</a>\u001b[0m f_inference \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms3://\u001b[39m\u001b[39m{\u001b[39;00mbucket\u001b[39m}\u001b[39;00m\u001b[39m/batch-images/f_inference\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/han/Desktop/Fusic/2022/aws_batch_transform/batch_image/batch_transform_image_input.ipynb#ch0000014?line=1'>2</a>\u001b[0m transformer\u001b[39m.\u001b[39;49mtransform(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/han/Desktop/Fusic/2022/aws_batch_transform/batch_image/batch_transform_image_input.ipynb#ch0000014?line=2'>3</a>\u001b[0m     data\u001b[39m=\u001b[39;49mf_inference,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/han/Desktop/Fusic/2022/aws_batch_transform/batch_image/batch_transform_image_input.ipynb#ch0000014?line=3'>4</a>\u001b[0m     data_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mS3Prefix\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/han/Desktop/Fusic/2022/aws_batch_transform/batch_image/batch_transform_image_input.ipynb#ch0000014?line=4'>5</a>\u001b[0m     content_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mapplication/jsonlines\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/han/Desktop/Fusic/2022/aws_batch_transform/batch_image/batch_transform_image_input.ipynb#ch0000014?line=5'>6</a>\u001b[0m     wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/han/Desktop/Fusic/2022/aws_batch_transform/batch_image/batch_transform_image_input.ipynb#ch0000014?line=6'>7</a>\u001b[0m     split_type\u001b[39m=\u001b[39;49msplit_type\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/han/Desktop/Fusic/2022/aws_batch_transform/batch_image/batch_transform_image_input.ipynb#ch0000014?line=7'>8</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aws_batch_transform-pg7SNxiG/lib/python3.9/site-packages/sagemaker/workflow/pipeline_context.py:248\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m     run_func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    246\u001b[0m     \u001b[39mreturn\u001b[39;00m self_instance\u001b[39m.\u001b[39msagemaker_session\u001b[39m.\u001b[39mcontext\n\u001b[0;32m--> 248\u001b[0m \u001b[39mreturn\u001b[39;00m run_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aws_batch_transform-pg7SNxiG/lib/python3.9/site-packages/sagemaker/transformer.py:243\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, data, data_type, content_type, compression_type, split_type, job_name, input_filter, output_filter, join_source, experiment_config, model_client_config, wait, logs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlatest_transform_job \u001b[39m=\u001b[39m _TransformJob\u001b[39m.\u001b[39mstart_new(\n\u001b[1;32m    229\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    230\u001b[0m     data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m     model_client_config,\n\u001b[1;32m    240\u001b[0m )\n\u001b[1;32m    242\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[0;32m--> 243\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlatest_transform_job\u001b[39m.\u001b[39;49mwait(logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aws_batch_transform-pg7SNxiG/lib/python3.9/site-packages/sagemaker/transformer.py:440\u001b[0m, in \u001b[0;36m_TransformJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, logs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    439\u001b[0m     \u001b[39mif\u001b[39;00m logs:\n\u001b[0;32m--> 440\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49mlogs_for_transform_job(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjob_name, wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    441\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session\u001b[39m.\u001b[39mwait_for_transform_job(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aws_batch_transform-pg7SNxiG/lib/python3.9/site-packages/sagemaker/session.py:4006\u001b[0m, in \u001b[0;36mSession.logs_for_transform_job\u001b[0;34m(self, job_name, wait, poll)\u001b[0m\n\u001b[1;32m   4003\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39m==\u001b[39m LogState\u001b[39m.\u001b[39mCOMPLETE:\n\u001b[1;32m   4004\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 4006\u001b[0m time\u001b[39m.\u001b[39;49msleep(poll)\n\u001b[1;32m   4008\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39m==\u001b[39m LogState\u001b[39m.\u001b[39mJOB_COMPLETE:\n\u001b[1;32m   4009\u001b[0m     state \u001b[39m=\u001b[39m LogState\u001b[39m.\u001b[39mCOMPLETE\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "f_inference = f's3://{bucket}/batch-images/f_inference'\n",
    "transformer.transform(\n",
    "    data=f_inference,\n",
    "    data_type=\"S3Prefix\",\n",
    "    content_type=\"application/jsonlines\",\n",
    "    wait=True,\n",
    "    split_type=split_type\n",
    ")\n",
    "\n",
    "\n",
    "# transformer = pytorch_model.transformer(instance_count=4,\n",
    "#                               instance_type=\"ml.p2.xlarge\",\n",
    "#                               max_concurrent_transforms=2,\n",
    "#                               max_payload=1,  # 1MB\n",
    "#                               strategy='MultiRecord',\n",
    "#                               env={'INFERENCE_PREFERRED_MODE': 'classification'},  # only useful with GPU\n",
    "#                               output_path=output_s3_path)\n",
    "\n",
    "# {\"in0\": [6, 17, 606, 19, 53, 67, 52, 12, 5, 10, 15, 10178, 7, 33, 652, 80, 15, 69, 821, 4], \"in1\": [16, 21, 13, 45, 14, 9, 80, 59, 164, 4]}\n",
    "# {\"in0\": [22, 1016, 32, 13, 25, 11, 5, 64, 573, 45, 5, 80, 15, 67, 21, 7, 9, 107, 4], \"in1\": [22, 32, 13, 25, 1016, 573, 3252, 4]}\n",
    "# {\"in0\": [774, 14, 21, 206], \"in1\": [21, 366, 125]}\n",
    "\n",
    "# {\"scores\":[0.195667684078216,0.395351558923721,0.408980727195739]}\n",
    "# {\"scores\":[0.251988261938095,0.258233487606048,0.489778339862823]}\n",
    "# {\"scores\":[0.280087798833847,0.368331134319305,0.351581096649169]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "253fc975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('s3://sagemaker-han/batch-images/inference/sample31.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "41507e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest transform job: pytorch-inference-2022-08-21-08-18-59-697\n"
     ]
    }
   ],
   "source": [
    "print(\"Latest transform job:\", transformer.latest_transform_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719ccee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.stop_transform_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cf813c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CreationTime': datetime.datetime(2022, 8, 21, 12, 13, 37, 860000, tzinfo=tzlocal()),\n",
      " 'DataProcessing': {'InputFilter': '$',\n",
      "                    'JoinSource': 'None',\n",
      "                    'OutputFilter': '$'},\n",
      " 'FailureReason': 'AlgorithmError: See job logs for more information',\n",
      " 'ModelName': 'pytorch-inference-2022-08-21-03-13-10-667',\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '949',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Sun, 21 Aug 2022 05:10:13 GMT',\n",
      "                                      'x-amzn-requestid': 'cd218913-c7fe-4e1f-9b31-2b2fa07372d4'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': 'cd218913-c7fe-4e1f-9b31-2b2fa07372d4',\n",
      "                      'RetryAttempts': 0},\n",
      " 'TransformEndTime': datetime.datetime(2022, 8, 21, 12, 18, 21, 796000, tzinfo=tzlocal()),\n",
      " 'TransformInput': {'CompressionType': 'None',\n",
      "                    'ContentType': 'application/x-image',\n",
      "                    'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
      "                                                    'S3Uri': 's3://sagemaker-han/sagemaker/batch_transform'}},\n",
      "                    'SplitType': 'None'},\n",
      " 'TransformJobArn': 'arn:aws:sagemaker:us-west-2:608095525235:transform-job/pytorch-inference-2022-08-21-03-13-37-242',\n",
      " 'TransformJobName': 'pytorch-inference-2022-08-21-03-13-37-242',\n",
      " 'TransformJobStatus': 'Failed',\n",
      " 'TransformOutput': {'AssembleWith': 'None',\n",
      "                     'KmsKeyId': '',\n",
      "                     'S3OutputPath': 's3://sagemaker-us-west-2-608095525235/pytorch-inference-2022-08-21-03-13-37-242'},\n",
      " 'TransformResources': {'InstanceCount': 1, 'InstanceType': 'ml.m5.large'},\n",
      " 'TransformStartTime': datetime.datetime(2022, 8, 21, 12, 16, 42, 175000, tzinfo=tzlocal())}\n"
     ]
    }
   ],
   "source": [
    "import pprint as pp\n",
    "\n",
    "job_name = 'pytorch-inference-2022-08-21-03-13-37-242'\n",
    "sm_cli = sagemaker_session.sagemaker_client\n",
    "job_info = sm_cli.describe_transform_job(TransformJobName=job_name)\n",
    "pp.pprint(job_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "53b2cbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model = PyTorchModel(model_data = 's3://sagemaker-us-west-2-582981179587/pytorch-inference-2022-08-23-05-57-39-711/model.tar.gz',\n",
    "                             entry_point='resnet_transform.py',\n",
    "                             source_dir = 'src',\n",
    "                             framework_version='1.12.0',\n",
    "                             py_version='py38',\n",
    "                             role = role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9b2ec57f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Model.transformer of <sagemaker.pytorch.model.PyTorchModel object at 0x1450538e0>>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_model.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d8b1c9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n",
      "{'predictions': 8}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "for f in os.listdir(local_path):\n",
    "    path = os.path.join(local_path, f)\n",
    "    with open(path, \"r\") as f:\n",
    "        pred = json.load(f)\n",
    "        print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61056ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('aws_batch_transform-pg7SNxiG')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "445eadaa75e92a02935b67c0f43eae30d5f0df35a0b86dd757723720e6c70438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
