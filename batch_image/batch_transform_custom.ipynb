{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb0c300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初期設定\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import os, boto3, json, sagemaker\n",
    "import numpy as np\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from io import BytesIO\n",
    "\n",
    "def make_dir(path):\n",
    "    if os.path.isdir(path):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(path)\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# AWS設定\n",
    "region = boto3.Session().region_name\n",
    "role = 'han_s3_full_access'\n",
    "bucket='sagemaker-han-batch'\n",
    "# role = 'FullAccessHan'\n",
    "# bucket='sagemaker-han'\n",
    "prefix = 'batch-images'\n",
    "bucket_path = 'https://s3-{}.amazonaws.com/{}'.format(region,bucket)\n",
    "model_prefix = os.path.join(prefix, 'model')\n",
    "input_prefix = os.path.join(prefix, 'inputs')\n",
    "output_prefix = os.path.join(prefix, 'outputs')\n",
    "inference_prefix = os.path.join(prefix, 'f_inference')\n",
    "\n",
    "# Local設定\n",
    "base_dir = os.getcwd()\n",
    "data_dir = os.path.join(base_dir, 'data')\n",
    "model_dir = os.path.join(base_dir, 'model')\n",
    "input_dir = os.path.join(base_dir, 'inputs')\n",
    "output_dir = os.path.join(base_dir, 'outputs')\n",
    "inference_dir = os.path.join(base_dir, 'inference')\n",
    "source_dir = os.path.join(base_dir, 'src')\n",
    "for dir_name in [model_dir, input_dir, output_dir, source_dir, inference_dir]:\n",
    "    make_dir(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f28de312",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Inference用のデータを作る(jsonlines)\n",
    "    - Sampling\n",
    "    - Convert Image into bytes\n",
    "    - Save as jsonlines\n",
    "'''\n",
    "from src.utils import image_to_bytes\n",
    "\n",
    "r_inference_path = os.path.join(base_dir, 'real_inference')\n",
    "n = 1000\n",
    "json_name = r_inference_path+f'/inf_data{n}.jsonl'\n",
    "\n",
    "image_to_bytes(json_name, data_dir, inference_dir, n)\n",
    "\n",
    "f_inference = sagemaker_session.upload_data(path=r_inference_path, bucket=bucket, key_prefix='batch-images/r_inference')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccb4fb4",
   "metadata": {},
   "source": [
    "# Batch Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a6c38ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = sagemaker_session.upload_data(path=model_dir, bucket=bucket, key_prefix=model_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3aa452a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-han-batch/batch-images/model'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# s3://sagemaker-han-batch/batch-images/model/model.tar.gz\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb925323",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "# home\n",
    "# model_path = 's3://sagemaker-us-west-2-608095525235/pytorch-training-2022-08-22-14-02-01-637/model.tar.gz'\n",
    "# fusic\n",
    "model_path = 's3://sagemaker-han-batch/batch-images/model/model.tar.gz'\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data = model_path,\n",
    "                             entry_point='inference.py',\n",
    "                             source_dir = 'custom_model/code',\n",
    "                             framework_version='1.12.0',\n",
    "                             py_version='py38',\n",
    "                             role = role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93732ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "# predictor = pytorch_model.deploy(instance_type='ml.m4.xlarge', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea2048d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_concurrent_transforms = 2\n",
    "max_payload = 1\n",
    "# strategy = 'SingleRecord'\n",
    "# split_type = None\n",
    "strategy = 'MultiRecord'\n",
    "split_type = 'Line'\n",
    "\n",
    "output_s3_path = 's3://{}/{}/bt_test_{}_{}_{}_{}'.format(bucket, output_prefix, max_concurrent_transforms, max_payload, strategy, split_type)\n",
    "\n",
    "transformer = pytorch_model.transformer(instance_count=1,\n",
    "                              instance_type=\"ml.m5.xlarge\",\n",
    "                              max_concurrent_transforms=max_concurrent_transforms,\n",
    "                              max_payload=max_payload,  \n",
    "                              strategy=strategy,\n",
    "                              output_path=output_s3_path,\n",
    "                              accept=\"application/jsonlines\",\n",
    "                              assemble_with=\"Line\"\n",
    "                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a14655a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................\u001b[34mCollecting omegaconf\n",
      "  Downloading omegaconf-2.2.3-py3-none-any.whl (79 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.3/79.3 kB 4.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting antlr4-python3-runtime==4.9.*\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117.0/117.0 kB 13.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: PyYAML>=5.1.0 in /opt/conda/lib/python3.8/site-packages (from omegaconf->-r /opt/ml/model/code/requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): started\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): finished with status 'done'\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=b1e84db33ec9640b24a1f3e98208ca06dfb793951c17e3137b747174e263c63d\n",
      "  Stored in directory: /root/.cache/pip/wheels/b1/a3/c2/6df046c09459b73cc9bb6c4401b0be6c47048baf9a1617c485\u001b[0m\n",
      "\u001b[34mSuccessfully built antlr4-python3-runtime\u001b[0m\n",
      "\u001b[34mInstalling collected packages: antlr4-python3-runtime, omegaconf\u001b[0m\n",
      "\u001b[34mSuccessfully installed antlr4-python3-runtime-4.9.3 omegaconf-2.2.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2 -> 22.2.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:52,502 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:52,615 [INFO ] main org.pytorch.serve.ModelServer - \u001b[0m\n",
      "\u001b[34mTorchserve version: 0.6.0\u001b[0m\n",
      "\u001b[34mTS Home: /opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mCurrent directory: /\u001b[0m\n",
      "\u001b[34mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[34mNumber of GPUs: 0\u001b[0m\n",
      "\u001b[34mNumber of CPUs: 4\u001b[0m\n",
      "\u001b[34mMax heap size: 2728 M\u001b[0m\n",
      "\u001b[34mPython executable: /opt/conda/bin/python3.8\u001b[0m\n",
      "\u001b[34mConfig file: /etc/sagemaker-ts.properties\u001b[0m\n",
      "\u001b[34mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mMetrics address: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mInitial Models: model=/opt/ml/model\u001b[0m\n",
      "\u001b[34mLog dir: /logs\u001b[0m\n",
      "\u001b[34mMetrics dir: /logs\u001b[0m\n",
      "\u001b[34mNetty threads: 0\u001b[0m\n",
      "\u001b[34mNetty client threads: 0\u001b[0m\n",
      "\u001b[34mDefault workers per model: 4\u001b[0m\n",
      "\u001b[34mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[34mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[34mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[34mLimit Maximum Image Pixels: true\u001b[0m\n",
      "\u001b[34mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[34mAllowed Urls: [file://.*|http(s)?://.*]\u001b[0m\n",
      "\u001b[34mCustom python dependency for model allowed: false\u001b[0m\n",
      "\u001b[34mMetrics report format: prometheus\u001b[0m\n",
      "\u001b[34mEnable metrics API: true\u001b[0m\n",
      "\u001b[34mWorkflow Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mModel config: N/A\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:52,625 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:52,654 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:52,658 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:52,658 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:52,661 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:52,677 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:52,899 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:52,899 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:52,910 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel server started.\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:53,367 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:53,538 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038273\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:53,549 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:48.1187629699707|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038273\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:53,562 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:7.746368408203125|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038273\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:53,563 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:13.9|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038273\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:53,564 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:14199.65234375|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038273\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:53,564 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1047.49609375|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038273\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:53,565 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:8.8|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038273\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:53,977 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:53,978 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]58\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:53,982 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:53,983 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:53,989 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:54,004 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:54,013 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1662038274013\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:54,054 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:54,059 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]57\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:54,060 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:54,062 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:54,060 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:54,068 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:54,088 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1662038274088\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:54,088 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:54,130 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:54,221 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:54,223 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]60\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:54,225 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:54,227 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:54,225 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:54,242 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1662038274242\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:54,242 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:54,290 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:54,407 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:54,410 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]59\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:54,410 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:54,412 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:54,411 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:54,418 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1662038274418\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:54,418 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:54,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:55,381 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1299\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:55,382 [INFO ] W-9000-model_1.0 TS_METRICS - W-9000-model_1.0.ms:2714|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038275\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:55,383 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:71|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038275\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:55,390 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1263\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:55,390 [INFO ] W-9003-model_1.0 TS_METRICS - W-9003-model_1.0.ms:2716|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038275\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:55,391 [INFO ] W-9003-model_1.0 TS_METRICS - WorkerThreadTime.ms:40|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038275\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:55,512 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1225\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:55,512 [INFO ] W-9002-model_1.0 TS_METRICS - W-9002-model_1.0.ms:2838|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038275\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:55,512 [INFO ] W-9002-model_1.0 TS_METRICS - WorkerThreadTime.ms:45|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038275\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:55,694 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1213\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:55,694 [INFO ] W-9001-model_1.0 TS_METRICS - W-9001-model_1.0.ms:3023|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038275\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:55,694 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:63|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038275\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:56,797 [INFO ] pool-2-thread-5 ACCESS_LOG - /169.254.255.130:51172 \"GET /ping HTTP/1.1\" 200 14\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:56,798 [INFO ] pool-2-thread-5 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038276\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:56,826 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:51188 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:56,826 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038276\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:57,049 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1662038277049\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:57,052 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1662038277\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:57,112 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1662038277112\u001b[0m\n",
      "\u001b[35m2022-09-01T13:17:56,797 [INFO ] pool-2-thread-5 ACCESS_LOG - /169.254.255.130:51172 \"GET /ping HTTP/1.1\" 200 14\u001b[0m\n",
      "\u001b[35m2022-09-01T13:17:56,798 [INFO ] pool-2-thread-5 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038276\u001b[0m\n",
      "\u001b[35m2022-09-01T13:17:56,826 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:51188 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[35m2022-09-01T13:17:56,826 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038276\u001b[0m\n",
      "\u001b[35m2022-09-01T13:17:57,049 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1662038277049\u001b[0m\n",
      "\u001b[35m2022-09-01T13:17:57,052 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1662038277\u001b[0m\n",
      "\u001b[35m2022-09-01T13:17:57,112 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1662038277112\u001b[0m\n",
      "\u001b[34m2022-09-01T13:17:57,125 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend received inference at: 1662038277\u001b[0m\n",
      "\u001b[35m2022-09-01T13:17:57,125 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend received inference at: 1662038277\u001b[0m\n",
      "\u001b[32m2022-09-01T13:17:56.833:[sagemaker logs]: MaxConcurrentTransforms=2, MaxPayloadInMB=1, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:00,535 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - request received : application/jsonlines\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:00,535 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3483\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:00,536 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:51196 \"POST /invocations HTTP/1.1\" 500 3492\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:00,536 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Num of data in a request: 100\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:00,537 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Input Object Shape: torch.Size([100, 3, 32, 32])\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:00,535 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - request received : application/jsonlines\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:00,535 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3483\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:00,536 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:51196 \"POST /invocations HTTP/1.1\" 500 3492\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:00,536 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Num of data in a request: 100\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:00,537 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Input Object Shape: torch.Size([100, 3, 32, 32])\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:00,537 [INFO ] W-9000-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038276\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:00,538 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038280\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:00,537 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:3481.68|#ModelName:model,Level:Model|#hostname:09fbe1d341eb,requestID:5d4d6761-7b3f-4dd4-871f-34c6b847d0c5,timestamp:1662038280\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:00,542 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:10|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038280\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:00,571 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1662038280571\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:00,574 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend received inference at: 1662038280\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:00,537 [INFO ] W-9000-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038276\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:00,538 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038280\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:00,537 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:3481.68|#ModelName:model,Level:Model|#hostname:09fbe1d341eb,requestID:5d4d6761-7b3f-4dd4-871f-34c6b847d0c5,timestamp:1662038280\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:00,542 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:10|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038280\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:00,571 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1662038280571\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:00,574 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend received inference at: 1662038280\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:03,667 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3094\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:03,668 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.255.130:51224 \"POST /invocations HTTP/1.1\" 500 3098\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:03,669 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038276\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:03,669 [INFO ] W-9002-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038283\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:03,670 [INFO ] W-9002-model_1.0 TS_METRICS - WorkerThreadTime.ms:5|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038283\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:03,672 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - request received : application/jsonlines\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:03,672 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Num of data in a request: 100\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:03,672 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Input Object Shape: torch.Size([100, 3, 32, 32])\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:03,673 [INFO ] W-9002-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:3093.37|#ModelName:model,Level:Model|#hostname:09fbe1d341eb,requestID:ed961213-1136-408f-beb6-2e48aa2e6c3c,timestamp:1662038283\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:03,708 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1662038283708\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:03,711 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1662038283\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:03,667 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3094\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:03,668 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.255.130:51224 \"POST /invocations HTTP/1.1\" 500 3098\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:03,669 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038276\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:03,669 [INFO ] W-9002-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038283\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:03,670 [INFO ] W-9002-model_1.0 TS_METRICS - WorkerThreadTime.ms:5|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038283\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:03,672 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - request received : application/jsonlines\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:03,672 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Num of data in a request: 100\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:03,672 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Input Object Shape: torch.Size([100, 3, 32, 32])\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:03,673 [INFO ] W-9002-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:3093.37|#ModelName:model,Level:Model|#hostname:09fbe1d341eb,requestID:ed961213-1136-408f-beb6-2e48aa2e6c3c,timestamp:1662038283\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:03,708 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1662038283708\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:03,711 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1662038283\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:06,726 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3015\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:06,726 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - request received : application/jsonlines\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:06,728 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Num of data in a request: 100\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:06,728 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Input Object Shape: torch.Size([100, 3, 32, 32])\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:06,729 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:3014.74|#ModelName:model,Level:Model|#hostname:09fbe1d341eb,requestID:f266a731-e495-483e-89d7-9a5ca6ef34d1,timestamp:1662038286\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:06,727 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:60182 \"POST /invocations HTTP/1.1\" 500 3020\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:06,730 [INFO ] W-9001-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038276\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:06,731 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038286\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:06,731 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:8|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038286\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:06,726 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 3015\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:06,726 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - request received : application/jsonlines\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:06,728 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Num of data in a request: 100\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:06,728 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Input Object Shape: torch.Size([100, 3, 32, 32])\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:06,729 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:3014.74|#ModelName:model,Level:Model|#hostname:09fbe1d341eb,requestID:f266a731-e495-483e-89d7-9a5ca6ef34d1,timestamp:1662038286\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:06,727 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:60182 \"POST /invocations HTTP/1.1\" 500 3020\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:06,730 [INFO ] W-9001-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038276\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:06,731 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038286\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:06,731 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:8|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038286\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:06,767 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1662038286767\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:06,771 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1662038286\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:06,767 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1662038286767\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:06,771 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1662038286\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:08,609 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 11495\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:08,609 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 11495\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:08,610 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.255.130:51208 \"POST /invocations HTTP/1.1\" 500 11502\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:08,613 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038276\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:08,614 [INFO ] W-9003-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038288\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:08,615 [INFO ] W-9003-model_1.0 TS_METRICS - WorkerThreadTime.ms:8|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038288\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:08,618 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - request received : application/jsonlines\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:08,619 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Num of data in a request: 343\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:08,619 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Input Object Shape: torch.Size([343, 3, 32, 32])\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:08,619 [INFO ] W-9003-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:11493.63|#ModelName:model,Level:Model|#hostname:09fbe1d341eb,requestID:0480abb2-7003-4689-8360-53bc0b806a56,timestamp:1662038288\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:08,670 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1662038288669\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:08,675 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend received inference at: 1662038288\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:08,610 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.255.130:51208 \"POST /invocations HTTP/1.1\" 500 11502\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:08,613 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038276\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:08,614 [INFO ] W-9003-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038288\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:08,615 [INFO ] W-9003-model_1.0 TS_METRICS - WorkerThreadTime.ms:8|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038288\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:08,618 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - request received : application/jsonlines\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:08,619 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Num of data in a request: 343\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:08,619 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Input Object Shape: torch.Size([343, 3, 32, 32])\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:08,619 [INFO ] W-9003-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:11493.63|#ModelName:model,Level:Model|#hostname:09fbe1d341eb,requestID:0480abb2-7003-4689-8360-53bc0b806a56,timestamp:1662038288\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:08,670 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1662038288669\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:08,675 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend received inference at: 1662038288\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:09,486 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2715\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:09,487 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - request received : application/jsonlines\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:09,487 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Num of data in a request: 100\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:09,487 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:60198 \"POST /invocations HTTP/1.1\" 500 2721\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:09,487 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Input Object Shape: torch.Size([100, 3, 32, 32])\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:09,486 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2715\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:09,487 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - request received : application/jsonlines\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:09,487 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Num of data in a request: 100\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:09,487 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:60198 \"POST /invocations HTTP/1.1\" 500 2721\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:09,487 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Input Object Shape: torch.Size([100, 3, 32, 32])\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:09,487 [INFO ] W-9000-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038276\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:09,487 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:2716.22|#ModelName:model,Level:Model|#hostname:09fbe1d341eb,requestID:78c490e3-8b07-4020-8778-ac945111a2c4,timestamp:1662038289\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:09,487 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038289\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:09,491 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:9|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038289\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:09,539 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1662038289539\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:09,541 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1662038289\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:09,487 [INFO ] W-9000-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038276\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:09,487 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:2716.22|#ModelName:model,Level:Model|#hostname:09fbe1d341eb,requestID:78c490e3-8b07-4020-8778-ac945111a2c4,timestamp:1662038289\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:09,487 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038289\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:09,491 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:9|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038289\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:09,539 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1662038289539\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:09,541 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1662038289\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:09.502:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data100.jsonl: Bad HTTP status received from algorithm: 500\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:09.502:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data100.jsonl: \u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:09.503:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data100.jsonl: Message:\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:09.503:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data100.jsonl: __init__() got an unexpected keyword argument 'mimetype'\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:09.503:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data100.jsonl: Traceback (most recent call last):\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:09.503:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data100.jsonl:   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 128, in transform\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:09.503:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data100.jsonl:     result = self._transform_fn(self._model, input_data, content_type, accept)\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:09.503:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data100.jsonl:   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 235, in _default_transform_fn\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:09.503:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data100.jsonl:     result = self._output_fn(prediction, accept)\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:09.503:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data100.jsonl:   File \"/opt/ml/model/code/inference.py\", line 87, in output_fn\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:09.503:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data100.jsonl:     return json.dumps(my_dict, mimetype=accept)\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:09.503:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data100.jsonl:   File \"/opt/conda/lib/python3.8/json/__init__.py\", line 234, in dumps\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:09.503:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data100.jsonl:     return cls(\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:09.503:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data100.jsonl: TypeError: __init__() got an unexpected keyword argument 'mimetype'\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:14,225 [INFO ] W-9001-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038276\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:14,226 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038294\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:14,227 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:16|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038294\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:14,356 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1662038294356\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:14,370 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend received inference at: 1662038294\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:14,225 [INFO ] W-9001-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038276\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:14,226 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038294\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:14,227 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:16|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038294\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:14,356 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1662038294356\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:14,370 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend received inference at: 1662038294\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:18,961 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - request received : application/jsonlines\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:18,961 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4598\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:18,966 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Num of data in a request: 157\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:18,967 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Input Object Shape: torch.Size([157, 3, 32, 32])\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:18,967 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.255.130:40798 \"POST /invocations HTTP/1.1\" 500 4612\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:18,967 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038276\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:18,967 [INFO ] W-9003-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:4596.57|#ModelName:model,Level:Model|#hostname:09fbe1d341eb,requestID:9f8c2971-f89a-4198-b862-99c8ee22169c,timestamp:1662038298\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:18,968 [INFO ] W-9003-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038298\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:18,968 [INFO ] W-9003-model_1.0 TS_METRICS - WorkerThreadTime.ms:14|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038298\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:19,016 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1662038299016\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:19,019 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1662038299\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:18,961 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - request received : application/jsonlines\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:18,961 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4598\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:18,966 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Num of data in a request: 157\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:18,967 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Input Object Shape: torch.Size([157, 3, 32, 32])\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:18,967 [INFO ] W-9003-model_1.0 ACCESS_LOG - /169.254.255.130:40798 \"POST /invocations HTTP/1.1\" 500 4612\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:18,967 [INFO ] W-9003-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038276\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:18,967 [INFO ] W-9003-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:4596.57|#ModelName:model,Level:Model|#hostname:09fbe1d341eb,requestID:9f8c2971-f89a-4198-b862-99c8ee22169c,timestamp:1662038298\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:18,968 [INFO ] W-9003-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038298\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:18,968 [INFO ] W-9003-model_1.0 TS_METRICS - WorkerThreadTime.ms:14|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038298\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:19,016 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1662038299016\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:19,019 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1662038299\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:19,458 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - request received : application/jsonlines\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:19,459 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Num of data in a request: 343\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:19,460 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Input Object Shape: torch.Size([343, 3, 32, 32])\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:19,460 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 10788\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:19,461 [INFO ] W-9002-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:10785.47|#ModelName:model,Level:Model|#hostname:09fbe1d341eb,requestID:7c08c67a-21f0-4c8f-89e6-d02f9c5cb685,timestamp:1662038299\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:19,461 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.255.130:60208 \"POST /invocations HTTP/1.1\" 500 10794\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:19,461 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038276\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:19,465 [INFO ] W-9002-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038299\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:19,467 [INFO ] W-9002-model_1.0 TS_METRICS - WorkerThreadTime.ms:10|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038299\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:19,536 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1662038299536\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:19,540 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1662038299\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:19,458 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - request received : application/jsonlines\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:19,459 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Num of data in a request: 343\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:19,460 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Input Object Shape: torch.Size([343, 3, 32, 32])\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:19,460 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 10788\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:19,461 [INFO ] W-9002-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:10785.47|#ModelName:model,Level:Model|#hostname:09fbe1d341eb,requestID:7c08c67a-21f0-4c8f-89e6-d02f9c5cb685,timestamp:1662038299\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:19,461 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.255.130:60208 \"POST /invocations HTTP/1.1\" 500 10794\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:19,461 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038276\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:19,465 [INFO ] W-9002-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038299\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:19,467 [INFO ] W-9002-model_1.0 TS_METRICS - WorkerThreadTime.ms:10|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038299\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:19,536 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1662038299536\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:19,540 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1662038299\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:23,551 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - request received : application/jsonlines\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:23,551 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4533\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:23,552 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40814 \"POST /invocations HTTP/1.1\" 500 4538\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:23,552 [INFO ] W-9000-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038276\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:23,552 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Num of data in a request: 157\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:23,552 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038303\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:23,553 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038303\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:23,553 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Input Object Shape: torch.Size([157, 3, 32, 32])\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:23,553 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:4532.39|#ModelName:model,Level:Model|#hostname:09fbe1d341eb,requestID:28e82c7e-9f5b-4223-8b40-ef4034950689,timestamp:1662038303\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:23,577 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1662038303577\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:23,583 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend received inference at: 1662038303\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:23,551 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - request received : application/jsonlines\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:23,551 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 4533\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:23,552 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40814 \"POST /invocations HTTP/1.1\" 500 4538\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:23,552 [INFO ] W-9000-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038276\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:23,552 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Num of data in a request: 157\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:23,552 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038303\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:23,553 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038303\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:23,553 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Input Object Shape: torch.Size([157, 3, 32, 32])\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:23,553 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:4532.39|#ModelName:model,Level:Model|#hostname:09fbe1d341eb,requestID:28e82c7e-9f5b-4223-8b40-ef4034950689,timestamp:1662038303\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:23,577 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1662038303577\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:23,583 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Backend received inference at: 1662038303\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:27.567:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data1000.jsonl: Bad HTTP status received from algorithm: 500\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:27.567:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data1000.jsonl: \u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:27.567:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data1000.jsonl: Message:\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:27.567:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data1000.jsonl: __init__() got an unexpected keyword argument 'mimetype'\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:27.568:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data1000.jsonl: Traceback (most recent call last):\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:27.568:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data1000.jsonl:   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 128, in transform\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:27.568:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data1000.jsonl:     result = self._transform_fn(self._model, input_data, content_type, accept)\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:27.568:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data1000.jsonl:   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 235, in _default_transform_fn\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:27.568:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data1000.jsonl:     result = self._output_fn(prediction, accept)\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:27.568:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data1000.jsonl:   File \"/opt/ml/model/code/inference.py\", line 87, in output_fn\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:27.568:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data1000.jsonl:     return json.dumps(my_dict, mimetype=accept)\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:27.568:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data1000.jsonl:   File \"/opt/conda/lib/python3.8/json/__init__.py\", line 234, in dumps\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:27.568:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data1000.jsonl:     return cls(\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:27.568:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data1000.jsonl: TypeError: __init__() got an unexpected keyword argument 'mimetype'\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:28,969 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Num of data in a request: 343\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:28,969 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Input Object Shape: torch.Size([343, 3, 32, 32])\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:28,969 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40828 \"POST /invocations HTTP/1.1\" 500 9466\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:28,969 [INFO ] W-9001-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038276\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:28,969 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038308\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:28,969 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:9|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038308\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:28,969 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:9429.1|#ModelName:model,Level:Model|#hostname:09fbe1d341eb,requestID:d334204c-2936-46c5-aeee-a2a86aa26a81,timestamp:1662038308\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:28,996 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1662038308996\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:29,004 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend received inference at: 1662038309\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:28,969 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Num of data in a request: 343\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:28,969 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Input Object Shape: torch.Size([343, 3, 32, 32])\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:28,969 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40828 \"POST /invocations HTTP/1.1\" 500 9466\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:28,969 [INFO ] W-9001-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038276\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:28,969 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038308\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:28,969 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:9|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038308\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:28,969 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:9429.1|#ModelName:model,Level:Model|#hostname:09fbe1d341eb,requestID:d334204c-2936-46c5-aeee-a2a86aa26a81,timestamp:1662038308\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:28,996 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1662038308996\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:29,004 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Backend received inference at: 1662038309\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:34,963 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - request received : application/jsonlines\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:34,963 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 5960\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:34,964 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Num of data in a request: 343\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:34,964 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.255.130:60870 \"POST /invocations HTTP/1.1\" 500 5975\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:34,964 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Input Object Shape: torch.Size([343, 3, 32, 32])\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:34,964 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038276\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:34,964 [INFO ] W-9002-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038314\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:34,965 [INFO ] W-9002-model_1.0 TS_METRICS - WorkerThreadTime.ms:9|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038314\u001b[0m\n",
      "\u001b[34m2022-09-01T13:18:34,965 [INFO ] W-9002-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:5958.99|#ModelName:model,Level:Model|#hostname:09fbe1d341eb,requestID:fd384b56-03c0-4b6e-bbe3-2e9ee87e6f56,timestamp:1662038314\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:34,963 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - request received : application/jsonlines\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:34,963 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 5960\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:34,964 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Num of data in a request: 343\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:34,964 [INFO ] W-9002-model_1.0 ACCESS_LOG - /169.254.255.130:60870 \"POST /invocations HTTP/1.1\" 500 5975\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:34,964 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Input Object Shape: torch.Size([343, 3, 32, 32])\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:34,964 [INFO ] W-9002-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038276\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:34,964 [INFO ] W-9002-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038314\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:34,965 [INFO ] W-9002-model_1.0 TS_METRICS - WorkerThreadTime.ms:9|#Level:Host|#hostname:09fbe1d341eb,timestamp:1662038314\u001b[0m\n",
      "\u001b[35m2022-09-01T13:18:34,965 [INFO ] W-9002-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:5958.99|#ModelName:model,Level:Model|#hostname:09fbe1d341eb,requestID:fd384b56-03c0-4b6e-bbe3-2e9ee87e6f56,timestamp:1662038314\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:34.968:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data1000.jsonl: Bad HTTP status received from algorithm: 500\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:34.968:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data1000.jsonl: \u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:34.968:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data1000.jsonl: Message:\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:34.968:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data1000.jsonl: __init__() got an unexpected keyword argument 'mimetype'\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:34.969:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data1000.jsonl: Traceback (most recent call last):\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:34.969:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data1000.jsonl:   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 128, in transform\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:34.969:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data1000.jsonl:     result = self._transform_fn(self._model, input_data, content_type, accept)\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:34.969:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data1000.jsonl:   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 235, in _default_transform_fn\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:34.969:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data1000.jsonl:     result = self._output_fn(prediction, accept)\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:34.969:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data1000.jsonl:   File \"/opt/ml/model/code/inference.py\", line 87, in output_fn\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:34.969:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data1000.jsonl:     return json.dumps(my_dict, mimetype=accept)\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:34.969:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data1000.jsonl:   File \"/opt/conda/lib/python3.8/json/__init__.py\", line 234, in dumps\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:34.969:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data1000.jsonl:     return cls(\u001b[0m\n",
      "\u001b[32m2022-09-01T13:18:34.969:[sagemaker logs]: sagemaker-han-batch/batch-images/r_inference/inf_data1000.jsonl: TypeError: __init__() got an unexpected keyword argument 'mimetype'\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Transform job pytorch-inference-2022-09-01-13-12-45-284: Failed. Reason: AlgorithmError: See job logs for more information",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/han/Desktop/Fusic/2022/aws_batch_transform/batch_image/batch_transform_custom.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/han/Desktop/Fusic/2022/aws_batch_transform/batch_image/batch_transform_custom.ipynb#ch0000008?line=0'>1</a>\u001b[0m inference_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms3://\u001b[39m\u001b[39m{\u001b[39;00mbucket\u001b[39m}\u001b[39;00m\u001b[39m/batch-images/r_inference\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/han/Desktop/Fusic/2022/aws_batch_transform/batch_image/batch_transform_custom.ipynb#ch0000008?line=1'>2</a>\u001b[0m transformer\u001b[39m.\u001b[39;49mtransform(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/han/Desktop/Fusic/2022/aws_batch_transform/batch_image/batch_transform_custom.ipynb#ch0000008?line=2'>3</a>\u001b[0m     data\u001b[39m=\u001b[39;49minference_path,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/han/Desktop/Fusic/2022/aws_batch_transform/batch_image/batch_transform_custom.ipynb#ch0000008?line=3'>4</a>\u001b[0m     data_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mS3Prefix\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/han/Desktop/Fusic/2022/aws_batch_transform/batch_image/batch_transform_custom.ipynb#ch0000008?line=4'>5</a>\u001b[0m     content_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mapplication/jsonlines\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/han/Desktop/Fusic/2022/aws_batch_transform/batch_image/batch_transform_custom.ipynb#ch0000008?line=5'>6</a>\u001b[0m     join_source\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mInput\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/han/Desktop/Fusic/2022/aws_batch_transform/batch_image/batch_transform_custom.ipynb#ch0000008?line=6'>7</a>\u001b[0m     wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/han/Desktop/Fusic/2022/aws_batch_transform/batch_image/batch_transform_custom.ipynb#ch0000008?line=7'>8</a>\u001b[0m     split_type\u001b[39m=\u001b[39;49msplit_type,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/han/Desktop/Fusic/2022/aws_batch_transform/batch_image/batch_transform_custom.ipynb#ch0000008?line=8'>9</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aws_batch_transform-pg7SNxiG/lib/python3.9/site-packages/sagemaker/workflow/pipeline_context.py:248\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m     run_func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    246\u001b[0m     \u001b[39mreturn\u001b[39;00m self_instance\u001b[39m.\u001b[39msagemaker_session\u001b[39m.\u001b[39mcontext\n\u001b[0;32m--> 248\u001b[0m \u001b[39mreturn\u001b[39;00m run_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aws_batch_transform-pg7SNxiG/lib/python3.9/site-packages/sagemaker/transformer.py:243\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, data, data_type, content_type, compression_type, split_type, job_name, input_filter, output_filter, join_source, experiment_config, model_client_config, wait, logs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlatest_transform_job \u001b[39m=\u001b[39m _TransformJob\u001b[39m.\u001b[39mstart_new(\n\u001b[1;32m    229\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    230\u001b[0m     data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m     model_client_config,\n\u001b[1;32m    240\u001b[0m )\n\u001b[1;32m    242\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[0;32m--> 243\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlatest_transform_job\u001b[39m.\u001b[39;49mwait(logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aws_batch_transform-pg7SNxiG/lib/python3.9/site-packages/sagemaker/transformer.py:440\u001b[0m, in \u001b[0;36m_TransformJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, logs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    439\u001b[0m     \u001b[39mif\u001b[39;00m logs:\n\u001b[0;32m--> 440\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49mlogs_for_transform_job(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjob_name, wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    441\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session\u001b[39m.\u001b[39mwait_for_transform_job(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aws_batch_transform-pg7SNxiG/lib/python3.9/site-packages/sagemaker/session.py:4023\u001b[0m, in \u001b[0;36mSession.logs_for_transform_job\u001b[0;34m(self, job_name, wait, poll)\u001b[0m\n\u001b[1;32m   4020\u001b[0m             state \u001b[39m=\u001b[39m LogState\u001b[39m.\u001b[39mJOB_COMPLETE\n\u001b[1;32m   4022\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[0;32m-> 4023\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_job_status(job_name, description, \u001b[39m\"\u001b[39;49m\u001b[39mTransformJobStatus\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   4024\u001b[0m     \u001b[39mif\u001b[39;00m dot:\n\u001b[1;32m   4025\u001b[0m         \u001b[39mprint\u001b[39m()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aws_batch_transform-pg7SNxiG/lib/python3.9/site-packages/sagemaker/session.py:3391\u001b[0m, in \u001b[0;36mSession._check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3385\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mCapacityError\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(reason):\n\u001b[1;32m   3386\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mCapacityError(\n\u001b[1;32m   3387\u001b[0m         message\u001b[39m=\u001b[39mmessage,\n\u001b[1;32m   3388\u001b[0m         allowed_statuses\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mCompleted\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mStopped\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   3389\u001b[0m         actual_status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m   3390\u001b[0m     )\n\u001b[0;32m-> 3391\u001b[0m \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   3392\u001b[0m     message\u001b[39m=\u001b[39mmessage,\n\u001b[1;32m   3393\u001b[0m     allowed_statuses\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mCompleted\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mStopped\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   3394\u001b[0m     actual_status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m   3395\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Transform job pytorch-inference-2022-09-01-13-12-45-284: Failed. Reason: AlgorithmError: See job logs for more information"
     ]
    }
   ],
   "source": [
    "\n",
    "inference_path = f's3://{bucket}/batch-images/r_inference'\n",
    "transformer.transform(\n",
    "    data=inference_path,\n",
    "    data_type=\"S3Prefix\",\n",
    "    content_type=\"application/jsonlines\",\n",
    "    join_source=\"Input\",\n",
    "    wait=True,\n",
    "    split_type=split_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719ccee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer.stop_transform_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cf813c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CreationTime': datetime.datetime(2022, 8, 21, 12, 13, 37, 860000, tzinfo=tzlocal()),\n",
      " 'DataProcessing': {'InputFilter': '$',\n",
      "                    'JoinSource': 'None',\n",
      "                    'OutputFilter': '$'},\n",
      " 'FailureReason': 'AlgorithmError: See job logs for more information',\n",
      " 'ModelName': 'pytorch-inference-2022-08-21-03-13-10-667',\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '949',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Sun, 21 Aug 2022 05:10:13 GMT',\n",
      "                                      'x-amzn-requestid': 'cd218913-c7fe-4e1f-9b31-2b2fa07372d4'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': 'cd218913-c7fe-4e1f-9b31-2b2fa07372d4',\n",
      "                      'RetryAttempts': 0},\n",
      " 'TransformEndTime': datetime.datetime(2022, 8, 21, 12, 18, 21, 796000, tzinfo=tzlocal()),\n",
      " 'TransformInput': {'CompressionType': 'None',\n",
      "                    'ContentType': 'application/x-image',\n",
      "                    'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
      "                                                    'S3Uri': 's3://sagemaker-han/sagemaker/batch_transform'}},\n",
      "                    'SplitType': 'None'},\n",
      " 'TransformJobArn': 'arn:aws:sagemaker:us-west-2:608095525235:transform-job/pytorch-inference-2022-08-21-03-13-37-242',\n",
      " 'TransformJobName': 'pytorch-inference-2022-08-21-03-13-37-242',\n",
      " 'TransformJobStatus': 'Failed',\n",
      " 'TransformOutput': {'AssembleWith': 'None',\n",
      "                     'KmsKeyId': '',\n",
      "                     'S3OutputPath': 's3://sagemaker-us-west-2-608095525235/pytorch-inference-2022-08-21-03-13-37-242'},\n",
      " 'TransformResources': {'InstanceCount': 1, 'InstanceType': 'ml.m5.large'},\n",
      " 'TransformStartTime': datetime.datetime(2022, 8, 21, 12, 16, 42, 175000, tzinfo=tzlocal())}\n"
     ]
    }
   ],
   "source": [
    "# import pprint as pp\n",
    "\n",
    "# job_name = 'pytorch-inference-2022-08-21-03-13-37-242'\n",
    "# sm_cli = sagemaker_session.sagemaker_client\n",
    "# job_info = sm_cli.describe_transform_job(TransformJobName=job_name)\n",
    "# pp.pprint(job_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "054838cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED SHAPE: torch.Size([2])\n",
      "tensor([1, 1])\n",
      "PRED SHAPE: torch.Size([2])\n",
      "tensor([1, 1])\n",
      "PRED SHAPE: torch.Size([2])\n",
      "tensor([1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"predictions\": [1, 1, 1, 1, 1, 1]}'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.resnet_batch_transform import resnet20\n",
    "\n",
    "tmp_model = resnet20()\n",
    "tmp_model.eval()\n",
    "tmp_input = torch.rand(2, 3, 32, 32)\n",
    "\n",
    "preds = []\n",
    "for _ in range(3):\n",
    "    tmp_output = tmp_model(tmp_input)\n",
    "    pred = torch.argmax(tmp_output, dim=1)\n",
    "    print(f'PRED SHAPE: {pred.shape}')\n",
    "    print(pred)\n",
    "    preds += pred\n",
    "\n",
    "preds = np.array(preds).tolist()\n",
    "\n",
    "p_return = {\"predictions\": preds}\n",
    "json.dumps(p_return)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "170915b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<botocore.client.SageMaker at 0x290eb4a60>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "969cb06d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['create_transform_job',\n",
       " 'describe_transform_job',\n",
       " 'list_transform_jobs',\n",
       " 'stop_transform_job']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: \"transform\" in x, dir(sm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "08288b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: x['TransformJobStatus'] == \"InProgress\", sm.list_transform_jobs()['TransformJobSummaries']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0c4369",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('aws_batch_transform-pg7SNxiG')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "445eadaa75e92a02935b67c0f43eae30d5f0df35a0b86dd757723720e6c70438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
