{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17897c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import os\n",
    "\n",
    "def make_dir(path):\n",
    "    if os.path.isdir(data_path):\n",
    "        pass\n",
    "    else:\n",
    "        os.path.make_dir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cde19064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初期設定\n",
    "import sagemaker\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# AWS設定\n",
    "role = 'FullAccessHan'\n",
    "region = boto3.Session().region_name\n",
    "bucket='sagemaker-han'\n",
    "prefix = 'sagemaker'\n",
    "testdata_dir = 'batch_transform_json'\n",
    "bucket_path = 'https://s3-{}.amazonaws.com/{}'.format(region,bucket)\n",
    "\n",
    "# Local設定\n",
    "data_dir = 'cifar10/testing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6cdba52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build image list\n",
    "path = './data/images'\n",
    "image_prefix = os.path.join(bucket_path, testdata_dir)\n",
    "img_files = [os.path.join(image_prefix, f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "\n",
    "# # Write jsonl file\n",
    "json_file = \"./data/test_img_path.json\"\n",
    "with open(json_file, mode=\"w\") as f:\n",
    "    for img_file in img_files:\n",
    "        json_obj = {\"inputs\": img_file}\n",
    "        json.dump(json_obj, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "# Upload to S3\n",
    "json_input = sagemaker_session.upload_data(path=json_file, bucket=bucket, key_prefix=testdata_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fece8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "model_data = 's3://sagemaker-us-west-2-608095525235/pytorch-training-2022-08-20-08-30-28-845/output/model.tar.gz'\n",
    "\n",
    "pytorch_model = PyTorchModel(model_data = model_data,\n",
    "                             entry_point='resnet_deploy.py',\n",
    "                             source_dir = 'src',\n",
    "                             framework_version='1.12.0',\n",
    "                             py_version='py38',\n",
    "                             role = role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11849205",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_s3_path = 'https://sagemaker-han.s3.us-west-2.amazonaws.com/sagemaker/batch_transform_output_json'\n",
    "max_concurrent_transforms = 100\n",
    "max_payload = 1\n",
    "\n",
    "transformer = pytorch_model.transformer(instance_count=1, \n",
    "                                        instance_type=\"ml.m5.large\",\n",
    "                                        output_path=output_s3_path,\n",
    "                                        max_concurrent_transforms = max_concurrent_transforms,\n",
    "                                        max_payload = max_payload\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70e6292a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............................\u001b[34mWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:11,708 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:11,790 [INFO ] main org.pytorch.serve.ModelServer - \u001b[0m\n",
      "\u001b[34mTorchserve version: 0.6.0\u001b[0m\n",
      "\u001b[34mTS Home: /opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mCurrent directory: /\u001b[0m\n",
      "\u001b[34mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[34mNumber of GPUs: 0\u001b[0m\n",
      "\u001b[34mNumber of CPUs: 2\u001b[0m\n",
      "\u001b[34mMax heap size: 962 M\u001b[0m\n",
      "\u001b[34mPython executable: /opt/conda/bin/python3.8\u001b[0m\n",
      "\u001b[34mConfig file: /etc/sagemaker-ts.properties\u001b[0m\n",
      "\u001b[34mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mMetrics address: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mInitial Models: model=/opt/ml/model\u001b[0m\n",
      "\u001b[34mLog dir: /logs\u001b[0m\n",
      "\u001b[34mMetrics dir: /logs\u001b[0m\n",
      "\u001b[34mNetty threads: 0\u001b[0m\n",
      "\u001b[34mNetty client threads: 0\u001b[0m\n",
      "\u001b[34mDefault workers per model: 2\u001b[0m\n",
      "\u001b[34mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[34mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[34mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[35mWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:11,708 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:11,790 [INFO ] main org.pytorch.serve.ModelServer - \u001b[0m\n",
      "\u001b[35mTorchserve version: 0.6.0\u001b[0m\n",
      "\u001b[35mTS Home: /opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[35mCurrent directory: /\u001b[0m\n",
      "\u001b[35mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[35mNumber of GPUs: 0\u001b[0m\n",
      "\u001b[35mNumber of CPUs: 2\u001b[0m\n",
      "\u001b[35mMax heap size: 962 M\u001b[0m\n",
      "\u001b[35mPython executable: /opt/conda/bin/python3.8\u001b[0m\n",
      "\u001b[35mConfig file: /etc/sagemaker-ts.properties\u001b[0m\n",
      "\u001b[35mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[35mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[35mMetrics address: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[35mModel Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[35mInitial Models: model=/opt/ml/model\u001b[0m\n",
      "\u001b[35mLog dir: /logs\u001b[0m\n",
      "\u001b[35mMetrics dir: /logs\u001b[0m\n",
      "\u001b[35mNetty threads: 0\u001b[0m\n",
      "\u001b[35mNetty client threads: 0\u001b[0m\n",
      "\u001b[35mDefault workers per model: 2\u001b[0m\n",
      "\u001b[35mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[35mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[35mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[34mLimit Maximum Image Pixels: true\u001b[0m\n",
      "\u001b[34mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[34mAllowed Urls: [file://.*|http(s)?://.*]\u001b[0m\n",
      "\u001b[34mCustom python dependency for model allowed: false\u001b[0m\n",
      "\u001b[34mMetrics report format: prometheus\u001b[0m\n",
      "\u001b[34mEnable metrics API: true\u001b[0m\n",
      "\u001b[34mWorkflow Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mModel config: N/A\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:11,799 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:11,836 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:11,841 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:11,841 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:11,844 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:11,862 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:12,105 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:12,106 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:12,122 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel server started.\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:12,935 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:13,239 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069713\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:13,240 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:48.206336975097656|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069713\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:13,241 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:7.658794403076172|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069713\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:13,242 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:13.7|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069713\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:13,246 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:6683.51171875|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069713\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:13,246 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:698.48828125|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069713\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:13,247 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:13.0|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069713\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:13,917 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:13,920 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]40\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:13,926 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:13,927 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:13,932 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35mLimit Maximum Image Pixels: true\u001b[0m\n",
      "\u001b[35mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[35mAllowed Urls: [file://.*|http(s)?://.*]\u001b[0m\n",
      "\u001b[35mCustom python dependency for model allowed: false\u001b[0m\n",
      "\u001b[35mMetrics report format: prometheus\u001b[0m\n",
      "\u001b[35mEnable metrics API: true\u001b[0m\n",
      "\u001b[35mWorkflow Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[35mModel config: N/A\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:11,799 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:11,836 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: /opt/ml/model\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:11,841 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:11,841 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:11,844 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:11,862 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:12,105 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:12,106 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:12,122 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[35mModel server started.\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:12,935 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:13,239 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069713\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:13,240 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:48.206336975097656|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069713\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:13,241 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:7.658794403076172|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069713\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:13,242 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:13.7|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069713\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:13,246 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:6683.51171875|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069713\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:13,246 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:698.48828125|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069713\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:13,247 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:13.0|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069713\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:13,917 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:13,920 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]40\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:13,926 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:13,927 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:13,932 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:13,937 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:13,943 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]39\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:13,943 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:13,944 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:13,944 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:13,961 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:13,971 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661069713971\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:13,976 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:13,977 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661069713977\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:14,154 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:14,156 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:14,834 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 679\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:14,836 [INFO ] W-9000-model_1.0 TS_METRICS - W-9000-model_1.0.ms:2982|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069714\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:14,842 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 686\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:14,842 [INFO ] W-9001-model_1.0 TS_METRICS - W-9001-model_1.0.ms:2984|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069714\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:14,843 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:187|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069714\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:14,843 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:186|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069714\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:13,937 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:13,943 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]39\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:13,943 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:13,944 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:13,944 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:13,961 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:13,971 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661069713971\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:13,976 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:13,977 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661069713977\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:14,154 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:14,156 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:14,834 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 679\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:14,836 [INFO ] W-9000-model_1.0 TS_METRICS - W-9000-model_1.0.ms:2982|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069714\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:14,842 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 686\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:14,842 [INFO ] W-9001-model_1.0 TS_METRICS - W-9001-model_1.0.ms:2984|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069714\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:14,843 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:187|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069714\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:14,843 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:186|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069714\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:20,709 [INFO ] pool-2-thread-3 ACCESS_LOG - /169.254.255.130:47372 \"GET /ping HTTP/1.1\" 200 59\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:20,715 [INFO ] pool-2-thread-3 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069720\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:20,754 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:47382 \"GET /execution-parameters HTTP/1.1\" 404 2\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:20,758 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069720\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:20,709 [INFO ] pool-2-thread-3 ACCESS_LOG - /169.254.255.130:47372 \"GET /ping HTTP/1.1\" 200 59\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:20,715 [INFO ] pool-2-thread-3 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069720\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:20,754 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:47382 \"GET /execution-parameters HTTP/1.1\" 404 2\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:20,758 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069720\u001b[0m\n",
      "\u001b[32m2022-08-21T08:15:20.776:[sagemaker logs]: MaxConcurrentTransforms=100, MaxPayloadInMB=1, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,098 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661069721098\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,100 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1661069721\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,101 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,102 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:0.51|#ModelName:model,Level:Model|#hostname:52a2d2c461e5,requestID:d0cf78e2-2b97-450d-99df-e5c8c1fba233,timestamp:1661069721\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,103 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:47388 \"POST /invocations HTTP/1.1\" 500 14\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,103 [INFO ] W-9000-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069720\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,104 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069721\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,104 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:5|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069721\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,127 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661069721127\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,130 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1661069721\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,131 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:0.47|#ModelName:model,Level:Model|#hostname:52a2d2c461e5,requestID:4b375291-efb3-4ab9-adf6-0b341510dd98,timestamp:1661069721\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,132 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 0\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,133 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:47398 \"POST /invocations HTTP/1.1\" 500 6\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,134 [INFO ] W-9001-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069720\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,134 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069721\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,135 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:8|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069721\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,154 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661069721154\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,156 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1661069721\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,156 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:0.19|#ModelName:model,Level:Model|#hostname:52a2d2c461e5,requestID:8b3d3791-7e94-4f4c-9284-a9c1c05ccfb7,timestamp:1661069721\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,158 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,158 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:47408 \"POST /invocations HTTP/1.1\" 500 4\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,098 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661069721098\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,100 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1661069721\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,101 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,102 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:0.51|#ModelName:model,Level:Model|#hostname:52a2d2c461e5,requestID:d0cf78e2-2b97-450d-99df-e5c8c1fba233,timestamp:1661069721\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,103 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:47388 \"POST /invocations HTTP/1.1\" 500 14\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,103 [INFO ] W-9000-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069720\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,104 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069721\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,104 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:5|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069721\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,127 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661069721127\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,130 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1661069721\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,131 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:0.47|#ModelName:model,Level:Model|#hostname:52a2d2c461e5,requestID:4b375291-efb3-4ab9-adf6-0b341510dd98,timestamp:1661069721\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,132 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 0\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,133 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:47398 \"POST /invocations HTTP/1.1\" 500 6\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,134 [INFO ] W-9001-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069720\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,134 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069721\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,135 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:8|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069721\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,154 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661069721154\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,156 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1661069721\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,156 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:0.19|#ModelName:model,Level:Model|#hostname:52a2d2c461e5,requestID:8b3d3791-7e94-4f4c-9284-a9c1c05ccfb7,timestamp:1661069721\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,158 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,158 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:47408 \"POST /invocations HTTP/1.1\" 500 4\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,158 [INFO ] W-9000-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069720\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,158 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069721\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,159 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069721\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,178 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661069721178\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,180 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1661069721\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,158 [INFO ] W-9000-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069720\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,158 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069721\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,159 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069721\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,178 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661069721178\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,180 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1661069721\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,181 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:0.18|#ModelName:model,Level:Model|#hostname:52a2d2c461e5,requestID:174d3bba-f120-43f6-826b-23846abdf50c,timestamp:1661069721\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,182 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 0\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,182 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:47412 \"POST /invocations HTTP/1.1\" 500 4\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,183 [INFO ] W-9001-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069720\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,183 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069721\u001b[0m\n",
      "\u001b[34m2022-08-21T08:15:21,184 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:6|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069721\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,181 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:0.18|#ModelName:model,Level:Model|#hostname:52a2d2c461e5,requestID:174d3bba-f120-43f6-826b-23846abdf50c,timestamp:1661069721\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,182 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 0\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,182 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:47412 \"POST /invocations HTTP/1.1\" 500 4\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,183 [INFO ] W-9001-model_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069720\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,183 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069721\u001b[0m\n",
      "\u001b[35m2022-08-21T08:15:21,184 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:6|#Level:Host|#hostname:52a2d2c461e5,timestamp:1661069721\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2022-08-21T08:15:21.190:[sagemaker logs]: sagemaker-han/batch_transform_json/test_img_path.json: Bad HTTP status received from algorithm: 500\u001b[0m\n",
      "\u001b[32m2022-08-21T08:15:21.191:[sagemaker logs]: sagemaker-han/batch_transform_json/test_img_path.json: \u001b[0m\n",
      "\u001b[32m2022-08-21T08:15:21.191:[sagemaker logs]: sagemaker-han/batch_transform_json/test_img_path.json: Message:\u001b[0m\n",
      "\u001b[32m2022-08-21T08:15:21.191:[sagemaker logs]: sagemaker-han/batch_transform_json/test_img_path.json: 'str' object has no attribute 'inputs'\u001b[0m\n",
      "\u001b[32m2022-08-21T08:15:21.191:[sagemaker logs]: sagemaker-han/batch_transform_json/test_img_path.json: Traceback (most recent call last):\u001b[0m\n",
      "\u001b[32m2022-08-21T08:15:21.191:[sagemaker logs]: sagemaker-han/batch_transform_json/test_img_path.json:   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 128, in transform\u001b[0m\n",
      "\u001b[32m2022-08-21T08:15:21.191:[sagemaker logs]: sagemaker-han/batch_transform_json/test_img_path.json:     result = self._transform_fn(self._model, input_data, content_type, accept)\u001b[0m\n",
      "\u001b[32m2022-08-21T08:15:21.191:[sagemaker logs]: sagemaker-han/batch_transform_json/test_img_path.json:   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 233, in _default_transform_fn\u001b[0m\n",
      "\u001b[32m2022-08-21T08:15:21.191:[sagemaker logs]: sagemaker-han/batch_transform_json/test_img_path.json:     data = self._input_fn(input_data, content_type)\u001b[0m\n",
      "\u001b[32m2022-08-21T08:15:21.191:[sagemaker logs]: sagemaker-han/batch_transform_json/test_img_path.json:   File \"/opt/ml/model/code/resnet_deploy.py\", line 85, in input_fn\u001b[0m\n",
      "\u001b[32m2022-08-21T08:15:21.191:[sagemaker logs]: sagemaker-han/batch_transform_json/test_img_path.json:     image_tensor = load_from_bytearray(request_body.inputs)\u001b[0m\n",
      "\u001b[32m2022-08-21T08:15:21.191:[sagemaker logs]: sagemaker-han/batch_transform_json/test_img_path.json: AttributeError: 'str' object has no attribute 'inputs'\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Transform job pytorch-inference-2022-08-21-08-10-14-217: Failed. Reason: AlgorithmError: See job logs for more information",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m json_input \u001b[38;5;241m=\u001b[39m sagemaker_session\u001b[38;5;241m.\u001b[39mupload_data(path\u001b[38;5;241m=\u001b[39mjson_file, bucket\u001b[38;5;241m=\u001b[39mbucket, key_prefix\u001b[38;5;241m=\u001b[39mtestdata_dir)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLine\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/workflow/pipeline_context.py:248\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m     run_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m self_instance\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mcontext\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/transformer.py:243\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, data, data_type, content_type, compression_type, split_type, job_name, input_filter, output_filter, join_source, experiment_config, model_client_config, wait, logs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_transform_job \u001b[38;5;241m=\u001b[39m _TransformJob\u001b[38;5;241m.\u001b[39mstart_new(\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    230\u001b[0m     data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m     model_client_config,\n\u001b[1;32m    240\u001b[0m )\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m--> 243\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_transform_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/transformer.py:440\u001b[0m, in \u001b[0;36m_TransformJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logs:\n\u001b[0;32m--> 440\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_transform_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_transform_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/session.py:4023\u001b[0m, in \u001b[0;36mSession.logs_for_transform_job\u001b[0;34m(self, job_name, wait, poll)\u001b[0m\n\u001b[1;32m   4020\u001b[0m             state \u001b[38;5;241m=\u001b[39m LogState\u001b[38;5;241m.\u001b[39mJOB_COMPLETE\n\u001b[1;32m   4022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 4023\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTransformJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4024\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dot:\n\u001b[1;32m   4025\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/batch_transform-aeIkgUX-/lib/python3.9/site-packages/sagemaker/session.py:3391\u001b[0m, in \u001b[0;36mSession._check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   3386\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   3387\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   3388\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   3389\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   3390\u001b[0m     )\n\u001b[0;32m-> 3391\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   3392\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   3393\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   3394\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   3395\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Transform job pytorch-inference-2022-08-21-08-10-14-217: Failed. Reason: AlgorithmError: See job logs for more information"
     ]
    }
   ],
   "source": [
    "json_input = sagemaker_session.upload_data(path=json_file, bucket=bucket, key_prefix=testdata_dir)\n",
    "transformer.transform(\n",
    "    data=json_input,\n",
    "    content_type='application/json',    \n",
    "    split_type='Line',\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d294ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.stop_transform_job()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
