{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1734e735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy\n",
      "  Downloading scipy-1.9.0-cp39-cp39-macosx_12_0_arm64.whl (29.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.9/29.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<1.25.0,>=1.18.5 in /Users/han/.local/share/virtualenvs/aws_batch_transform-pg7SNxiG/lib/python3.9/site-packages (from scipy) (1.23.2)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy\n",
    "import pandas as pd\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9f0f14eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1b/_jbv1lxn457d4z71n10rh0sc0000gn/T/ipykernel_92894/2129752244.py:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = pd.read_csv(\n",
      "b'Skipping line 92523: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 343254: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 524626: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 623024: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 977412: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1496867: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1711638: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1787213: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2395306: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2527690: expected 15 fields, saw 22\\n'\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [104]\u001b[0m, in \u001b[0;36m<cell line: 75>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m np\u001b[38;5;241m.\u001b[39msavetxt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/reviews/train.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_sparse\u001b[38;5;241m.\u001b[39mtodense(), delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m, fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m np\u001b[38;5;241m.\u001b[39msavetxt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/reviews/test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, test_sparse\u001b[38;5;241m.\u001b[39mtodense(), delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m, fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m train_s3 \u001b[38;5;241m=\u001b[39m \u001b[43msess\u001b[49m\u001b[38;5;241m.\u001b[39mupload_data(\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/reviews/train.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, bucket\u001b[38;5;241m=\u001b[39mbucket, key_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/pca/train\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(prefix)\n\u001b[1;32m     77\u001b[0m )\n\u001b[1;32m     79\u001b[0m test_s3 \u001b[38;5;241m=\u001b[39m sess\u001b[38;5;241m.\u001b[39mupload_data(\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/reviews/test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, bucket\u001b[38;5;241m=\u001b[39mbucket, key_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/pca/test\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(prefix)\n\u001b[1;32m     81\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sess' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    \"/tmp/reviews/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz\",\n",
    "    delimiter=\"\\t\",\n",
    "    error_bad_lines=False,\n",
    ")\n",
    "df = df[[\"customer_id\", \"product_id\", \"star_rating\", \"product_title\"]]\n",
    "customers = df[\"customer_id\"].value_counts()\n",
    "products = df[\"product_id\"].value_counts()\n",
    "\n",
    "quantiles = [\n",
    "    0,\n",
    "    0.1,\n",
    "    0.25,\n",
    "    0.5,\n",
    "    0.75,\n",
    "    0.8,\n",
    "    0.85,\n",
    "    0.9,\n",
    "    0.95,\n",
    "    0.96,\n",
    "    0.97,\n",
    "    0.98,\n",
    "    0.99,\n",
    "    0.995,\n",
    "    0.999,\n",
    "    0.9999,\n",
    "    1,\n",
    "]\n",
    "customers = customers[customers >= 35]\n",
    "products = products[products >= 20]\n",
    "\n",
    "reduced_df = df.merge(pd.DataFrame({\"customer_id\": customers.index})).merge(\n",
    "    pd.DataFrame({\"product_id\": products.index})\n",
    ")\n",
    "\n",
    "customers = reduced_df[\"customer_id\"].value_counts()\n",
    "products = reduced_df[\"product_id\"].value_counts()\n",
    "\n",
    "test_products = products.sample(frac=0.005)\n",
    "train_products = products[~(products.index.isin(test_products.index))]\n",
    "\n",
    "customer_index = pd.DataFrame(\n",
    "    {\"customer_id\": customers.index, \"user\": np.arange(customers.shape[0])}\n",
    ")\n",
    "train_product_index = pd.DataFrame(\n",
    "    {\"product_id\": train_products.index, \"item\": np.arange(train_products.shape[0])}\n",
    ")\n",
    "test_product_index = pd.DataFrame(\n",
    "    {\"product_id\": test_products.index, \"item\": np.arange(test_products.shape[0])}\n",
    ")\n",
    "\n",
    "train_df = reduced_df.merge(customer_index).merge(train_product_index)\n",
    "test_df = reduced_df.merge(customer_index).merge(test_product_index)\n",
    "\n",
    "train_sparse = scipy.sparse.csr_matrix(\n",
    "    (\n",
    "        np.where(train_df[\"star_rating\"].values >= 4, 1, 0),\n",
    "        (train_df[\"item\"].values, train_df[\"user\"].values),\n",
    "    ),\n",
    "    shape=(train_df[\"item\"].nunique(), customers.count()),\n",
    ")\n",
    "\n",
    "test_sparse = scipy.sparse.csr_matrix(\n",
    "    (\n",
    "        np.where(test_df[\"star_rating\"].values >= 4, 1, 0),\n",
    "        (test_df[\"item\"].values, test_df[\"user\"].values),\n",
    "    ),\n",
    "    shape=(test_df[\"item\"].nunique(), customers.count()),\n",
    ")\n",
    "\n",
    "np.savetxt(\"/tmp/reviews/train.csv\", train_sparse.todense(), delimiter=\",\", fmt=\"%i\")\n",
    "\n",
    "np.savetxt(\"/tmp/reviews/test.csv\", test_sparse.todense(), delimiter=\",\", fmt=\"%i\")\n",
    "\n",
    "\n",
    "train_s3 = sagemaker_session.upload_data(\n",
    "    \"/tmp/reviews/train.csv\", bucket=bucket, key_prefix=\"{}/pca/train\".format(prefix)\n",
    ")\n",
    "\n",
    "test_s3 = sagemaker_session.upload_data(\n",
    "    \"/tmp/reviews/test.csv\", bucket=bucket, key_prefix=\"{}/pca/test\".format(prefix)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ffdfa1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-han-batch/tutorial/pca/train/train.csv'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a190f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b009524e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "17897c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初期設定\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import os, boto3, json, sagemaker, numpy as np\n",
    "from io import BytesIO\n",
    "\n",
    "def make_dir(path):\n",
    "    if os.path.isdir(path):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(path)\n",
    "        \n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# AWS設定\n",
    "role = 'han_s3_full_access'\n",
    "region = boto3.Session().region_name\n",
    "bucket='sagemaker-han-batch'\n",
    "prefix = 'tutorial'\n",
    "bucket_path = 'https://s3-{}.amazonaws.com/{}'.format(region,bucket)\n",
    "\n",
    "# Local設定\n",
    "data_dir = './tutorial'\n",
    "model_dir = os.path.join(data_dir, 'model')\n",
    "input_dir = os.path.join(data_dir, 'inputs')\n",
    "output_dir = os.path.join(data_dir, 'outputs')\n",
    "code_dir = os.path.join(data_dir, 'code')\n",
    "for dir_name in [data_dir, model_dir, input_dir, output_dir, code_dir]:\n",
    "    make_dir(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24866b85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create temp model and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2d0d2cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./tutorial/model/my_model.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {model_dir}/my_model.txt\n",
    "Hello my great machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "35833f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipfile                           batch_transform_image_input.ipynb\r\n",
      "Pipfile.lock                      requirements.txt\r\n",
      "Untitled.ipynb                    \u001b[34msrc\u001b[m\u001b[m\r\n",
      "batch-transform-tutorial.ipynb    train.csv.out\r\n",
      "batch_transform_1.ipynb           \u001b[34mtutorial\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f57d8ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/han/Desktop/Fusic/2022/aws_batch_transform/tutorial/model\n",
      "a ./model.tar.gztar: ./model.tar.gz: Can't add archive to itself\n",
      "\n",
      "a ./my_model.txt\n",
      "/Users/han/Desktop/Fusic/2022/aws_batch_transform\n",
      "s3://sagemaker-han-batch/tutorial_model/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Upload into S3\n",
    "%cd {model_dir}\n",
    "!tar zcvf model.tar.gz ./*\n",
    "%cd ../..\n",
    "\n",
    "model_s3_uri = sagemaker.session.Session().upload_data(\n",
    "    f'{model_dir}/model.tar.gz',\n",
    "    bucket = bucket,\n",
    "    key_prefix = 'tutorial_model'\n",
    ")\n",
    "print(model_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "93e5dd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./tutorial/code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {code_dir}/inference.py\n",
    "import os\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    with open(os.path.join(model_dir,'my_model.txt')) as f:\n",
    "        model = f.read()[:-1] # 改行を除外\n",
    "    return model\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    response = f'{model} for the {input_data}st time'\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "12179e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "# モデルとコンテナの指定\n",
    "\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "model_name = 'TxtModel'\n",
    "model_data = model_s3_uri\n",
    "endpoint_name = model_name + 'Endpoint'\n",
    "\n",
    "pytorch_model = PyTorchModel(\n",
    "    name = model_name,\n",
    "    model_data = model_data,\n",
    "    role = role,\n",
    "    framework_version = '1.12.0',\n",
    "    py_version='py38',\n",
    "    entry_point='inference.py',\n",
    "    source_dir = code_dir\n",
    ")\n",
    "\n",
    "pytorch_predictor = pytorch_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    enpoint_name=endpoint_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "84e1084a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my great machine learning model for the 1st time <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "response = pytorch_predictor.predict(1)\n",
    "print(response,type(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc23a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae640a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-inference:1.11.0-cpu-py38\n"
     ]
    }
   ],
   "source": [
    "container_image_uri = sagemaker.image_uris.retrieve(\n",
    "    \"pytorch\", \n",
    "    sagemaker.session.Session().boto_region_name, # ECR のリージョンを指定\n",
    "    version='1.11.0', # SKLearn のバージョンを指定\n",
    "    instance_type = 'ml.m4.large', # インスタンスタイプを指定\n",
    "    image_scope = 'inference' # 推論コンテナを指定\n",
    ")\n",
    "print(container_image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "773c0e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./tutorial/code/inference1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {code_dir}/inference1.py\n",
    "import logging\n",
    "import os, json\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    with open(os.path.join(model_dir,'my_model.txt')) as f:\n",
    "        hello = f.read()[:-1] # 改行を除外\n",
    "    return hello\n",
    "\n",
    "def input_fn(input_data, content_type):\n",
    "    logger.info(len(input_data)\n",
    "#     if content_type == 'text/csv':\n",
    "#         transformed_data = input_data.split(',')\n",
    "#     else:\n",
    "#         raise ValueError(f\"Illegal content type {content_type}. The only allowed content_type is text/csv\")\n",
    "    return list(input_data)\n",
    "\n",
    "def predict_fn(transformed_data, model):\n",
    "    prediction_list = []\n",
    "    logger.info(len(transformed_data))\n",
    "#     for data in transformed_data:\n",
    "#         if data[-1] == '1':\n",
    "#             ordinal = f'{data}st'\n",
    "#         elif data[-1] == '2':\n",
    "#             ordinal = f'{data}nd'\n",
    "#         elif data[-1] == '3':\n",
    "#             ordinal = f'{data}rd'\n",
    "#         else:\n",
    "#             ordinal = f'{data}th'\n",
    "#         prediction = f'{model} for the {ordinal} time'\n",
    "#         prediction_list.append(prediction)   \n",
    "    return len(transformed_data)\n",
    "def output_fn(prediction_list, accept):\n",
    "#     if accept == 'text/csv':    \n",
    "#         response = ''\n",
    "#         for prediction in prediction_list:\n",
    "#             response += prediction + '\\n'\n",
    "#     else:\n",
    "#         raise ValueError(f\"Illegal accept type {accept}. The only allowed accept type is text/csv\")\n",
    "    return prediction_list, accept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ec8ba53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/han/Desktop/Fusic/2022/aws_batch_transform/tutorial/model\n",
      "a ./inference.py\n",
      "a ./inference1.py\n",
      "a ./my_model.txt\n",
      "/Users/han/Desktop/Fusic/2022/aws_batch_transform\n"
     ]
    }
   ],
   "source": [
    "%cd {model_dir}\n",
    "!rm model.tar.gz\n",
    "!cp ../../{code_dir}/inference1.py ./\n",
    "!tar zcvf model.tar.gz ./*\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8192f08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-han-batch/tutorial_model/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "model_s3_uri = sagemaker.session.Session().upload_data(\n",
    "    f'{model_dir}/model.tar.gz',\n",
    "    bucket = bucket,\n",
    "    key_prefix = 'tutorial_model'\n",
    ")\n",
    "print(model_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "70b3d4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-han-batch/tutorial/input\n"
     ]
    }
   ],
   "source": [
    "# Batch Transform\n",
    "\n",
    "input_data1 = 'input_data1.csv'\n",
    "input_data2 = 'input_data2.csv'\n",
    "long_input = ','.join([str(i) for i in range(99999)])\n",
    "with open(os.path.join(input_dir,input_data1),'wt') as f:\n",
    "    f.write(long_input)\n",
    "with open(os.path.join(input_dir,input_data2),'wt') as f:\n",
    "    f.write('11,22,33,44,55')\n",
    "\n",
    "input_prefix = prefix + '/input'\n",
    "input_data_s3_uri = sagemaker.Session().upload_data(\n",
    "    input_dir,\n",
    "    bucket = bucket,\n",
    "    key_prefix = input_prefix)\n",
    "print(input_data_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b8ccc91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_concurrent_transforms = 1\n",
    "max_payload = 1\n",
    "strategy = 'MultiRecord'# 'SingleRecord'\n",
    "\n",
    "data_type = \"S3Prefix\"\n",
    "split_type = \"Line\" # \"Line\"\n",
    "\n",
    "\n",
    "\n",
    "model_data = 's3://sagemaker-us-west-2-582981179587/TxtModel/model.tar.gz'\n",
    "pytorch_model = PyTorchModel(model_data = model_data,\n",
    "                             entry_point='inference1.py',\n",
    "                             source_dir = code_dir,\n",
    "                             framework_version='1.12.0',\n",
    "                             py_version='py38',\n",
    "                             role = role)\n",
    "\n",
    "output_s3_path = 's3://sagemaker-han-batch/tutorial/output'\n",
    "transformer = pytorch_model.transformer(instance_count=1, \n",
    "                                        instance_type=\"ml.m5.large\",\n",
    "                                        output_path=output_s3_path,\n",
    "                                        accept = 'text/csv',\n",
    "                                        max_concurrent_transforms = max_concurrent_transforms,\n",
    "                                        max_payload = max_payload,\n",
    "                                        strategy = strategy\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d24e4964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........................\u001b[34mWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:13,318 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:13,455 [INFO ] main org.pytorch.serve.ModelServer - \u001b[0m\n",
      "\u001b[34mTorchserve version: 0.6.0\u001b[0m\n",
      "\u001b[34mTS Home: /opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mCurrent directory: /\u001b[0m\n",
      "\u001b[34mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[34mNumber of GPUs: 0\u001b[0m\n",
      "\u001b[34mNumber of CPUs: 2\u001b[0m\n",
      "\u001b[34mMax heap size: 980 M\u001b[0m\n",
      "\u001b[34mPython executable: /opt/conda/bin/python3.8\u001b[0m\n",
      "\u001b[34mConfig file: /etc/sagemaker-ts.properties\u001b[0m\n",
      "\u001b[34mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mMetrics address: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mInitial Models: model=/opt/ml/model\u001b[0m\n",
      "\u001b[34mLog dir: /logs\u001b[0m\n",
      "\u001b[34mMetrics dir: /logs\u001b[0m\n",
      "\u001b[34mNetty threads: 0\u001b[0m\n",
      "\u001b[34mNetty client threads: 0\u001b[0m\n",
      "\u001b[34mDefault workers per model: 2\u001b[0m\n",
      "\u001b[34mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[34mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[34mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[34mLimit Maximum Image Pixels: true\u001b[0m\n",
      "\u001b[34mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[34mAllowed Urls: [file://.*|http(s)?://.*]\u001b[0m\n",
      "\u001b[34mCustom python dependency for model allowed: false\u001b[0m\n",
      "\u001b[34mMetrics report format: prometheus\u001b[0m\n",
      "\u001b[34mEnable metrics API: true\u001b[0m\n",
      "\u001b[34mWorkflow Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mModel config: N/A\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:13,468 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:13,505 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: /opt/ml/model\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:13,513 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:13,513 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:13,518 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:13,547 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:13,792 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:13,794 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:13,803 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel server started.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:14,635 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:14,898 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:1e2d4f9a7709,timestamp:1661151434\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:14,907 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:48.20740509033203|#Level:Host|#hostname:1e2d4f9a7709,timestamp:1661151434\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:14,908 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:7.657726287841797|#Level:Host|#hostname:1e2d4f9a7709,timestamp:1661151434\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:14,909 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:13.7|#Level:Host|#hostname:1e2d4f9a7709,timestamp:1661151434\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:14,910 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:6762.00390625|#Level:Host|#hostname:1e2d4f9a7709,timestamp:1661151434\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:14,911 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:703.671875|#Level:Host|#hostname:1e2d4f9a7709,timestamp:1661151434\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:14,913 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:12.9|#Level:Host|#hostname:1e2d4f9a7709,timestamp:1661151434\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,390 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,392 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]56\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,393 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,393 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,420 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,479 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,486 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151435486\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,608 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,615 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]55\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,618 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,620 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,619 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,682 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,684 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,684 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151435684\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,722 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,762 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,762 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,762 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,763 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,763 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,763 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,764 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,764 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,764 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,765 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,765 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,765 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,773 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,774 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,774 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,774 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,774 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,775 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,775 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,775 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,775 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,776 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,776 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,776 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,776 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,777 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,777 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 839, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,777 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 976, in get_code\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,795 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,806 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,778 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 906, in source_to_code\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,806 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,807 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/inference1.py\", line 17\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,807 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,807 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return list(input_data)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,807 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     ^\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,808 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - SyntaxError: invalid syntax\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,808 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,808 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,810 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,835 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,836 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,836 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,836 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,837 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,837 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,837 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,838 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:15,838 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:16,249 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:16,341 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2022-08-22T06:57:17,849 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:17,851 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]99\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:17,852 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:17,852 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:17,853 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:17,869 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:17,869 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151437869\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:17,882 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:17,883 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]102\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:17,884 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:17,884 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:17,884 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:17,907 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:17,907 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151437907\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:17,966 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:17,972 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,046 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,048 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,048 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,049 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,052 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,052 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,052 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,053 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,053 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,053 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,054 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,056 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,057 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,057 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,057 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,058 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,058 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,058 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,061 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,063 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,063 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,064 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,064 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,066 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,066 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,066 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,067 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,067 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,068 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,068 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,068 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,069 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,069 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,069 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,070 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,070 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,070 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,071 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,071 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,072 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,072 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,072 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,073 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,073 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,073 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,074 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,074 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,074 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,075 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,075 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 839, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,076 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 976, in get_code\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,076 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 906, in source_to_code\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,076 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,082 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/inference1.py\", line 17\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,085 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return list(input_data)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,075 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,077 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,087 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,088 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,089 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,089 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,088 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,090 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,091 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,091 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,092 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 1 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,087 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,102 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,103 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     ^\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,103 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,601 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:18,610 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,142 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,147 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]123\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,147 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,148 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,147 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,149 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151440149\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,150 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,150 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,215 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,216 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,218 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,219 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,219 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,216 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,221 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,221 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,224 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,224 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,220 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,230 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,270 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,273 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]124\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,273 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,273 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,277 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151440277\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,279 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,279 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,283 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,343 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,346 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,347 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,347 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,348 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,348 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,348 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,349 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,349 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,349 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,350 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,346 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,354 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,354 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,357 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,357 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 2 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,364 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,820 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:20,927 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,277 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,279 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]145\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,279 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,280 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,280 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,283 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151443283\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,283 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,285 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,327 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,328 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,333 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,333 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,334 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,334 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,337 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,333 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,338 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,361 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,363 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]149\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,363 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,363 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,363 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,365 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,366 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151443366\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,367 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,403 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,404 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,404 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,404 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,405 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,405 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,405 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,406 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 3 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,405 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,407 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:23,972 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:24,019 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,380 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,383 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]192\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,383 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,383 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,387 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,388 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,387 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151447387\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,390 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,433 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,434 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]195\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,435 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,435 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,436 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,437 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,437 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151447437\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,439 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,457 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,458 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,458 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,458 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,459 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,459 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,459 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,460 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,460 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,460 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,461 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,461 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,461 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,462 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,462 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,462 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,463 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,463 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,463 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,458 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,464 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,464 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,465 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,465 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,465 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,504 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,504 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,504 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,504 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,504 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,505 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,505 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,505 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,505 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,505 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,510 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,510 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,511 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,511 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,511 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,511 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,511 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,511 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,511 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,511 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,511 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,512 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,512 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,512 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,512 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,512 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,513 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 839, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,513 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 976, in get_code\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,513 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 906, in source_to_code\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,513 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,513 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/inference1.py\", line 17\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,513 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return list(input_data)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,514 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     ^\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,514 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - SyntaxError: invalid syntax\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,516 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,517 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,517 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,518 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:27,520 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 5 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:28,051 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:28,071 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:28,071 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:28,801 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:58734 \"GET /ping HTTP/1.1\" 200 44\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:28,804 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:1e2d4f9a7709,timestamp:1661151448\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:28,825 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:58750 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:28,826 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:1e2d4f9a7709,timestamp:1661151448\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2022-08-22T06:57:28.871:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=1, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,451 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,452 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]217\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,452 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,453 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,451 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,452 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]217\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,452 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,453 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,453 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,454 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151453454\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,463 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,464 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,501 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,501 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,502 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,502 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,503 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,503 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,453 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,454 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151453454\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,463 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,464 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,501 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,501 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,502 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,502 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,503 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,503 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,503 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,503 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,504 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,504 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,504 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,505 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,505 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,506 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,506 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,506 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,507 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,508 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,508 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,508 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,509 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,509 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,509 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,509 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,510 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,510 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,510 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 839, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,510 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 976, in get_code\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,510 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 906, in source_to_code\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,511 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,512 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/inference1.py\", line 17\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,512 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return list(input_data)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,512 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     ^\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,513 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - SyntaxError: invalid syntax\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,515 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,503 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,503 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,504 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,504 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,504 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,505 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,505 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,506 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,506 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,506 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,507 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,508 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,508 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,508 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,509 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,509 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,509 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,509 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,510 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,510 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,510 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 839, in exec_module\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,510 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 976, in get_code\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,510 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 906, in source_to_code\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,511 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,512 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/inference1.py\", line 17\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,512 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return list(input_data)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,512 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     ^\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,513 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - SyntaxError: invalid syntax\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,515 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,515 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,516 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,516 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,516 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,618 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,620 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]221\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,620 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,620 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,622 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151453622\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,620 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,623 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,623 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,687 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,688 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,688 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,688 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,689 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,689 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,689 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,689 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,690 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,690 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,690 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,691 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,691 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,691 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,691 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,692 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,692 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,692 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,515 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,516 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,516 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,516 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,618 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,620 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]221\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,620 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,620 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,622 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151453622\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,620 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,623 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,623 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,687 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,688 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,688 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,688 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,689 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,689 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,689 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,689 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,690 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,690 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,690 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,691 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,691 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,691 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,691 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,692 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,692 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,692 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,693 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,693 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,693 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,693 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,694 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,688 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,694 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,695 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,696 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,696 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 8 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:33,697 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,693 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,693 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,693 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,693 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,694 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,688 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,694 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,695 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,696 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,696 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 8 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:33,697 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:34,095 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:34,096 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:34,188 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2022-08-22T06:57:34,095 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:34,096 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:34,188 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,449 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,450 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]239\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,450 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,450 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,450 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,451 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151462451\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,452 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,453 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,515 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,515 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,515 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,516 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,449 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,450 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]239\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,450 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,450 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,450 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,451 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151462451\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,452 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,453 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,515 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,515 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,515 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,516 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,516 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,516 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,517 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,517 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,517 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,517 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,518 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,625 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,627 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]243\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,628 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,629 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,628 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,630 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,630 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151462630\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,632 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,668 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,668 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,669 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,669 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,669 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,670 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,670 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,670 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,670 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,671 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,671 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,671 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,516 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,516 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,517 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,517 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,517 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,517 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,518 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,625 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,627 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]243\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,628 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,629 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,628 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,630 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,630 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151462630\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,632 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,668 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,668 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,669 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,669 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,669 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,670 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,670 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,670 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,670 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,671 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,671 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,671 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,671 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,672 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,672 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,672 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,672 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,673 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,673 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,673 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,673 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,674 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,674 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,675 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,675 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,675 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,675 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 839, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,676 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 976, in get_code\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,676 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 906, in source_to_code\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,676 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,677 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/inference1.py\", line 17\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,677 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return list(input_data)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,677 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     ^\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,682 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - SyntaxError: invalid syntax\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,685 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,685 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,686 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,687 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:42,687 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 13 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,671 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,672 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,672 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,672 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,672 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,673 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,673 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,673 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,673 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,674 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,674 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,675 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,675 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,675 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,675 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 839, in exec_module\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,676 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 976, in get_code\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,676 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 906, in source_to_code\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,676 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,677 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/inference1.py\", line 17\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,677 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return list(input_data)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,677 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     ^\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,682 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - SyntaxError: invalid syntax\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,685 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,685 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,686 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,687 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:42,687 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 13 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:43,093 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:43,201 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:43,201 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:43,093 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:43,201 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:43,201 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2022-08-22T06:57:56,431 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,431 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,433 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,433 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151476433\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,434 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,469 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,470 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,470 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,472 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,472 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,472 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,472 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,473 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,473 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,473 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,473 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,473 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,473 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,473 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,474 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,478 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,478 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,431 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,431 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,433 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,433 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151476433\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,434 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,469 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,470 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,470 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,472 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,472 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,472 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,472 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,473 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,473 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,473 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,473 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,473 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,473 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,473 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,474 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,478 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,478 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,478 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,478 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,478 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,479 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,479 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,479 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,479 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,479 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,479 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,479 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 839, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,479 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 976, in get_code\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,480 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 906, in source_to_code\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,480 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,480 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/inference1.py\", line 17\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,480 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return list(input_data)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,480 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     ^\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,480 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - SyntaxError: invalid syntax\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,484 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,485 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,485 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,485 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,485 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,669 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,670 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]265\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,670 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,670 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,670 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,672 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,679 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151476679\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,680 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,478 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,478 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,478 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,479 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,479 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,479 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,479 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,479 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,479 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,479 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 839, in exec_module\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,479 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 976, in get_code\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,480 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 906, in source_to_code\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,480 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,480 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/inference1.py\", line 17\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,480 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return list(input_data)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,480 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     ^\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,480 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - SyntaxError: invalid syntax\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,484 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,485 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,485 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,485 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,485 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,669 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,670 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]265\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,670 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,670 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,670 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,672 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,679 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151476679\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,680 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,753 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,754 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,755 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,753 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,755 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,756 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,756 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,756 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 21 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:56,756 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,753 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,754 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,755 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,753 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,755 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,756 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,756 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,756 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 21 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:56,756 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:57,049 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:57,049 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:57:57,217 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2022-08-22T06:57:57,049 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:57,049 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T06:57:57,217 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:14,669 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:13.7|#Level:Host|#hostname:1e2d4f9a7709,timestamp:1661151494\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:14,669 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:6325.3671875|#Level:Host|#hostname:1e2d4f9a7709,timestamp:1661151494\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:14,669 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:1140.359375|#Level:Host|#hostname:1e2d4f9a7709,timestamp:1661151494\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:14,670 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:18.5|#Level:Host|#hostname:1e2d4f9a7709,timestamp:1661151494\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:14,679 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - --- Logging error ---\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1089, in emit\n",
      "    self.flush()\n",
      "  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1069, in flush\n",
      "    self.stream.flush()\u001b[0m\n",
      "\u001b[34mBrokenPipeError: [Errno 32] Broken pipe\u001b[0m\n",
      "\u001b[34mCall stack:\n",
      "  File \"ts/metrics/metric_collector.py\", line 29, in <module>\n",
      "    check_process_mem_usage(sys.stdin)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/ts/metrics/process_memory_metric.py\", line 40, in check_process_mem_usage\n",
      "    logging.info(\"%s:%d\", process, get_cpu_usage(process))\u001b[0m\n",
      "\u001b[34mMessage: '%s:%d'\u001b[0m\n",
      "\u001b[34mArguments: ('261', 0)\u001b[0m\n",
      "\u001b[34m--- Logging error ---\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/_common.py\", line 442, in wrapper\n",
      "    ret = self._cache[fun]\u001b[0m\n",
      "\u001b[34mAttributeError: _cache\u001b[0m\n",
      "\u001b[34mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:14,669 [INFO ] pool-3-thread-2 TS_METRICS - DiskUtilization.Percent:13.7|#Level:Host|#hostname:1e2d4f9a7709,timestamp:1661151494\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:14,669 [INFO ] pool-3-thread-2 TS_METRICS - MemoryAvailable.Megabytes:6325.3671875|#Level:Host|#hostname:1e2d4f9a7709,timestamp:1661151494\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:14,669 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUsed.Megabytes:1140.359375|#Level:Host|#hostname:1e2d4f9a7709,timestamp:1661151494\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:14,670 [INFO ] pool-3-thread-2 TS_METRICS - MemoryUtilization.Percent:18.5|#Level:Host|#hostname:1e2d4f9a7709,timestamp:1661151494\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:14,679 [ERROR] Thread-2 org.pytorch.serve.metrics.MetricCollector - --- Logging error ---\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1089, in emit\n",
      "    self.flush()\n",
      "  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1069, in flush\n",
      "    self.stream.flush()\u001b[0m\n",
      "\u001b[35mBrokenPipeError: [Errno 32] Broken pipe\u001b[0m\n",
      "\u001b[35mCall stack:\n",
      "  File \"ts/metrics/metric_collector.py\", line 29, in <module>\n",
      "    check_process_mem_usage(sys.stdin)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/ts/metrics/process_memory_metric.py\", line 40, in check_process_mem_usage\n",
      "    logging.info(\"%s:%d\", process, get_cpu_usage(process))\u001b[0m\n",
      "\u001b[35mMessage: '%s:%d'\u001b[0m\n",
      "\u001b[35mArguments: ('261', 0)\u001b[0m\n",
      "\u001b[35m--- Logging error ---\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/_common.py\", line 442, in wrapper\n",
      "    ret = self._cache[fun]\u001b[0m\n",
      "\u001b[35mAttributeError: _cache\u001b[0m\n",
      "\u001b[35mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/_pslinux.py\", line 1642, in wrapper\n",
      "    return fun(self, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/_common.py\", line 445, in wrapper\n",
      "    return fun(self)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/_pslinux.py\", line 1684, in _parse_stat_file\n",
      "    data = bcat(\"%s/%s/stat\" % (self._procfs_path, self.pid))\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/_common.py\", line 775, in bcat\n",
      "    return cat(fname, fallback=fallback, _open=open_binary)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/_common.py\", line 763, in cat\n",
      "    with _open(fname) as f:\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/_common.py\", line 727, in open_binary\n",
      "    return open(fname, \"rb\", buffering=FILE_READ_BUFFER_SIZE)\u001b[0m\n",
      "\u001b[34mFileNotFoundError: [Errno 2] No such file or directory: '/proc/265/stat'\u001b[0m\n",
      "\u001b[34mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/__init__.py\", line 361, in _init\n",
      "    self.create_time()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/__init__.py\", line 714, in create_time\n",
      "    self._create_time = self._proc.create_time()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/_pslinux.py\", line 1642, in wrapper\n",
      "    return fun(self, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/_pslinux.py\", line 1852, in create_time\n",
      "    ctime = float(self._parse_stat_file()['create_time'])\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/_pslinux.py\", line 1649, in wrapper\n",
      "    raise NoSuchProcess(self.pid, self._name)\u001b[0m\n",
      "\u001b[34mpsutil.NoSuchProcess: process no longer exists (pid=265)\u001b[0m\n",
      "\u001b[34mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/ts/metrics/process_memory_metric.py\", line 20, in get_cpu_usage\n",
      "    process = psutil.Process(int(pid))\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/__init__.py\", line 332, in __init__\n",
      "    self._init(pid)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/__init__.py\", line 373, in _init\n",
      "    raise NoSuchProcess(pid, msg='process PID not found')\u001b[0m\n",
      "\u001b[34mpsutil.NoSuchProcess: process PID not found (pid=265)\u001b[0m\n",
      "\u001b[34mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1089, in emit\n",
      "    self.flush()\n",
      "  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1069, in flush\n",
      "    self.stream.flush()\u001b[0m\n",
      "\u001b[34mBrokenPipeError: [Errno 32] Broken pipe\u001b[0m\n",
      "\u001b[34mCall stack:\n",
      "  File \"ts/metrics/metric_collector.py\", line 29, in <module>\n",
      "    check_process_mem_usage(sys.stdin)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/ts/metrics/process_memory_metric.py\", line 40, in check_process_mem_usage\n",
      "    logging.info(\"%s:%d\", process, get_cpu_usage(process))\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/ts/metrics/process_memory_metric.py\", line 22, in get_cpu_usage\n",
      "    logging.error(\"Failed get process for pid: %s\", pid, exc_info=True)\u001b[0m\n",
      "\u001b[34mMessage: 'Failed get process for pid: %s'\u001b[0m\n",
      "\u001b[34mArguments: ('265',)\u001b[0m\n",
      "\u001b[34m--- Logging error ---\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1089, in emit\n",
      "    self.flush()\n",
      "  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1069, in flush\n",
      "    self.stream.flush()\u001b[0m\n",
      "\u001b[34mBrokenPipeError: [Errno 32] Broken pipe\u001b[0m\n",
      "\u001b[34mCall stack:\n",
      "  File \"ts/metrics/metric_collector.py\", line 29, in <module>\n",
      "    check_process_mem_usage(sys.stdin)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/ts/metrics/process_memory_metric.py\", line 40, in check_process_mem_usage\n",
      "    logging.info(\"%s:%d\", process, get_cpu_usage(process))\u001b[0m\n",
      "\u001b[34mMessage: '%s:%d'\u001b[0m\n",
      "\u001b[34mArguments: ('265', 0)\u001b[0m\n",
      "\u001b[34mException ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>\u001b[0m\n",
      "\u001b[34mBrokenPipeError: [Errno 32] Broken pipe\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/_pslinux.py\", line 1642, in wrapper\n",
      "    return fun(self, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/_common.py\", line 445, in wrapper\n",
      "    return fun(self)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/_pslinux.py\", line 1684, in _parse_stat_file\n",
      "    data = bcat(\"%s/%s/stat\" % (self._procfs_path, self.pid))\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/_common.py\", line 775, in bcat\n",
      "    return cat(fname, fallback=fallback, _open=open_binary)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/_common.py\", line 763, in cat\n",
      "    with _open(fname) as f:\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/_common.py\", line 727, in open_binary\n",
      "    return open(fname, \"rb\", buffering=FILE_READ_BUFFER_SIZE)\u001b[0m\n",
      "\u001b[35mFileNotFoundError: [Errno 2] No such file or directory: '/proc/265/stat'\u001b[0m\n",
      "\u001b[35mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/__init__.py\", line 361, in _init\n",
      "    self.create_time()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/__init__.py\", line 714, in create_time\n",
      "    self._create_time = self._proc.create_time()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/_pslinux.py\", line 1642, in wrapper\n",
      "    return fun(self, *args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/_pslinux.py\", line 1852, in create_time\n",
      "    ctime = float(self._parse_stat_file()['create_time'])\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/_pslinux.py\", line 1649, in wrapper\n",
      "    raise NoSuchProcess(self.pid, self._name)\u001b[0m\n",
      "\u001b[35mpsutil.NoSuchProcess: process no longer exists (pid=265)\u001b[0m\n",
      "\u001b[35mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/ts/metrics/process_memory_metric.py\", line 20, in get_cpu_usage\n",
      "    process = psutil.Process(int(pid))\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/__init__.py\", line 332, in __init__\n",
      "    self._init(pid)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/psutil/__init__.py\", line 373, in _init\n",
      "    raise NoSuchProcess(pid, msg='process PID not found')\u001b[0m\n",
      "\u001b[35mpsutil.NoSuchProcess: process PID not found (pid=265)\u001b[0m\n",
      "\u001b[35mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1089, in emit\n",
      "    self.flush()\n",
      "  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1069, in flush\n",
      "    self.stream.flush()\u001b[0m\n",
      "\u001b[35mBrokenPipeError: [Errno 32] Broken pipe\u001b[0m\n",
      "\u001b[35mCall stack:\n",
      "  File \"ts/metrics/metric_collector.py\", line 29, in <module>\n",
      "    check_process_mem_usage(sys.stdin)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/ts/metrics/process_memory_metric.py\", line 40, in check_process_mem_usage\n",
      "    logging.info(\"%s:%d\", process, get_cpu_usage(process))\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/ts/metrics/process_memory_metric.py\", line 22, in get_cpu_usage\n",
      "    logging.error(\"Failed get process for pid: %s\", pid, exc_info=True)\u001b[0m\n",
      "\u001b[35mMessage: 'Failed get process for pid: %s'\u001b[0m\n",
      "\u001b[35mArguments: ('265',)\u001b[0m\n",
      "\u001b[35m--- Logging error ---\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1089, in emit\n",
      "    self.flush()\n",
      "  File \"/opt/conda/lib/python3.8/logging/__init__.py\", line 1069, in flush\n",
      "    self.stream.flush()\u001b[0m\n",
      "\u001b[35mBrokenPipeError: [Errno 32] Broken pipe\u001b[0m\n",
      "\u001b[35mCall stack:\n",
      "  File \"ts/metrics/metric_collector.py\", line 29, in <module>\n",
      "    check_process_mem_usage(sys.stdin)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/ts/metrics/process_memory_metric.py\", line 40, in check_process_mem_usage\n",
      "    logging.info(\"%s:%d\", process, get_cpu_usage(process))\u001b[0m\n",
      "\u001b[35mMessage: '%s:%d'\u001b[0m\n",
      "\u001b[35mArguments: ('265', 0)\u001b[0m\n",
      "\u001b[35mException ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>\u001b[0m\n",
      "\u001b[35mBrokenPipeError: [Errno 32] Broken pipe\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,287 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,288 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]285\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,289 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,289 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,289 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,290 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,287 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,288 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]285\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,289 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,289 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,289 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,290 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,291 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151498291\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,291 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,343 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,343 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,343 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,344 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,344 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,345 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,343 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,345 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,346 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,730 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,731 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]289\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,731 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,732 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,732 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,734 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151498734\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,736 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,737 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,768 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,769 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,770 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,770 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,771 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,771 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,771 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,771 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 34 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,771 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,772 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,291 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151498291\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,291 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,343 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,343 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,343 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,344 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,344 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,345 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,343 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,345 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,346 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,730 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,731 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]289\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,731 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,732 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,732 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,734 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151498734\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,736 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,737 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,768 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,769 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,770 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,770 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,771 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,771 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,771 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,771 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 34 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,771 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:18,772 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:18,944 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:19,215 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2022-08-22T06:58:18,944 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:19,215 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,076 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,076 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,076 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,078 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151533078\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,078 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,079 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,139 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,141 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,142 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,143 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,143 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,076 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,076 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,076 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,078 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151533078\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,078 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,079 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,139 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,141 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,142 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,143 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,143 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,143 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,144 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,142 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,146 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,701 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,701 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]312\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,702 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,702 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,702 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,703 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,704 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151533704\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,710 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,711 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,738 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,738 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,738 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,739 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,739 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,739 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,739 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,739 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,739 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,739 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,739 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,739 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,739 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,739 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,739 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,739 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,143 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,144 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,142 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,146 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,701 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,701 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]312\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,702 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,702 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,702 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,703 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,704 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151533704\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,710 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,711 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,738 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,738 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,738 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,739 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,739 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,739 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,739 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,739 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,739 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,739 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,739 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,739 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,739 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,739 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,739 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,739 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,739 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 839, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 976, in get_code\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 906, in source_to_code\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/inference1.py\", line 17\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return list(input_data)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     ^\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - SyntaxError: invalid syntax\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,745 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,745 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,746 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,746 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:53,746 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 55 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,739 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 839, in exec_module\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 976, in get_code\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 906, in source_to_code\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/inference1.py\", line 17\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return list(input_data)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     ^\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,740 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - SyntaxError: invalid syntax\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,745 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,745 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,746 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,746 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:53,746 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 55 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:54,171 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:58:54,171 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2022-08-22T06:58:54,171 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T06:58:54,171 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:48,793 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:48,793 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:48,793 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:48,794 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:48,795 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151588795\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:48,795 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:48,793 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:48,793 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:48,793 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:48,794 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:48,795 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151588795\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:48,795 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:48,858 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:48,858 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:48,859 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:48,859 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:48,859 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:48,859 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:48,860 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:48,860 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:48,860 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:48,860 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:48,860 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:48,861 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:48,861 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:48,861 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,411 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,562 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,563 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]334\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,564 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,564 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,565 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151589565\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,568 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,568 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,568 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,601 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,602 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,603 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,603 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,605 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,608 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,608 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,608 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,608 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,609 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,609 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:48,858 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:48,858 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:48,859 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:48,859 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:48,859 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:48,859 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:48,860 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:48,860 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:48,860 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:48,860 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:48,860 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:48,861 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:48,861 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:48,861 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 89 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,411 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,562 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,563 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]334\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,564 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,564 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,565 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151589565\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,568 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,568 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,568 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,601 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,602 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,603 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,603 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,605 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,608 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,608 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,608 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,608 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,609 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,609 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,609 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,609 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,610 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,610 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,610 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,610 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,611 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,611 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,611 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,611 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,611 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,612 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,612 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,612 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,612 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,613 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 839, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,613 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 976, in get_code\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,613 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 906, in source_to_code\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,614 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,614 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/inference1.py\", line 17\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,614 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return list(input_data)\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,614 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     ^\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,615 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - SyntaxError: invalid syntax\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,605 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,615 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,616 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,616 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:49,616 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 89 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,609 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,609 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,610 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,610 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,610 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,610 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,611 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,611 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,611 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,611 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,611 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,612 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,612 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,612 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,612 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,613 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 839, in exec_module\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,613 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 976, in get_code\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,613 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 906, in source_to_code\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,614 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,614 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/inference1.py\", line 17\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,614 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return list(input_data)\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,614 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     ^\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,615 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - SyntaxError: invalid syntax\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,605 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,615 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,616 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,616 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:49,616 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 89 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:50,046 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T06:59:50,046 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2022-08-22T06:59:50,046 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T06:59:50,046 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,499 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,499 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,499 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,501 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151678501\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,504 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,504 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,539 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,539 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,540 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,540 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,540 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,540 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,540 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,541 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,541 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,541 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,541 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,541 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,542 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,542 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,542 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,539 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,542 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,543 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,543 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,543 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,544 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,499 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,499 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,499 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,501 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151678501\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,504 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,504 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,539 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,539 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,540 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,540 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,540 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,540 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,540 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,541 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,541 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,541 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,541 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,541 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,542 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,542 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,542 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,539 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,542 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,543 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,543 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,543 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,544 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,544 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,544 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,544 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,544 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,544 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,545 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,545 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 839, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,545 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 976, in get_code\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,543 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,545 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,546 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,546 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 144 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:18,546 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,544 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,544 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,544 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,544 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,544 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,545 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,545 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 839, in exec_module\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,545 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 976, in get_code\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,543 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,545 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,546 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,546 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 144 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:18,546 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,128 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,315 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,316 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]361\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,316 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,317 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,319 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151679319\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,320 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,320 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,320 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,355 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,355 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,355 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,355 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,356 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,356 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,128 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,315 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,316 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]361\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,316 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,317 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,319 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151679319\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,320 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,320 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,320 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,355 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,355 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,355 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,355 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,356 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,356 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,356 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,357 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,357 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,357 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,357 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,357 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,357 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,358 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,358 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,358 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,358 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,358 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,358 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,358 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,358 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,358 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,359 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,359 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,359 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,359 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,359 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 839, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,359 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 976, in get_code\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,359 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 906, in source_to_code\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,359 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,360 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/inference1.py\", line 17\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,360 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return list(input_data)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,360 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     ^\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,360 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - SyntaxError: invalid syntax\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,361 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,361 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,361 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,363 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,363 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 144 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,790 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T07:01:19,790 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2022-08-22T07:01:19,356 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,357 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,357 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,357 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,357 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,357 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,357 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,358 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,358 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,358 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,358 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,358 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,358 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,358 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,358 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,358 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,359 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,359 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,359 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,359 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,359 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 839, in exec_module\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,359 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 976, in get_code\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,359 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 906, in source_to_code\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,359 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,360 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/inference1.py\", line 17\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,360 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return list(input_data)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,360 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     ^\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,360 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - SyntaxError: invalid syntax\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,361 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,361 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,361 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,363 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,363 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 144 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,790 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T07:01:19,790 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:43,231 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 976, in get_code\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:43,231 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 906, in source_to_code\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:43,231 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:43,231 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/inference1.py\", line 17\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:43,231 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return list(input_data)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:43,231 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     ^\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:43,231 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - SyntaxError: invalid syntax\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:43,236 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:43,236 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:43,236 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:43,231 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 976, in get_code\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:43,231 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 906, in source_to_code\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:43,231 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:43,231 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/inference1.py\", line 17\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:43,231 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return list(input_data)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:43,231 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     ^\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:43,231 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - SyntaxError: invalid syntax\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:43,236 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:43,236 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:43,236 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:43,236 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:43,236 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 233 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:43,690 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:43,690 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:43,236 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:43,236 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 233 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:43,690 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:43,690 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,030 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,031 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]386\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,031 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,031 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,032 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,032 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151824032\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,033 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,033 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,067 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,067 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,067 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,067 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,067 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,068 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,068 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,068 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,068 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,068 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,068 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,068 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,068 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,068 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,068 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,068 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,068 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,074 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,074 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,030 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,031 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]386\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,031 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,031 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,032 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,032 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661151824032\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,033 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,033 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,067 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,067 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,067 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,067 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,067 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,068 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,068 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,068 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,068 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,068 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,068 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,068 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,068 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,068 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,068 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,068 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,068 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,074 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,074 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,074 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,074 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,074 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,074 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,074 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,075 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,075 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,075 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 839, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,075 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 976, in get_code\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,075 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 906, in source_to_code\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,075 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,075 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/inference1.py\", line 17\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,075 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return list(input_data)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,075 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     ^\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,075 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - SyntaxError: invalid syntax\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,076 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,077 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,077 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,077 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,077 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 233 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,496 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T07:03:44,496 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2022-08-22T07:03:44,074 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,074 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,074 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,074 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,074 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,075 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,075 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,075 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 839, in exec_module\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,075 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 976, in get_code\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,075 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 906, in source_to_code\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,075 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,075 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/inference1.py\", line 17\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,075 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return list(input_data)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,075 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     ^\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,075 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - SyntaxError: invalid syntax\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,076 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,077 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,077 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,077 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,077 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 233 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,496 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T07:03:44,496 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:36,928 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:36,928 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:36,928 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:36,928 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:36,929 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:36,929 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:36,929 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:36,929 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:36,930 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:36,930 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:36,925 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:36,934 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:36,934 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:36,934 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:36,935 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 377 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:36,934 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:36,944 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:36,928 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:36,928 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:36,928 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:36,928 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:36,929 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:36,929 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:36,929 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:36,929 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:36,930 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:36,930 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:36,925 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:36,934 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:36,934 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:36,934 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:36,935 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 377 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:36,934 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:36,944 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,425 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,756 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,757 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]412\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,758 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,758 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,760 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,761 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,761 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661152057761\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,761 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,425 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,756 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,757 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]412\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,758 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,758 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,760 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,761 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,761 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661152057761\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,761 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,800 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,800 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,800 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,800 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,800 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,800 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,800 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 839, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,800 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 976, in get_code\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,800 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 906, in source_to_code\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,800 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,800 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/inference1.py\", line 17\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,800 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return list(input_data)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,800 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     ^\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,800 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - SyntaxError: invalid syntax\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,801 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,801 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,802 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,802 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:37,802 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 377 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,799 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,800 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,800 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,800 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,800 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,800 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,800 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,800 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 839, in exec_module\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,800 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 976, in get_code\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,800 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 906, in source_to_code\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,800 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,800 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/inference1.py\", line 17\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,800 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return list(input_data)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,800 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     ^\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,800 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - SyntaxError: invalid syntax\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,801 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,801 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,802 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,802 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:37,802 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 377 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:38,231 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T07:07:38,231 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2022-08-22T07:07:38,231 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T07:07:38,231 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,565 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,565 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,571 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661152434571\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,576 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,576 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,577 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,610 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,610 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,610 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,610 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,565 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,565 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,571 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661152434571\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,576 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,576 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,577 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,610 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,610 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,610 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,610 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 839, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 976, in get_code\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 906, in source_to_code\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/inference1.py\", line 17\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return list(input_data)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     ^\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - SyntaxError: invalid syntax\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,612 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,613 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,614 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,614 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:54,615 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 610 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,042 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,043 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 839, in exec_module\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 976, in get_code\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 906, in source_to_code\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/inference1.py\", line 17\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     return list(input_data)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG -     ^\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,611 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - SyntaxError: invalid syntax\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,612 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,613 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,614 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,614 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:54,615 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 610 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,042 [INFO ] W-9000-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,043 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,448 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,449 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]440\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,449 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,449 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,448 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,449 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]440\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,449 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,449 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,449 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,450 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661152435450\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,451 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,451 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,485 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,485 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,485 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,485 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,485 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,485 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,485 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,485 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 839, in exec_module\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 976, in get_code\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 906, in source_to_code\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/inference1.py\", line 17\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return list(input_data)\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     ^\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - SyntaxError: invalid syntax\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,487 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,487 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,488 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,488 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,489 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 610 seconds.\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,909 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[34m2022-08-22T07:13:55,909 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2022-08-22T07:13:55,449 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,450 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1661152435450\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,451 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,451 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,485 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend worker process died.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,485 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Traceback (most recent call last):\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,485 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 210, in <module>\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,485 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     worker.run_server()\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,485 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 181, in run_server\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,485 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,485 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 139, in handle_connection\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,485 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_service_worker.py\", line 104, in load_model\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     service = model_loader.load(\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/ts/model_loader.py\", line 151, in load\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     initialize_fn(service.context)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/handler_service.py\", line 51, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     super().initialize(context)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/default_handler_service.py\", line 66, in initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._service.validate_and_initialize(model_dir=model_dir)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 159, in validate_and_initialize\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     self._validate_user_module_and_set_functions()\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/site-packages/sagemaker_inference/transformer.py\", line 182, in _validate_user_module_and_set_functions\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     user_module = importlib.import_module(user_module_name)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/conda/lib/python3.8/importlib/__init__.py\", line 127, in import_module\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 839, in exec_module\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 976, in get_code\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap_external>\", line 906, in source_to_code\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -   File \"/opt/ml/model/code/inference1.py\", line 17\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     return list(input_data)\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG -     ^\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,486 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - SyntaxError: invalid syntax\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,487 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9001 Worker disconnected. WORKER_STARTED\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,487 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,488 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stderr\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,488 [WARN ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,489 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9001 in 610 seconds.\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,909 [INFO ] W-9001-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stdout\u001b[0m\n",
      "\u001b[35m2022-08-22T07:13:55,909 [INFO ] W-9001-model_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9001-model_1.0-stderr\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [117]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_s3\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msplit_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext/csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aws_batch_transform-pg7SNxiG/lib/python3.9/site-packages/sagemaker/workflow/pipeline_context.py:248\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m     run_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m self_instance\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mcontext\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aws_batch_transform-pg7SNxiG/lib/python3.9/site-packages/sagemaker/transformer.py:243\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, data, data_type, content_type, compression_type, split_type, job_name, input_filter, output_filter, join_source, experiment_config, model_client_config, wait, logs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_transform_job \u001b[38;5;241m=\u001b[39m _TransformJob\u001b[38;5;241m.\u001b[39mstart_new(\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    230\u001b[0m     data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m     model_client_config,\n\u001b[1;32m    240\u001b[0m )\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m--> 243\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_transform_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aws_batch_transform-pg7SNxiG/lib/python3.9/site-packages/sagemaker/transformer.py:440\u001b[0m, in \u001b[0;36m_TransformJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logs:\n\u001b[0;32m--> 440\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_transform_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_transform_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/aws_batch_transform-pg7SNxiG/lib/python3.9/site-packages/sagemaker/session.py:4006\u001b[0m, in \u001b[0;36mSession.logs_for_transform_job\u001b[0;34m(self, job_name, wait, poll)\u001b[0m\n\u001b[1;32m   4003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;241m==\u001b[39m LogState\u001b[38;5;241m.\u001b[39mCOMPLETE:\n\u001b[1;32m   4004\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 4006\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4008\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;241m==\u001b[39m LogState\u001b[38;5;241m.\u001b[39mJOB_COMPLETE:\n\u001b[1;32m   4009\u001b[0m     state \u001b[38;5;241m=\u001b[39m LogState\u001b[38;5;241m.\u001b[39mCOMPLETE\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transformer.transform(\n",
    "    data=train_s3,\n",
    "    data_type = data_type,\n",
    "    split_type = split_type,\n",
    "    content_type = 'text/csv',\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ed692d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
